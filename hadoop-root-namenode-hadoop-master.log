2019-01-27 12:30:11,371 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = hadoop-master/172.18.0.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.2
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.2.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.2.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.2.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.2.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.2-tests.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.2.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.2.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.2-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r b165c4fe8a74265c792ce23f546c64604acf0e41; compiled by 'jenkins' on 2016-01-26T00:08Z
STARTUP_MSG:   java = 1.7.0_181
************************************************************/
2019-01-27 12:30:11,378 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-01-27 12:30:11,383 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2019-01-27 12:30:11,688 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-01-27 12:30:11,793 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-01-27 12:30:11,793 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2019-01-27 12:30:11,795 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://hadoop-master:9000/
2019-01-27 12:30:11,796 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use hadoop-master:9000 to access this namenode/service.
2019-01-27 12:30:12,051 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2019-01-27 12:30:12,108 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-01-27 12:30:12,118 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-01-27 12:30:12,125 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2019-01-27 12:30:12,131 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-01-27 12:30:12,134 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-01-27 12:30:12,134 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-01-27 12:30:12,134 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-01-27 12:30:12,174 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-01-27 12:30:12,175 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-01-27 12:30:12,193 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2019-01-27 12:30:12,193 INFO org.mortbay.log: jetty-6.1.26
2019-01-27 12:30:12,341 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2019-01-27 12:30:12,372 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-01-27 12:30:12,372 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-01-27 12:30:12,405 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2019-01-27 12:30:12,405 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2019-01-27 12:30:12,438 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2019-01-27 12:30:12,438 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-01-27 12:30:12,439 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-01-27 12:30:12,440 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Jan 27 12:30:12
2019-01-27 12:30:12,442 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-01-27 12:30:12,442 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-01-27 12:30:12,443 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2019-01-27 12:30:12,443 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2019-01-27 12:30:12,449 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2019-01-27 12:30:12,449 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 2
2019-01-27 12:30:12,449 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-01-27 12:30:12,449 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-01-27 12:30:12,449 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-01-27 12:30:12,449 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2019-01-27 12:30:12,449 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-01-27 12:30:12,449 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-01-27 12:30:12,455 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-01-27 12:30:12,455 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-01-27 12:30:12,455 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2019-01-27 12:30:12,455 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2019-01-27 12:30:12,456 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2019-01-27 12:30:12,490 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-01-27 12:30:12,490 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-01-27 12:30:12,491 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2019-01-27 12:30:12,491 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2019-01-27 12:30:12,502 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-01-27 12:30:12,502 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-01-27 12:30:12,502 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2019-01-27 12:30:12,503 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2019-01-27 12:30:12,509 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-01-27 12:30:12,509 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-01-27 12:30:12,509 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2019-01-27 12:30:12,509 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2019-01-27 12:30:12,510 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-01-27 12:30:12,510 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2019-01-27 12:30:12,511 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2019-01-27 12:30:12,513 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-01-27 12:30:12,513 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-01-27 12:30:12,513 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-01-27 12:30:12,514 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2019-01-27 12:30:12,514 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-01-27 12:30:12,516 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2019-01-27 12:30:12,516 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-01-27 12:30:12,516 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2019-01-27 12:30:12,516 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2019-01-27 12:30:12,545 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /root/hdfs/namenode/in_use.lock acquired by nodename 815@hadoop-master
2019-01-27 12:30:12,618 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /root/hdfs/namenode/current
2019-01-27 12:30:12,618 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2019-01-27 12:30:12,663 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2019-01-27 12:30:12,697 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-01-27 12:30:12,697 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /root/hdfs/namenode/current/fsimage_0000000000000000000
2019-01-27 12:30:12,709 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2019-01-27 12:30:12,709 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2019-01-27 12:30:12,776 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-01-27 12:30:12,777 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 257 msecs
2019-01-27 12:30:13,009 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to hadoop-master:9000
2019-01-27 12:30:13,017 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2019-01-27 12:30:13,025 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2019-01-27 12:30:13,082 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2019-01-27 12:30:13,090 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2019-01-27 12:30:13,090 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2019-01-27 12:30:13,090 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2019-01-27 12:30:13,090 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2019-01-27 12:30:13,090 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2019-01-27 12:30:13,090 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2019-01-27 12:30:13,097 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2019-01-27 12:30:13,098 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2019-01-27 12:30:13,098 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2019-01-27 12:30:13,098 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2019-01-27 12:30:13,098 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2019-01-27 12:30:13,098 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2019-01-27 12:30:13,098 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 8 msec
2019-01-27 12:30:13,128 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-01-27 12:30:13,128 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2019-01-27 12:30:13,131 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: hadoop-master/172.18.0.4:9000
2019-01-27 12:30:13,131 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2019-01-27 12:30:13,135 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2019-01-27 12:30:18,473 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.3:50010, datanodeUuid=79bc8e05-505a-43b0-a28c-f23d175866bb, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3428d32f-669a-4980-807b-bac983b05aa8;nsid=545847971;c=0) storage 79bc8e05-505a-43b0-a28c-f23d175866bb
2019-01-27 12:30:18,473 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2019-01-27 12:30:18,474 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.3:50010
2019-01-27 12:30:18,478 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.2:50010, datanodeUuid=6c8bbf2d-ad06-474c-98f7-7a2e4073d019, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3428d32f-669a-4980-807b-bac983b05aa8;nsid=545847971;c=0) storage 6c8bbf2d-ad06-474c-98f7-7a2e4073d019
2019-01-27 12:30:18,478 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2019-01-27 12:30:18,479 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.2:50010
2019-01-27 12:30:18,558 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2019-01-27 12:30:18,558 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2 for DN 172.18.0.3:50010
2019-01-27 12:30:18,559 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2019-01-27 12:30:18,559 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50 for DN 172.18.0.2:50010
2019-01-27 12:30:18,600 INFO BlockStateChange: BLOCK* processReport: from storage DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2 node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=79bc8e05-505a-43b0-a28c-f23d175866bb, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3428d32f-669a-4980-807b-bac983b05aa8;nsid=545847971;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2019-01-27 12:30:18,600 INFO BlockStateChange: BLOCK* processReport: from storage DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50 node DatanodeRegistration(172.18.0.2:50010, datanodeUuid=6c8bbf2d-ad06-474c-98f7-7a2e4073d019, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3428d32f-669a-4980-807b-bac983b05aa8;nsid=545847971;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2019-01-27 12:31:46,006 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 18 Total time for transactions(ms): 30 Number of transactions batched in Syncs: 0 Number of syncs: 13 SyncTimes(ms): 104 
2019-01-27 12:31:47,950 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:31:49,475 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:31:49,482 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:31:49,491 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:31:50,460 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:31:50,461 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:31:50,463 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:31:52,010 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:31:52,011 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:31:52,012 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:31:54,905 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:31:54,906 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:31:54,908 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:31:57,867 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:31:57,869 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:31:57,871 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:32:02,220 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:32:02,221 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:32:02,223 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:32:08,806 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:32:08,808 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:32:08,810 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:32:11,142 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:32:11,144 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:32:11,151 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:32:15,890 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:32:15,891 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:32:15,892 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:32:21,179 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:32:21,180 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:32:21,182 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:32:25,726 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:32:25,728 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:32:25,728 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:32:31,657 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:32:31,659 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:32:31,660 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:32:39,030 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:32:39,031 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:32:39,032 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:32:43,034 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:32:43,035 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:32:43,036 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:32:44,418 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:32:44,419 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:32:44,426 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:32:48,929 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:32:48,930 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:32:48,932 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:32:48,932 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 75 Total time for transactions(ms): 45 Number of transactions batched in Syncs: 0 Number of syncs: 36 SyncTimes(ms): 1027 
2019-01-27 12:32:56,465 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:32:56,466 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:32:56,468 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:33:00,463 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:33:00,463 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:33:00,465 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:33:01,880 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:33:01,881 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:33:01,882 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:33:06,753 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:33:06,753 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:33:06,755 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:33:13,079 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:33:13,080 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:33:13,080 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:33:18,961 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:33:18,962 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:33:18,964 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741847_1023{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:33:24,116 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741847_1023{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:33:24,117 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741847_1023{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:33:24,118 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:33:29,299 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:33:29,299 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:33:29,300 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741849_1025{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:33:34,974 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741849_1025{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:33:34,976 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741849_1025{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:33:34,977 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741850_1026{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:33:40,350 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741850_1026{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:33:40,352 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741850_1026{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:33:40,353 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741851_1027{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:33:45,962 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741851_1027{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:33:45,962 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741851_1027{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:33:45,964 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741852_1028{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:33:53,652 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741852_1028{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:33:53,681 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741852_1028{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:33:53,682 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741853_1029{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:33:53,682 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 111 Total time for transactions(ms): 46 Number of transactions batched in Syncs: 0 Number of syncs: 48 SyncTimes(ms): 1942 
2019-01-27 12:33:59,789 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741853_1029{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:33:59,790 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741853_1029{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:33:59,791 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741854_1030{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:34:02,352 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741854_1030{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:34:02,353 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741854_1030{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:34:02,355 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741855_1031{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:34:07,848 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741855_1031{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:34:07,849 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741855_1031{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:34:07,850 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:34:11,846 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:34:11,847 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:34:11,848 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741857_1033{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:34:15,784 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741857_1033{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:34:15,785 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741857_1033{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:34:15,787 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741858_1034{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:34:20,626 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741858_1034{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:34:20,641 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741858_1034{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:34:20,642 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:34:26,519 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:34:26,520 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:34:26,521 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:34:30,392 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:34:30,393 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:34:30,394 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741861_1037{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:34:35,670 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741861_1037{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:34:35,671 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741861_1037{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:34:35,672 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741862_1038{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:34:44,314 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741862_1038{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:34:44,315 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741862_1038{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:34:44,317 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741863_1039{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:34:51,833 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741863_1039{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:34:51,834 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741863_1039{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:34:51,835 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741864_1040{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:34:58,646 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741864_1040{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:34:58,647 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741864_1040{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:34:58,649 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741865_1041{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:34:58,649 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 148 Total time for transactions(ms): 46 Number of transactions batched in Syncs: 0 Number of syncs: 61 SyncTimes(ms): 3099 
2019-01-27 12:35:06,428 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741865_1041{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:35:06,429 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741865_1041{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:35:06,432 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741866_1042{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:35:12,215 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741866_1042{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:35:12,215 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741866_1042{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:35:12,216 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:35:17,094 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:35:17,095 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:35:17,097 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:35:26,798 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:35:26,803 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:35:59,810 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 160 Total time for transactions(ms): 46 Number of transactions batched in Syncs: 0 Number of syncs: 67 SyncTimes(ms): 3662 
2019-01-27 12:35:59,815 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741825_1001 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:35:59,815 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741826_1002 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:35:59,815 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741827_1003 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:35:59,815 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741828_1004 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:35:59,815 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741829_1005 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:35:59,815 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741830_1006 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:35:59,815 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741831_1007 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:35:59,816 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741832_1008 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:35:59,816 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741833_1009 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:35:59,816 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741834_1010 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:35:59,816 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741835_1011 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:35:59,816 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741836_1012 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:35:59,816 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741837_1013 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:35:59,816 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741838_1014 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:35:59,816 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741839_1015 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:35:59,816 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741840_1016 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:35:59,816 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741841_1017 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:35:59,816 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741842_1018 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:35:59,816 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741843_1019 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:35:59,816 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741844_1020 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:35:59,816 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741845_1021 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:35:59,816 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741846_1022 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:35:59,816 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741847_1023 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:35:59,817 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741848_1024 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:35:59,817 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741849_1025 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:35:59,817 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741850_1026 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:35:59,817 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741851_1027 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:35:59,817 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741852_1028 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:35:59,817 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741853_1029 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:35:59,817 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741854_1030 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:35:59,817 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741855_1031 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:35:59,817 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741856_1032 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:35:59,817 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741857_1033 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:35:59,817 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741858_1034 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:35:59,817 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741859_1035 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:35:59,817 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741860_1036 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:35:59,817 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741861_1037 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:35:59,817 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741862_1038 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:35:59,817 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741863_1039 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:35:59,818 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741864_1040 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:35:59,818 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741865_1041 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:35:59,818 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741866_1042 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:35:59,818 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741867_1043 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:35:59,818 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:36:01,131 INFO BlockStateChange: BLOCK* BlockManager: ask 172.18.0.2:50010 to delete [blk_1073741825_1001, blk_1073741826_1002, blk_1073741827_1003, blk_1073741828_1004, blk_1073741829_1005, blk_1073741830_1006, blk_1073741831_1007, blk_1073741832_1008, blk_1073741833_1009, blk_1073741834_1010, blk_1073741835_1011, blk_1073741836_1012, blk_1073741837_1013, blk_1073741838_1014, blk_1073741839_1015, blk_1073741840_1016, blk_1073741841_1017, blk_1073741842_1018, blk_1073741843_1019, blk_1073741844_1020, blk_1073741845_1021, blk_1073741846_1022, blk_1073741847_1023, blk_1073741848_1024, blk_1073741849_1025, blk_1073741850_1026, blk_1073741851_1027, blk_1073741852_1028, blk_1073741853_1029, blk_1073741854_1030, blk_1073741855_1031, blk_1073741856_1032, blk_1073741857_1033, blk_1073741858_1034, blk_1073741859_1035, blk_1073741860_1036, blk_1073741861_1037, blk_1073741862_1038, blk_1073741863_1039, blk_1073741864_1040, blk_1073741865_1041, blk_1073741866_1042, blk_1073741867_1043, blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]}]
2019-01-27 12:36:04,131 INFO BlockStateChange: BLOCK* BlockManager: ask 172.18.0.3:50010 to delete [blk_1073741825_1001, blk_1073741826_1002, blk_1073741827_1003, blk_1073741828_1004, blk_1073741829_1005, blk_1073741830_1006, blk_1073741831_1007, blk_1073741832_1008, blk_1073741833_1009, blk_1073741834_1010, blk_1073741835_1011, blk_1073741836_1012, blk_1073741837_1013, blk_1073741838_1014, blk_1073741839_1015, blk_1073741840_1016, blk_1073741841_1017, blk_1073741842_1018, blk_1073741843_1019, blk_1073741844_1020, blk_1073741845_1021, blk_1073741846_1022, blk_1073741847_1023, blk_1073741848_1024, blk_1073741849_1025, blk_1073741850_1026, blk_1073741851_1027, blk_1073741852_1028, blk_1073741853_1029, blk_1073741854_1030, blk_1073741855_1031, blk_1073741856_1032, blk_1073741857_1033, blk_1073741858_1034, blk_1073741859_1035, blk_1073741860_1036, blk_1073741861_1037, blk_1073741862_1038, blk_1073741863_1039, blk_1073741864_1040, blk_1073741865_1041, blk_1073741866_1042, blk_1073741867_1043, blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]}]
2019-01-27 12:36:18,824 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741869_1045{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:36:19,554 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741869_1045{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:36:19,556 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741869_1045{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:36:19,557 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741870_1046{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:36:20,198 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741870_1046{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:36:20,201 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741870_1046{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:36:20,202 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:36:21,390 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:36:21,393 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741872_1048{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:36:21,401 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741871_1047 size 134217728
2019-01-27 12:36:23,493 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741872_1048{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:36:23,494 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741872_1048{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:36:23,496 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741873_1049{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:36:25,375 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741873_1049{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:36:25,377 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741874_1050{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:36:25,379 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741873_1049 size 134217728
2019-01-27 12:36:27,261 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741874_1050{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:36:27,263 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741875_1051{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:36:27,269 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741874_1050 size 134217728
2019-01-27 12:36:27,956 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741875_1051{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:36:27,959 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741876_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:36:27,959 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741875_1051 size 134217728
2019-01-27 12:36:30,204 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741876_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:36:30,204 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741876_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:36:30,207 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741877_1053{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:36:33,363 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741877_1053{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:36:33,364 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741877_1053{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:36:33,366 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741878_1054{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:36:43,126 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741878_1054{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:36:43,127 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741878_1054{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:36:43,128 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741879_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:36:53,356 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741879_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:36:53,357 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741879_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:36:53,359 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741880_1056{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:37:01,577 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741880_1056{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:37:01,578 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741880_1056{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:37:01,579 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741881_1057{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:37:01,580 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 208 Total time for transactions(ms): 51 Number of transactions batched in Syncs: 0 Number of syncs: 89 SyncTimes(ms): 4104 
2019-01-27 12:37:07,756 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741881_1057{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:37:07,757 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741881_1057{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:37:07,759 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741882_1058{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:37:15,857 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741882_1058{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:37:15,858 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741882_1058{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:37:15,860 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741883_1059{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:37:23,218 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741883_1059{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:37:23,219 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741883_1059{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:37:23,221 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741884_1060{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:37:30,697 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741884_1060{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:37:30,697 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741884_1060{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:37:30,699 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741885_1061{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:37:38,436 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741885_1061{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:37:38,437 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741885_1061{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:37:38,439 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741886_1062{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:37:46,218 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741886_1062{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:37:46,219 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741886_1062{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:37:46,220 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741887_1063{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:37:53,996 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741887_1063{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:37:53,997 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741887_1063{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:37:53,999 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741888_1064{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:38:01,597 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741888_1064{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:38:01,599 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741888_1064{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:38:01,646 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741889_1065{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:38:01,646 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 232 Total time for transactions(ms): 54 Number of transactions batched in Syncs: 0 Number of syncs: 97 SyncTimes(ms): 4752 
2019-01-27 12:38:09,057 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741889_1065{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:38:09,058 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741889_1065{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:38:09,060 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741890_1066{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:38:16,448 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741890_1066{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:38:16,449 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741890_1066{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:38:16,451 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741891_1067{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:38:21,571 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741891_1067{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:38:21,572 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741891_1067{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:38:21,574 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741892_1068{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:38:26,988 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741892_1068{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:38:26,988 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741892_1068{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:38:26,990 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741893_1069{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:38:33,598 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741893_1069{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:38:33,599 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741893_1069{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:38:33,600 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741894_1070{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:38:41,048 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741894_1070{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:38:41,051 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741895_1071{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:38:41,056 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741894_1070 size 134217728
2019-01-27 12:38:47,487 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741895_1071{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:38:47,488 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741895_1071{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:38:47,519 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741896_1072{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:38:54,807 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741896_1072{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:38:54,808 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741896_1072{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:38:54,809 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741897_1073{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:39:02,526 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741897_1073{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:39:02,526 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741897_1073{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:39:02,528 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741898_1074{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:39:02,529 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 259 Total time for transactions(ms): 55 Number of transactions batched in Syncs: 0 Number of syncs: 106 SyncTimes(ms): 5604 
2019-01-27 12:39:10,737 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741898_1074{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:39:10,737 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741898_1074{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:39:10,739 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741899_1075{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:39:19,779 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741899_1075{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:39:19,780 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741899_1075{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:39:19,782 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741900_1076{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:39:27,688 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741900_1076{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:39:27,688 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741900_1076{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:39:27,691 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741901_1077{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:39:34,767 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741901_1077{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:39:34,768 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741901_1077{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:39:34,769 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741902_1078{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:39:41,488 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741902_1078{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:39:41,489 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741902_1078{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:39:41,490 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741903_1079{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:39:48,430 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741903_1079{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:39:48,431 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741903_1079{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:39:48,433 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741904_1080{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:39:53,165 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741904_1080{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:39:53,166 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741904_1080{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:39:53,169 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741905_1081{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:39:58,068 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741905_1081{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:39:58,068 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741905_1081{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:39:58,070 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741906_1082{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:40:03,907 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741906_1082{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:40:03,908 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741906_1082{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:40:03,909 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741907_1083{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:40:03,909 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 286 Total time for transactions(ms): 55 Number of transactions batched in Syncs: 0 Number of syncs: 115 SyncTimes(ms): 6403 
2019-01-27 12:40:09,277 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741907_1083{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:40:09,278 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741907_1083{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:40:09,280 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741908_1084{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:40:15,691 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741908_1084{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:40:15,692 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741908_1084{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:40:15,694 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741909_1085{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:40:23,668 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741909_1085{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:40:23,669 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741909_1085{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:40:23,671 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741910_1086{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:40:31,680 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741910_1086{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:40:31,681 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741910_1086{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:40:31,682 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741911_1087{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:40:39,219 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741911_1087{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:40:39,221 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741911_1087{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:40:39,223 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741912_1088{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:40:39,796 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741912_1088{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:40:39,797 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741912_1088{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:40:39,799 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741913_1089{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:40:40,390 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741913_1089{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:40:40,395 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741913_1089{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:40:40,395 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741914_1090{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:40:41,190 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741914_1090{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:40:41,191 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741914_1090{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:40:41,192 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741915_1091{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:40:43,281 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741915_1091{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:40:43,282 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741915_1091{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:40:43,284 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741916_1092{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:40:45,987 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741916_1092{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:40:45,988 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741916_1092{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:40:45,989 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741917_1093{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:40:49,946 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741917_1093{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:40:49,947 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741917_1093{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:40:49,949 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741918_1094{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:40:53,610 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741918_1094{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:40:53,610 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741918_1094{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:40:53,612 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741919_1095{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:40:57,248 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741919_1095{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:40:57,250 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741919_1095{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:40:57,251 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741920_1096{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv
2019-01-27 12:41:00,996 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741920_1096{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:41:00,997 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741920_1096{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:41:01,218 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/books/library-collection-inventory-preprocessed.tsv is closed by DFSClient_NONMAPREDUCE_840964126_1
2019-01-27 12:41:09,518 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 328 Total time for transactions(ms): 56 Number of transactions batched in Syncs: 0 Number of syncs: 131 SyncTimes(ms): 8227 
2019-01-27 12:41:10,680 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741921_1097{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /tmp/hive/root/94849c61-99d7-41ea-b3d5-11d303a657a7/hive_2019-01-27_12-41-09_601_8214468303066055363-2/-mr-10004/290721e5-5fe5-44c0-9a42-d9c7626292f0/map.xml
2019-01-27 12:41:10,693 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741921_1097{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:41:10,694 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741921_1097{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:41:10,699 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/94849c61-99d7-41ea-b3d5-11d303a657a7/hive_2019-01-27_12-41-09_601_8214468303066055363-2/-mr-10004/290721e5-5fe5-44c0-9a42-d9c7626292f0/map.xml is closed by DFSClient_NONMAPREDUCE_840964126_1
2019-01-27 12:41:10,710 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 2 to 10 for /tmp/hive/root/94849c61-99d7-41ea-b3d5-11d303a657a7/hive_2019-01-27_12-41-09_601_8214468303066055363-2/-mr-10004/290721e5-5fe5-44c0-9a42-d9c7626292f0/map.xml
2019-01-27 12:41:11,165 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 172.18.0.4:51502 Call#243 Retry#0: java.io.FileNotFoundException: File does not exist: /tmp/hive/root/94849c61-99d7-41ea-b3d5-11d303a657a7/hive_2019-01-27_12-41-09_601_8214468303066055363-2/-mr-10004/290721e5-5fe5-44c0-9a42-d9c7626292f0/reduce.xml
2019-01-27 12:41:11,339 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741922_1098{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1548592229700_0001/job.jar
2019-01-27 12:41:12,742 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741922_1098{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:41:12,743 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741922_1098{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:41:12,864 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1548592229700_0001/job.jar is closed by DFSClient_NONMAPREDUCE_840964126_1
2019-01-27 12:41:12,867 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 2 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1548592229700_0001/job.jar
2019-01-27 12:41:13,751 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 2 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1548592229700_0001/job.split
2019-01-27 12:41:13,844 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741923_1099{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1548592229700_0001/job.split
2019-01-27 12:41:13,960 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741923_1099{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:41:13,961 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741923_1099{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:41:14,081 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1548592229700_0001/job.split is closed by DFSClient_NONMAPREDUCE_840964126_1
2019-01-27 12:41:14,242 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741924_1100{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1548592229700_0001/job.splitmetainfo
2019-01-27 12:41:14,337 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741924_1100{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:41:14,338 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741924_1100{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:41:14,445 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1548592229700_0001/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_840964126_1
2019-01-27 12:41:14,717 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741925_1101{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1548592229700_0001/job.xml
2019-01-27 12:41:14,869 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741925_1101{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:41:14,870 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741925_1101{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:41:14,979 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1548592229700_0001/job.xml is closed by DFSClient_NONMAPREDUCE_840964126_1
2019-01-27 12:41:28,221 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741926_1102{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1548592229700_0001/job_1548592229700_0001_1_conf.xml
2019-01-27 12:41:28,303 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741926_1102{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:41:28,304 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741926_1102{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:41:28,310 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1548592229700_0001/job_1548592229700_0001_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-254055627_1
2019-01-27 12:41:47,382 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741927_1103{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000011_0
2019-01-27 12:41:47,508 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741928_1104{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000005_0
2019-01-27 12:41:47,773 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741929_1105{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000003_0
2019-01-27 12:41:47,955 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741930_1106{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000001_0
2019-01-27 12:41:48,060 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741931_1107{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000009_0
2019-01-27 12:41:49,545 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741932_1108{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000007_0
2019-01-27 12:41:50,424 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741933_1109{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000011_0
2019-01-27 12:41:50,447 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741934_1110{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000011_0
2019-01-27 12:41:50,535 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741935_1111{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000005_0
2019-01-27 12:41:50,541 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741936_1112{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000003_0
2019-01-27 12:41:50,682 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741937_1113{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000011_0
2019-01-27 12:41:50,833 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741938_1114{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000011_0
2019-01-27 12:41:50,842 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741939_1115{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000001_0
2019-01-27 12:41:50,866 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741940_1116{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000011_0
2019-01-27 12:41:50,922 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741941_1117{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000001_0
2019-01-27 12:41:51,093 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741942_1118{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000009_0
2019-01-27 12:41:51,103 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741943_1119{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000011_0
2019-01-27 12:41:51,111 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741944_1120{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000005_0
2019-01-27 12:41:51,126 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741945_1121{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000005_0
2019-01-27 12:41:51,134 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741946_1122{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000005_0
2019-01-27 12:41:51,173 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741947_1123{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000003_0
2019-01-27 12:41:51,189 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741948_1124{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000011_0
2019-01-27 12:41:51,200 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741949_1125{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000009_0
2019-01-27 12:41:51,231 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741950_1126{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000009_0
2019-01-27 12:41:51,282 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741951_1127{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000005_0
2019-01-27 12:41:51,336 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741952_1128{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000003_0
2019-01-27 12:41:51,373 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741953_1129{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000009_0
2019-01-27 12:41:51,380 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741954_1130{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000005_0
2019-01-27 12:41:51,435 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741955_1131{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000005_0
2019-01-27 12:41:51,454 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741956_1132{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000011_0
2019-01-27 12:41:51,484 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741957_1133{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000009_0
2019-01-27 12:41:51,501 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741958_1134{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000009_0
2019-01-27 12:41:51,525 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741959_1135{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000003_0
2019-01-27 12:41:51,626 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741960_1136{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000011_0
2019-01-27 12:41:51,632 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741961_1137{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000005_0
2019-01-27 12:41:51,650 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741962_1138{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000005_0
2019-01-27 12:41:51,661 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741963_1139{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000001_0
2019-01-27 12:41:51,695 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741964_1140{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000003_0
2019-01-27 12:41:51,736 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741965_1141{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000011_0
2019-01-27 12:41:51,755 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741966_1142{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000005_0
2019-01-27 12:41:51,761 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741967_1143{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000001_0
2019-01-27 12:41:51,761 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741968_1144{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000001_0
2019-01-27 12:41:51,775 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741969_1145{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000003_0
2019-01-27 12:41:51,780 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741970_1146{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000003_0
2019-01-27 12:41:51,797 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741971_1147{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000005_0
2019-01-27 12:41:51,799 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741972_1148{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000011_0
2019-01-27 12:41:51,800 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741973_1149{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000005_0
2019-01-27 12:41:51,812 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741974_1150{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000009_0
2019-01-27 12:41:51,836 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741975_1151{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000001_0
2019-01-27 12:41:51,859 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741976_1152{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000009_0
2019-01-27 12:41:51,880 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741977_1153{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000011_0
2019-01-27 12:41:51,881 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741978_1154{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000001_0
2019-01-27 12:41:51,934 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741979_1155{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000001_0
2019-01-27 12:41:51,960 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741980_1156{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000003_0
2019-01-27 12:41:52,004 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741981_1157{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000005_0
2019-01-27 12:41:52,025 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741982_1158{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000003_0
2019-01-27 12:41:52,030 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741983_1159{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000001_0
2019-01-27 12:41:52,064 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741984_1160{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000001_0
2019-01-27 12:41:52,088 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741985_1161{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000011_0
2019-01-27 12:41:52,095 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741986_1162{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000009_0
2019-01-27 12:41:52,125 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741987_1163{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000005_0
2019-01-27 12:41:52,128 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741988_1164{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000009_0
2019-01-27 12:41:52,161 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741989_1165{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000001_0
2019-01-27 12:41:52,199 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741990_1166{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000005_0
2019-01-27 12:41:52,254 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741991_1167{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000003_0
2019-01-27 12:41:52,301 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741992_1168{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000011_0
2019-01-27 12:41:52,301 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741993_1169{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000011_0
2019-01-27 12:41:52,315 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741994_1170{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000009_0
2019-01-27 12:41:52,334 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741995_1171{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000011_0
2019-01-27 12:41:52,356 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741996_1172{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000009_0
2019-01-27 12:41:52,411 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741997_1173{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000005_0
2019-01-27 12:41:52,420 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741998_1174{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000001_0
2019-01-27 12:41:52,421 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741999_1175{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000001_0
2019-01-27 12:41:52,446 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742000_1176{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000011_0
2019-01-27 12:41:52,459 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742001_1177{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000007_0
2019-01-27 12:41:52,462 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742002_1178{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000007_0
2019-01-27 12:41:52,515 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742003_1179{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000007_0
2019-01-27 12:41:52,536 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742004_1180{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000001_0
2019-01-27 12:41:52,541 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742005_1181{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000003_0
2019-01-27 12:41:52,549 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742006_1182{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000003_0
2019-01-27 12:41:52,602 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742007_1183{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000009_0
2019-01-27 12:41:52,611 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742008_1184{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000007_0
2019-01-27 12:41:52,628 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742009_1185{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000003_0
2019-01-27 12:41:52,655 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742010_1186{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000009_0
2019-01-27 12:41:52,723 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742011_1187{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000005_0
2019-01-27 12:41:52,771 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742012_1188{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000001_0
2019-01-27 12:41:52,897 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742013_1189{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000005_0
2019-01-27 12:41:52,899 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742014_1190{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000009_0
2019-01-27 12:41:52,931 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742015_1191{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000001_0
2019-01-27 12:41:52,975 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742016_1192{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000003_0
2019-01-27 12:41:52,987 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742017_1193{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000007_0
2019-01-27 12:41:52,990 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742018_1194{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000005_0
2019-01-27 12:41:53,027 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742019_1195{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000011_0
2019-01-27 12:41:53,057 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742020_1196{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000005_0
2019-01-27 12:41:53,126 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742021_1197{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000007_0
2019-01-27 12:41:53,130 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742022_1198{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000007_0
2019-01-27 12:41:53,163 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742023_1199{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000011_0
2019-01-27 12:41:53,172 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742024_1200{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000011_0
2019-01-27 12:41:53,235 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742025_1201{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000001_0
2019-01-27 12:41:53,248 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742026_1202{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000009_0
2019-01-27 12:41:53,254 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742027_1203{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000005_0
2019-01-27 12:41:53,282 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742028_1204{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000011_0
2019-01-27 12:41:53,347 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742029_1205{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000007_0
2019-01-27 12:41:53,407 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742030_1206{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000003_0
2019-01-27 12:41:53,409 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742031_1207{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000009_0
2019-01-27 12:41:53,616 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742032_1208{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000011_0
2019-01-27 12:41:53,634 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742033_1209{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000003_0
2019-01-27 12:41:53,636 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742034_1210{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000005_0
2019-01-27 12:41:53,654 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742035_1211{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000005_0
2019-01-27 12:41:53,657 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742036_1212{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000003_0
2019-01-27 12:41:53,682 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742037_1213{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000003_0
2019-01-27 12:41:53,725 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742038_1214{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000007_0
2019-01-27 12:41:53,739 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742039_1215{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000007_0
2019-01-27 12:41:53,762 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742040_1216{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000009_0
2019-01-27 12:41:53,769 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742041_1217{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000007_0
2019-01-27 12:41:53,786 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742042_1218{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000011_0
2019-01-27 12:41:53,833 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742043_1219{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000001_0
2019-01-27 12:41:53,894 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742044_1220{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000001_0
2019-01-27 12:41:53,910 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742045_1221{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000011_0
2019-01-27 12:41:53,933 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742046_1222{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000011_0
2019-01-27 12:41:53,964 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742047_1223{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000009_0
2019-01-27 12:41:53,981 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742048_1224{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000009_0
2019-01-27 12:41:54,069 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742049_1225{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000007_0
2019-01-27 12:41:54,087 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742050_1226{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000005_0
2019-01-27 12:41:54,099 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742051_1227{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000011_0
2019-01-27 12:41:54,154 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742052_1228{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000007_0
2019-01-27 12:41:54,164 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742053_1229{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000003_0
2019-01-27 12:41:54,177 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742054_1230{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000005_0
2019-01-27 12:41:54,204 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742055_1231{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000001_0
2019-01-27 12:41:54,230 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742056_1232{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000001_0
2019-01-27 12:41:54,234 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742057_1233{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000003_0
2019-01-27 12:41:54,235 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742058_1234{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000009_0
2019-01-27 12:41:54,310 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742059_1235{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000003_0
2019-01-27 12:41:54,331 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742060_1236{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000001_0
2019-01-27 12:41:54,369 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742061_1237{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000007_0
2019-01-27 12:41:54,406 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742062_1238{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000009_0
2019-01-27 12:41:54,460 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742063_1239{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000005_0
2019-01-27 12:41:54,527 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742064_1240{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000001_0
2019-01-27 12:41:54,535 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742065_1241{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000001_0
2019-01-27 12:41:54,559 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742066_1242{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000007_0
2019-01-27 12:41:54,590 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742067_1243{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000001_0
2019-01-27 12:41:54,595 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742068_1244{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000003_0
2019-01-27 12:41:54,712 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742069_1245{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000009_0
2019-01-27 12:41:54,723 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742070_1246{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000001_0
2019-01-27 12:41:54,738 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742071_1247{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000007_0
2019-01-27 12:41:54,841 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742072_1248{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000009_0
2019-01-27 12:41:54,845 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742073_1249{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000003_0
2019-01-27 12:41:54,897 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742074_1250{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000009_0
2019-01-27 12:41:55,009 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742075_1251{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000007_0
2019-01-27 12:41:55,165 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742076_1252{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000009_0
2019-01-27 12:41:55,204 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742077_1253{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000003_0
2019-01-27 12:41:55,367 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742078_1254{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000007_0
2019-01-27 12:41:55,483 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742079_1255{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000011_0
2019-01-27 12:41:55,488 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742080_1256{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000003_0
2019-01-27 12:41:55,493 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742081_1257{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000007_0
2019-01-27 12:41:55,549 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742082_1258{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000003_0
2019-01-27 12:41:55,689 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742083_1259{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000007_0
2019-01-27 12:41:55,740 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742084_1260{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000007_0
2019-01-27 12:41:55,749 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742085_1261{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000007_0
2019-01-27 12:41:55,994 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742086_1262{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000001_0
2019-01-27 12:41:56,100 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742087_1263{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000007_0
2019-01-27 12:41:56,321 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742088_1264{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000007_0
2019-01-27 12:41:56,369 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742089_1265{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000007_0
2019-01-27 12:41:56,428 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742090_1266{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000007_0
2019-01-27 12:41:56,499 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742091_1267{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000004_0
2019-01-27 12:41:56,687 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742092_1268{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000005_0
2019-01-27 12:41:56,767 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742093_1269{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000009_0
2019-01-27 12:41:57,211 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742094_1270{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000003_0
2019-01-27 12:41:57,340 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742095_1271{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000012_0
2019-01-27 12:41:57,911 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742096_1272{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000014_0
2019-01-27 12:41:57,936 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742097_1273{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000007_0
2019-01-27 12:41:58,479 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742098_1274{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000008_0
2019-01-27 12:41:58,929 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742099_1275{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000004_0
2019-01-27 12:41:58,944 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742100_1276{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000004_0
2019-01-27 12:41:59,046 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742101_1277{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000004_0
2019-01-27 12:41:59,096 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742102_1278{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000004_0
2019-01-27 12:41:59,463 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742103_1279{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000004_0
2019-01-27 12:41:59,602 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742104_1280{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000004_0
2019-01-27 12:41:59,629 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742105_1281{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000004_0
2019-01-27 12:41:59,717 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742106_1282{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000004_0
2019-01-27 12:41:59,841 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742107_1283{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000012_0
2019-01-27 12:41:59,866 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742108_1284{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000004_0
2019-01-27 12:41:59,867 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742109_1285{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000004_0
2019-01-27 12:41:59,947 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742110_1286{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000000_0
2019-01-27 12:42:00,010 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742111_1287{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000014_0
2019-01-27 12:42:00,169 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742112_1288{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000004_0
2019-01-27 12:42:00,206 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742113_1289{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000002_0
2019-01-27 12:42:00,221 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742114_1290{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000004_0
2019-01-27 12:42:00,251 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742115_1291{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000012_0
2019-01-27 12:42:00,255 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742116_1292{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000004_0
2019-01-27 12:42:00,305 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742117_1293{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000004_0
2019-01-27 12:42:00,635 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742118_1294{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000004_0
2019-01-27 12:42:00,675 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742119_1295{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000008_0
2019-01-27 12:42:00,733 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742120_1296{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000004_0
2019-01-27 12:42:00,796 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742121_1297{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000014_0
2019-01-27 12:42:00,828 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742122_1298{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000004_0
2019-01-27 12:42:00,903 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742123_1299{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000008_0
2019-01-27 12:42:00,950 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742124_1300{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000012_0
2019-01-27 12:42:00,981 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742125_1301{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000008_0
2019-01-27 12:42:01,001 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742126_1302{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000014_0
2019-01-27 12:42:01,015 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742127_1303{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000008_0
2019-01-27 12:42:01,075 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742128_1304{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000014_0
2019-01-27 12:42:01,088 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742129_1305{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000014_0
2019-01-27 12:42:01,167 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742130_1306{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000012_0
2019-01-27 12:42:01,170 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742131_1307{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000012_0
2019-01-27 12:42:01,395 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742132_1308{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000012_0
2019-01-27 12:42:01,427 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742133_1309{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000014_0
2019-01-27 12:42:01,466 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742134_1310{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000008_0
2019-01-27 12:42:01,503 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742135_1311{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000008_0
2019-01-27 12:42:01,525 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742136_1312{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000008_0
2019-01-27 12:42:01,556 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742137_1313{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000012_0
2019-01-27 12:42:01,696 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742138_1314{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000004_0
2019-01-27 12:42:01,718 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742139_1315{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000008_0
2019-01-27 12:42:01,727 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742140_1316{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000010_0
2019-01-27 12:42:01,746 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742141_1317{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000004_0
2019-01-27 12:42:01,746 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742142_1318{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000012_0
2019-01-27 12:42:01,795 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742143_1319{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000012_0
2019-01-27 12:42:01,800 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742144_1320{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000012_0
2019-01-27 12:42:01,802 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742145_1321{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000004_0
2019-01-27 12:42:01,884 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742146_1322{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000014_0
2019-01-27 12:42:01,887 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742147_1323{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000012_0
2019-01-27 12:42:01,913 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742148_1324{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000004_0
2019-01-27 12:42:01,945 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742149_1325{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000014_0
2019-01-27 12:42:01,976 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742150_1326{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000012_0
2019-01-27 12:42:02,017 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742151_1327{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000012_0
2019-01-27 12:42:02,017 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742152_1328{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000014_0
2019-01-27 12:42:02,023 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742153_1329{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000012_0
2019-01-27 12:42:02,096 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742154_1330{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000014_0
2019-01-27 12:42:02,216 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742155_1331{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000012_0
2019-01-27 12:42:02,242 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742156_1332{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000008_0
2019-01-27 12:42:02,244 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742157_1333{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000008_0
2019-01-27 12:42:02,300 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742158_1334{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000014_0
2019-01-27 12:42:02,370 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742159_1335{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000014_0
2019-01-27 12:42:02,396 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742160_1336{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000008_0
2019-01-27 12:42:02,471 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742161_1337{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000008_0
2019-01-27 12:42:02,500 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742162_1338{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000014_0
2019-01-27 12:42:02,522 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742163_1339{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000004_0
2019-01-27 12:42:02,636 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742164_1340{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000008_0
2019-01-27 12:42:02,638 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742165_1341{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000014_0
2019-01-27 12:42:02,722 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742166_1342{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000012_0
2019-01-27 12:42:02,885 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742167_1343{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000004_0
2019-01-27 12:42:02,962 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742168_1344{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000008_0
2019-01-27 12:42:02,992 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742169_1345{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000014_0
2019-01-27 12:42:03,066 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742170_1346{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000004_0
2019-01-27 12:42:03,109 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742171_1347{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000000_0
2019-01-27 12:42:03,129 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742172_1348{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000004_0
2019-01-27 12:42:03,143 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742173_1349{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000004_0
2019-01-27 12:42:03,168 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742174_1350{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000012_0
2019-01-27 12:42:03,170 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742175_1351{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000012_0
2019-01-27 12:42:03,445 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742176_1352{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000012_0
2019-01-27 12:42:03,465 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742177_1353{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000012_0
2019-01-27 12:42:03,497 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742178_1354{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000012_0
2019-01-27 12:42:03,527 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742179_1355{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000012_0
2019-01-27 12:42:03,530 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742180_1356{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000008_0
2019-01-27 12:42:03,564 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742181_1357{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000000_0
2019-01-27 12:42:03,784 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742182_1358{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000012_0
2019-01-27 12:42:03,797 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742183_1359{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000012_0
2019-01-27 12:42:03,809 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742184_1360{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000012_0
2019-01-27 12:42:03,814 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742185_1361{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000012_0
2019-01-27 12:42:03,815 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742186_1362{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000010_0
2019-01-27 12:42:03,873 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742187_1363{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000010_0
2019-01-27 12:42:03,995 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742188_1364{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000006_0
2019-01-27 12:42:04,216 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742189_1365{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000000_0
2019-01-27 12:42:04,248 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742190_1366{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000008_0
2019-01-27 12:42:04,393 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742191_1367{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000002_0
2019-01-27 12:42:04,407 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742192_1368{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000014_0
2019-01-27 12:42:04,445 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742193_1369{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000010_0
2019-01-27 12:42:04,522 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742194_1370{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000014_0
2019-01-27 12:42:04,562 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742195_1371{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000014_0
2019-01-27 12:42:04,563 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742196_1372{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000014_0
2019-01-27 12:42:04,567 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742197_1373{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000002_0
2019-01-27 12:42:04,581 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742198_1374{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000002_0
2019-01-27 12:42:04,637 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742199_1375{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000014_0
2019-01-27 12:42:04,652 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742200_1376{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000014_0
2019-01-27 12:42:04,665 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742201_1377{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000000_0
2019-01-27 12:42:04,729 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742202_1378{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000002_0
2019-01-27 12:42:04,774 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742203_1379{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000014_0
2019-01-27 12:42:04,785 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742204_1380{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000014_0
2019-01-27 12:42:04,820 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742205_1381{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000014_0
2019-01-27 12:42:04,967 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742206_1382{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000014_0
2019-01-27 12:42:04,995 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742207_1383{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000014_0
2019-01-27 12:42:05,072 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742208_1384{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000002_0
2019-01-27 12:42:05,246 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742209_1385{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000000_0
2019-01-27 12:42:05,263 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742210_1386{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000000_0
2019-01-27 12:42:05,263 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742211_1387{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000000_0
2019-01-27 12:42:05,263 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742212_1388{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000000_0
2019-01-27 12:42:05,264 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742213_1389{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000000_0
2019-01-27 12:42:05,335 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742214_1390{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000010_0
2019-01-27 12:42:05,402 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742215_1391{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000002_0
2019-01-27 12:42:05,406 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742216_1392{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000002_0
2019-01-27 12:42:05,486 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742217_1393{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000000_0
2019-01-27 12:42:05,488 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742218_1394{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000000_0
2019-01-27 12:42:05,508 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742219_1395{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000000_0
2019-01-27 12:42:05,508 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742220_1396{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000000_0
2019-01-27 12:42:05,528 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742221_1397{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000002_0
2019-01-27 12:42:05,676 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742222_1398{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000010_0
2019-01-27 12:42:05,677 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742223_1399{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000010_0
2019-01-27 12:42:05,758 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742224_1400{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000002_0
2019-01-27 12:42:05,996 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742225_1401{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000002_0
2019-01-27 12:42:06,007 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742226_1402{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000002_0
2019-01-27 12:42:06,248 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742227_1403{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000010_0
2019-01-27 12:42:06,254 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742228_1404{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000010_0
2019-01-27 12:42:06,357 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742229_1405{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000004_0
2019-01-27 12:42:06,478 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742230_1406{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000010_0
2019-01-27 12:42:06,575 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742231_1407{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000002_0
2019-01-27 12:42:06,586 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742232_1408{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000002_0
2019-01-27 12:42:06,699 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742233_1409{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000010_0
2019-01-27 12:42:06,922 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742234_1410{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000002_0
2019-01-27 12:42:07,153 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742235_1411{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000006_0
2019-01-27 12:42:07,163 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742236_1412{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000002_0
2019-01-27 12:42:07,167 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742237_1413{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000006_0
2019-01-27 12:42:07,182 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742238_1414{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000006_0
2019-01-27 12:42:07,183 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742239_1415{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000006_0
2019-01-27 12:42:07,377 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742240_1416{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000008_0
2019-01-27 12:42:07,389 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742241_1417{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000000_0
2019-01-27 12:42:07,389 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742242_1418{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000000_0
2019-01-27 12:42:07,529 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742243_1419{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000008_0
2019-01-27 12:42:07,686 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742244_1420{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000008_0
2019-01-27 12:42:07,686 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742245_1421{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000008_0
2019-01-27 12:42:07,692 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:07,696 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:07,697 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:07,697 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742246_1422{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000008_0
2019-01-27 12:42:07,714 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:07,714 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:07,714 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:07,714 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742247_1423{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000010_0
2019-01-27 12:42:07,715 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:07,715 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:07,716 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:07,716 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742248_1424{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000010_0
2019-01-27 12:42:07,716 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:07,716 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:07,716 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:07,717 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742249_1425{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000010_0
2019-01-27 12:42:07,717 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:07,717 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:07,717 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:07,717 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742250_1426{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000010_0
2019-01-27 12:42:07,733 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:07,733 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:07,734 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:07,734 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742251_1427{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000008_0
2019-01-27 12:42:07,755 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:07,755 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:07,756 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:07,756 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742252_1428{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000008_0
2019-01-27 12:42:07,766 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:07,766 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:07,766 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:07,766 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742253_1429{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000008_0
2019-01-27 12:42:07,776 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:07,777 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:07,777 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:07,777 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742254_1430{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000008_0
2019-01-27 12:42:07,855 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:07,855 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:07,855 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:07,855 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:07,855 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:07,856 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:07,856 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742255_1431{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000000_0
2019-01-27 12:42:07,856 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742256_1432{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000008_0
2019-01-27 12:42:07,939 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:07,939 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:07,939 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:07,940 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742257_1433{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000010_0
2019-01-27 12:42:07,956 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:07,956 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:07,956 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:07,956 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742258_1434{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000002_0
2019-01-27 12:42:08,103 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:08,103 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:08,103 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:08,103 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742259_1435{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000006_0
2019-01-27 12:42:08,294 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:08,294 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:08,294 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:08,294 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742260_1436{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000000_0
2019-01-27 12:42:08,298 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:08,298 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:08,298 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:08,299 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742261_1437{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000002_0
2019-01-27 12:42:08,439 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:08,439 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:08,439 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:08,439 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742262_1438{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000006_0
2019-01-27 12:42:08,635 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:08,635 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:08,635 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:08,635 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742263_1439{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000006_0
2019-01-27 12:42:08,635 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:08,635 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:08,635 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:08,635 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:08,635 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:08,636 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742264_1440{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000006_0
2019-01-27 12:42:08,636 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:08,636 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:08,636 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:08,636 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742265_1441{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000006_0
2019-01-27 12:42:08,636 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:08,636 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742266_1442{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000006_0
2019-01-27 12:42:08,639 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:08,639 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:08,639 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:08,639 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742267_1443{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000006_0
2019-01-27 12:42:08,639 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:08,639 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:08,639 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:08,640 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:08,640 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:08,640 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:08,640 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742268_1444{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000006_0
2019-01-27 12:42:08,640 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:08,640 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:08,640 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:08,641 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742269_1445{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000006_0
2019-01-27 12:42:08,641 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742270_1446{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000006_0
2019-01-27 12:42:08,976 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:08,976 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:08,976 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:08,977 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:51012 Call#115 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000002_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:09,427 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:09,427 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:09,427 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:09,428 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:51008 Call#114 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000006_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:09,430 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:09,430 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:09,430 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:09,430 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:51008 Call#113 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000006_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:09,453 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:09,453 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:09,453 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:09,453 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:51008 Call#115 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000006_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:09,515 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:09,515 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:09,516 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:09,516 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:51008 Call#116 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000006_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:09,530 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:09,530 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:09,530 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:09,530 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:51012 Call#116 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000002_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:09,551 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:09,551 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:09,551 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:09,551 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:51012 Call#117 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000002_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:09,729 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:09,730 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:09,730 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:09,730 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:50910 Call#129 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000014_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:09,806 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:09,806 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:09,806 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:09,806 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:50972 Call#115 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000000_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:09,808 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:09,808 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:09,808 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:09,808 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:51008 Call#117 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000006_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:10,150 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:10,151 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:10,151 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:10,151 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:50936 Call#128 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000012_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:10,224 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:10,228 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:10,259 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:10,252 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:10,265 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:10,265 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:10,265 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:10,265 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:10,265 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:50972 Call#118 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000000_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:10,265 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:10,265 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:10,266 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:50972 Call#119 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000000_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:10,265 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:10,266 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:50972 Call#117 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000000_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:10,266 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:10,288 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:50972 Call#116 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000000_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:10,545 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742142_1318{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:10,545 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742142_1318{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:10,977 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 1869 Total time for transactions(ms): 164 Number of transactions batched in Syncs: 224 Number of syncs: 712 SyncTimes(ms): 24738 
2019-01-27 12:42:11,074 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000012_0 is closed by DFSClient_attempt_1548592229700_0001_m_000012_0_-26464423_1
2019-01-27 12:42:11,158 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742155_1331{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:11,171 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742155_1331{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:11,200 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:11,200 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:11,200 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:11,200 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742271_1447{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000010_0
2019-01-27 12:42:11,359 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000012_0 is closed by DFSClient_attempt_1548592229700_0001_m_000012_0_-26464423_1
2019-01-27 12:42:11,636 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742115_1291{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:11,850 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742115_1291{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:11,856 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742192_1368{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:12,007 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000012_0 is closed by DFSClient_attempt_1548592229700_0001_m_000012_0_-26464423_1
2019-01-27 12:42:12,422 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742192_1368{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:12,425 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:12,425 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:12,425 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:12,425 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742272_1448{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000010_0
2019-01-27 12:42:12,553 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:12,554 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:12,554 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:12,554 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742273_1449{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1548592229700_0001/job_1548592229700_0001_1.jhist
2019-01-27 12:42:12,655 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000014_0 is closed by DFSClient_attempt_1548592229700_0001_m_000014_0_-128812184_1
2019-01-27 12:42:12,799 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742149_1325{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:12,820 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:12,820 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:12,820 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:12,820 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:51004 Call#119 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000010_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:12,829 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742144_1320{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:12,941 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742149_1325 size 471274
2019-01-27 12:42:13,024 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000014_0 is closed by DFSClient_attempt_1548592229700_0001_m_000014_0_-128812184_1
2019-01-27 12:42:13,045 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742144_1320{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:13,084 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:13,084 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:13,084 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:13,084 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:50952 Call#129 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000008_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:13,196 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000012_0 is closed by DFSClient_attempt_1548592229700_0001_m_000012_0_-26464423_1
2019-01-27 12:42:13,396 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742121_1297{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:13,404 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742121_1297 size 714299
2019-01-27 12:42:13,560 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000014_0 is closed by DFSClient_attempt_1548592229700_0001_m_000014_0_-128812184_1
2019-01-27 12:42:13,728 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742158_1334{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:13,728 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742178_1354{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:13,730 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742178_1354{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:13,732 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742158_1334{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:13,883 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000012_0 is closed by DFSClient_attempt_1548592229700_0001_m_000012_0_-26464423_1
2019-01-27 12:42:13,953 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742180_1356{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:13,955 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742180_1356{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:13,966 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742151_1327{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000012_0
2019-01-27 12:42:13,967 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742151_1327{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 465708
2019-01-27 12:42:13,967 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742151_1327 size 465708
2019-01-27 12:42:13,991 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000014_0 is closed by DFSClient_attempt_1548592229700_0001_m_000014_0_-128812184_1
2019-01-27 12:42:14,124 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000008_0 is closed by DFSClient_attempt_1548592229700_0001_m_000008_0_987611707_1
2019-01-27 12:42:14,295 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742162_1338{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:14,298 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742162_1338{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:14,464 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000014_0 is closed by DFSClient_attempt_1548592229700_0001_m_000014_0_-128812184_1
2019-01-27 12:42:14,544 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000012_0 is closed by DFSClient_attempt_1548592229700_0001_m_000012_0_-26464423_1
2019-01-27 12:42:14,625 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742156_1332{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:14,855 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000008_0 is closed by DFSClient_attempt_1548592229700_0001_m_000008_0_987611707_1
2019-01-27 12:42:15,357 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742156_1332 size 608038
2019-01-27 12:42:16,045 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742160_1336{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:16,046 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742195_1371{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:16,158 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742195_1371{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:16,162 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742160_1336{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:16,162 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742176_1352{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:16,405 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000014_0 is closed by DFSClient_attempt_1548592229700_0001_m_000014_0_-128812184_1
2019-01-27 12:42:16,407 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742176_1352 size 231863
2019-01-27 12:42:16,493 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000008_0 is closed by DFSClient_attempt_1548592229700_0001_m_000008_0_987611707_1
2019-01-27 12:42:16,493 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000012_0 is closed by DFSClient_attempt_1548592229700_0001_m_000012_0_-26464423_1
2019-01-27 12:42:16,739 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742184_1360{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:16,926 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742123_1299{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:16,928 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742200_1376{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:16,928 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742236_1412{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:16,953 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/root/.staging/job_1548592229700_0001/job_1548592229700_0001_1.jhist for DFSClient_NONMAPREDUCE_-254055627_1
2019-01-27 12:42:17,278 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742184_1360 size 164587
2019-01-27 12:42:17,387 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742200_1376{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:17,387 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742123_1299 size 969155
2019-01-27 12:42:17,433 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000012_0 is closed by DFSClient_attempt_1548592229700_0001_m_000012_0_-26464423_1
2019-01-27 12:42:17,435 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:17,435 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:17,435 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:17,435 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:50936 Call#141 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000012_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:17,496 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742236_1412{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:17,519 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000014_0 is closed by DFSClient_attempt_1548592229700_0001_m_000014_0_-128812184_1
2019-01-27 12:42:17,519 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000008_0 is closed by DFSClient_attempt_1548592229700_0001_m_000008_0_987611707_1
2019-01-27 12:42:17,659 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000002_0 is closed by DFSClient_attempt_1548592229700_0001_m_000002_0_-1498653074_1
2019-01-27 12:42:18,415 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742217_1393{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:18,417 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742217_1393{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:18,433 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742234_1410{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:18,433 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742234_1410{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:18,453 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742206_1382{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000014_0
2019-01-27 12:42:18,454 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742206_1382{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 150743
2019-01-27 12:42:18,456 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742161_1337{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000008_0
2019-01-27 12:42:18,459 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742161_1337{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 543591
2019-01-27 12:42:18,462 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742161_1337 size 543591
2019-01-27 12:42:18,463 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742206_1382 size 150743
2019-01-27 12:42:18,474 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742248_1424{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:18,587 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000010_0 is closed by DFSClient_attempt_1548592229700_0001_m_000010_0_1133495959_1
2019-01-27 12:42:18,587 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000002_0 is closed by DFSClient_attempt_1548592229700_0001_m_000002_0_-1498653074_1
2019-01-27 12:42:18,588 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000000_0 is closed by DFSClient_attempt_1548592229700_0001_m_000000_0_-536182207_1
2019-01-27 12:42:18,636 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:18,636 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:18,636 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:18,637 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742274_1450{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000010_0
2019-01-27 12:42:18,694 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742225_1401{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:18,698 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742241_1417{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:18,762 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000000_0 is closed by DFSClient_attempt_1548592229700_0001_m_000000_0_-536182207_1
2019-01-27 12:42:18,762 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000002_0 is closed by DFSClient_attempt_1548592229700_0001_m_000002_0_-1498653074_1
2019-01-27 12:42:18,778 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742197_1373{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:18,807 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742219_1395{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:18,819 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742274_1450{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:42:18,883 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742225_1401 size 119813
2019-01-27 12:42:18,884 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742241_1417 size 108238
2019-01-27 12:42:18,891 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000002_0 is closed by DFSClient_attempt_1548592229700_0001_m_000002_0_-1498653074_1
2019-01-27 12:42:18,892 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742197_1373 size 174660
2019-01-27 12:42:18,892 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742219_1395 size 127332
2019-01-27 12:42:18,988 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742231_1407{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:19,009 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000008_0 is closed by DFSClient_attempt_1548592229700_0001_m_000008_0_987611707_1
2019-01-27 12:42:19,009 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000000_0 is closed by DFSClient_attempt_1548592229700_0001_m_000000_0_-536182207_1
2019-01-27 12:42:19,009 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000014_0 is closed by DFSClient_attempt_1548592229700_0001_m_000014_0_-128812184_1
2019-01-27 12:42:19,010 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000010_0 is closed by DFSClient_attempt_1548592229700_0001_m_000010_0_1133495959_1
2019-01-27 12:42:19,010 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742231_1407 size 113245
2019-01-27 12:42:19,015 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:19,015 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:19,015 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:19,016 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742275_1451{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000014_0
2019-01-27 12:42:19,029 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742245_1421{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:19,037 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742181_1357{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:19,037 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742257_1433{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:19,038 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742245_1421{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:19,042 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742181_1357{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:19,225 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000002_0 is closed by DFSClient_attempt_1548592229700_0001_m_000002_0_-1498653074_1
2019-01-27 12:42:19,372 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000010_0 is closed by DFSClient_attempt_1548592229700_0001_m_000010_0_1133495959_1
2019-01-27 12:42:19,372 WARN org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.complete from 172.18.0.2:51004 Call#128 Retry#0: output error
2019-01-27 12:42:19,372 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000 caught an exception
java.nio.channels.ClosedChannelException
	at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:270)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:479)
	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:2583)
	at org.apache.hadoop.ipc.Server.access$1900(Server.java:136)
	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:986)
	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1051)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2104)
2019-01-27 12:42:19,374 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000008_0 is closed by DFSClient_attempt_1548592229700_0001_m_000008_0_987611707_1
2019-01-27 12:42:19,374 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000000_0 is closed by DFSClient_attempt_1548592229700_0001_m_000000_0_-536182207_1
2019-01-27 12:42:19,399 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742275_1451{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:42:19,488 INFO BlockStateChange: BLOCK* ask 172.18.0.3:50010 to replicate blk_1073742248_1424 to datanode(s) 172.18.0.2:50010
2019-01-27 12:42:19,488 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742274_1450 to datanode(s) 172.18.0.3:50010
2019-01-27 12:42:19,488 INFO BlockStateChange: BLOCK* ask 172.18.0.3:50010 to replicate blk_1073742257_1433 to datanode(s) 172.18.0.2:50010
2019-01-27 12:42:19,488 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742275_1451 to datanode(s) 172.18.0.3:50010
2019-01-27 12:42:19,499 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000014_0 is closed by DFSClient_attempt_1548592229700_0001_m_000014_0_-128812184_1
2019-01-27 12:42:19,851 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742220_1396{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:19,851 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742246_1422{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:19,856 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742208_1384{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:19,864 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742220_1396 size 130038
2019-01-27 12:42:19,865 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742208_1384{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:19,955 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000008_0 is closed by DFSClient_attempt_1548592229700_0001_m_000008_0_987611707_1
2019-01-27 12:42:20,042 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000000_0 is closed by DFSClient_attempt_1548592229700_0001_m_000000_0_-536182207_1
2019-01-27 12:42:20,160 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742252_1428{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:20,169 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742165_1341{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:20,190 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:20,190 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:20,190 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:20,191 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742276_1452{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000000_0
2019-01-27 12:42:20,216 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742165_1341 size 386664
2019-01-27 12:42:20,351 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000008_0 is closed by DFSClient_attempt_1548592229700_0001_m_000008_0_987611707_1
2019-01-27 12:42:20,865 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:20,865 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:20,865 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:20,866 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742277_1453{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000008_0
2019-01-27 12:42:21,154 WARN org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:50972 Call#131 Retry#0: output error
2019-01-27 12:42:21,155 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000 caught an exception
java.nio.channels.ClosedChannelException
	at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:270)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:479)
	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:2583)
	at org.apache.hadoop.ipc.Server.access$1900(Server.java:136)
	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:986)
	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1051)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2104)
2019-01-27 12:42:21,155 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000014_0 is closed by DFSClient_attempt_1548592229700_0001_m_000014_0_-128812184_1
2019-01-27 12:42:21,168 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742196_1372{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:21,170 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742196_1372{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:21,280 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742277_1453{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:42:21,351 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000014_0 is closed by DFSClient_attempt_1548592229700_0001_m_000014_0_-128812184_1
2019-01-27 12:42:21,363 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742194_1370{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:21,366 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742194_1370{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:21,435 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000008_0 is closed by DFSClient_attempt_1548592229700_0001_m_000008_0_987611707_1
2019-01-27 12:42:21,444 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:21,444 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:21,444 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:21,444 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742278_1454{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000005_0
2019-01-27 12:42:21,466 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742244_1420{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:21,475 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742244_1420 size 289917
2019-01-27 12:42:21,548 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000014_0 is closed by DFSClient_attempt_1548592229700_0001_m_000014_0_-128812184_1
2019-01-27 12:42:21,755 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000008_0 is closed by DFSClient_attempt_1548592229700_0001_m_000008_0_987611707_1
2019-01-27 12:42:22,003 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742154_1330{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:22,155 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000014_0 is closed by DFSClient_attempt_1548592229700_0001_m_000014_0_-128812184_1
2019-01-27 12:42:22,489 INFO BlockStateChange: BLOCK* ask 172.18.0.3:50010 to replicate blk_1073742246_1422 to datanode(s) 172.18.0.2:50010
2019-01-27 12:42:22,489 INFO BlockStateChange: BLOCK* ask 172.18.0.3:50010 to replicate blk_1073742252_1428 to datanode(s) 172.18.0.2:50010
2019-01-27 12:42:22,489 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742277_1453 to datanode(s) 172.18.0.3:50010
2019-01-27 12:42:22,489 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742154_1330 to datanode(s) 172.18.0.3:50010
2019-01-27 12:42:22,504 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742168_1344{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:22,532 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742168_1344 size 510153
2019-01-27 12:42:22,532 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742275_1451 size 5230
2019-01-27 12:42:22,532 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742154_1330 size 476716
2019-01-27 12:42:22,533 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742274_1450 size 42995
2019-01-27 12:42:22,695 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000008_0 is closed by DFSClient_attempt_1548592229700_0001_m_000008_0_987611707_1
2019-01-27 12:42:22,868 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742279_1455{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000011_0
2019-01-27 12:42:23,129 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742248_1424 size 105279
2019-01-27 12:42:23,177 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742257_1433 size 74057
2019-01-27 12:42:23,285 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742096_1272{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:23,294 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742096_1272 size 6144882
2019-01-27 12:42:23,475 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000014_0 is closed by DFSClient_attempt_1548592229700_0001_m_000014_0_-128812184_1
2019-01-27 12:42:23,479 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742205_1381{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:23,481 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742205_1381{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:23,646 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742190_1366{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:23,649 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742190_1366{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:23,919 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000008_0 is closed by DFSClient_attempt_1548592229700_0001_m_000008_0_987611707_1
2019-01-27 12:42:23,919 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000014_0 is closed by DFSClient_attempt_1548592229700_0001_m_000014_0_-128812184_1
2019-01-27 12:42:23,947 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742139_1315{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:23,949 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742139_1315{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:23,971 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742126_1302{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:23,975 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742126_1302{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:24,079 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000008_0 is closed by DFSClient_attempt_1548592229700_0001_m_000008_0_987611707_1
2019-01-27 12:42:24,223 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000014_0 is closed by DFSClient_attempt_1548592229700_0001_m_000014_0_-128812184_1
2019-01-27 12:42:24,335 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742199_1375{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:24,336 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742199_1375 size 204946
2019-01-27 12:42:24,473 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742098_1274{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:24,480 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000014_0 is closed by DFSClient_attempt_1548592229700_0001_m_000014_0_-128812184_1
2019-01-27 12:42:24,483 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742098_1274 size 8401889
2019-01-27 12:42:24,496 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742146_1322{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:24,497 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742146_1322 size 536908
2019-01-27 12:42:24,637 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000008_0 is closed by DFSClient_attempt_1548592229700_0001_m_000008_0_987611707_1
2019-01-27 12:42:24,730 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000014_0 is closed by DFSClient_attempt_1548592229700_0001_m_000014_0_-128812184_1
2019-01-27 12:42:24,808 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742277_1453 size 3718
2019-01-27 12:42:25,093 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742254_1430{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000008_0
2019-01-27 12:42:25,095 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742246_1422 size 275548
2019-01-27 12:42:25,101 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742254_1430{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 202387
2019-01-27 12:42:25,108 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742207_1383{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:25,109 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742207_1383 size 122950
2019-01-27 12:42:25,294 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000014_0 is closed by DFSClient_attempt_1548592229700_0001_m_000014_0_-128812184_1
2019-01-27 12:42:25,310 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742252_1428 size 217526
2019-01-27 12:42:25,367 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742169_1345{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:25,475 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742169_1345{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:25,651 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000014_0 is closed by DFSClient_attempt_1548592229700_0001_m_000014_0_-128812184_1
2019-01-27 12:42:25,684 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742204_1380{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:25,687 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742204_1380{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:25,852 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000008_0 is closed by DFSClient_attempt_1548592229700_0001_m_000008_0_987611707_1
2019-01-27 12:42:25,860 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742127_1303{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:25,861 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742127_1303{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:25,886 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000014_0 is closed by DFSClient_attempt_1548592229700_0001_m_000014_0_-128812184_1
2019-01-27 12:42:25,976 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000008_0 is closed by DFSClient_attempt_1548592229700_0001_m_000008_0_987611707_1
2019-01-27 12:42:25,998 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742128_1304{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:25,999 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742128_1304{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:26,240 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742251_1427{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:26,444 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000008_0 is closed by DFSClient_attempt_1548592229700_0001_m_000008_0_987611707_1
2019-01-27 12:42:27,138 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742135_1311{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:27,140 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742135_1311{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:27,312 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000008_0 is closed by DFSClient_attempt_1548592229700_0001_m_000008_0_987611707_1
2019-01-27 12:42:27,369 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742125_1301{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:27,371 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742125_1301{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:27,547 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000008_0 is closed by DFSClient_attempt_1548592229700_0001_m_000008_0_987611707_1
2019-01-27 12:42:27,783 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742243_1419{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:27,786 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742243_1419{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:27,882 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000008_0 is closed by DFSClient_attempt_1548592229700_0001_m_000008_0_987611707_1
2019-01-27 12:42:27,943 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742157_1333{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:27,954 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742157_1333{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:28,069 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000008_0 is closed by DFSClient_attempt_1548592229700_0001_m_000008_0_987611707_1
2019-01-27 12:42:28,096 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742280_1456{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000007_0
2019-01-27 12:42:28,176 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742281_1457{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000008_0
2019-01-27 12:42:28,491 INFO BlockStateChange: BLOCK* ask 172.18.0.3:50010 to replicate blk_1073742254_1430 to datanode(s) 172.18.0.2:50010
2019-01-27 12:42:28,491 INFO BlockStateChange: BLOCK* ask 172.18.0.3:50010 to replicate blk_1073742251_1427 to datanode(s) 172.18.0.2:50010
2019-01-27 12:42:31,275 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742251_1427 size 277470
2019-01-27 12:42:31,286 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742254_1430 size 202387
2019-01-27 12:42:31,628 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742282_1458{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000004_0
2019-01-27 12:42:35,107 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742283_1459{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000009_0
2019-01-27 12:42:35,818 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742284_1460{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000006_1
2019-01-27 12:42:37,338 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742285_1461{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000001_0
2019-01-27 12:42:37,539 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742286_1462{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000006_1
2019-01-27 12:42:37,610 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742287_1463{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000006_1
2019-01-27 12:42:37,774 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742288_1464{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000006_1
2019-01-27 12:42:37,775 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742289_1465{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000006_1
2019-01-27 12:42:37,840 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742290_1466{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000006_1
2019-01-27 12:42:38,303 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742291_1467{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000006_1
2019-01-27 12:42:38,376 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742292_1468{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000006_1
2019-01-27 12:42:38,394 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742293_1469{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000006_1
2019-01-27 12:42:38,396 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742294_1470{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000006_1
2019-01-27 12:42:38,532 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742295_1471{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000006_1
2019-01-27 12:42:38,684 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742296_1472{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000006_1
2019-01-27 12:42:38,884 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742297_1473{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000003_0
2019-01-27 12:42:39,847 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742298_1474{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000006_1
2019-01-27 12:42:40,632 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742299_1475{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000006_1
2019-01-27 12:42:40,633 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742300_1476{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000006_1
2019-01-27 12:42:40,634 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742301_1477{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000006_1
2019-01-27 12:42:40,634 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742302_1478{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000006_1
2019-01-27 12:42:40,635 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742303_1479{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000006_1
2019-01-27 12:42:40,635 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742304_1480{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000006_1
2019-01-27 12:42:40,665 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742305_1481{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000006_1
2019-01-27 12:42:40,839 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742306_1482{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000006_1
2019-01-27 12:42:40,840 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742307_1483{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000006_1
2019-01-27 12:42:40,841 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742308_1484{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000006_1
2019-01-27 12:42:40,842 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742309_1485{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000006_1
2019-01-27 12:42:40,842 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742310_1486{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000006_1
2019-01-27 12:42:40,842 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742311_1487{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000006_1
2019-01-27 12:42:41,235 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742312_1488{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000006_1
2019-01-27 12:42:41,675 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742313_1489{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000016_0
2019-01-27 12:42:42,131 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742314_1490{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000012_1
2019-01-27 12:42:42,962 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742315_1491{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000006_1
2019-01-27 12:42:44,945 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742316_1492{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000016_0
2019-01-27 12:42:45,114 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742317_1493{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000016_0
2019-01-27 12:42:45,151 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742318_1494{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000016_0
2019-01-27 12:42:45,152 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742319_1495{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000016_0
2019-01-27 12:42:45,244 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742320_1496{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000012_1
2019-01-27 12:42:45,244 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742321_1497{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000016_0
2019-01-27 12:42:45,267 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742322_1498{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000012_1
2019-01-27 12:42:45,536 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742323_1499{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000012_1
2019-01-27 12:42:45,554 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742324_1500{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000012_1
2019-01-27 12:42:45,555 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742325_1501{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000012_1
2019-01-27 12:42:45,578 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742326_1502{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000012_1
2019-01-27 12:42:45,587 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742327_1503{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000012_1
2019-01-27 12:42:45,606 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742328_1504{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000010_1
2019-01-27 12:42:45,642 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742329_1505{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000012_1
2019-01-27 12:42:45,650 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742330_1506{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000012_1
2019-01-27 12:42:45,653 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742331_1507{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000012_1
2019-01-27 12:42:45,835 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:45,835 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:45,835 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:45,835 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742332_1508{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000016_0
2019-01-27 12:42:45,840 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:45,841 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:45,841 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:45,841 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742333_1509{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000012_1
2019-01-27 12:42:45,925 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:45,925 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:45,925 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:45,926 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742334_1510{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000012_1
2019-01-27 12:42:45,959 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:45,959 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:45,959 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:45,959 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742335_1511{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000012_1
2019-01-27 12:42:46,064 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:46,065 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:46,065 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:46,065 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742336_1512{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000012_1
2019-01-27 12:42:46,126 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:46,126 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:46,126 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:46,126 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742337_1513{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000012_1
2019-01-27 12:42:46,355 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:46,355 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:46,355 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:46,355 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742338_1514{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000012_1
2019-01-27 12:42:46,406 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:46,406 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:46,406 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:46,406 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742339_1515{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000016_0
2019-01-27 12:42:46,659 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:46,659 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:46,659 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:46,660 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742340_1516{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000012_1
2019-01-27 12:42:46,661 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:46,661 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:46,661 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:46,661 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742341_1517{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000012_1
2019-01-27 12:42:46,698 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:46,698 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:46,698 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:46,698 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742342_1518{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000012_1
2019-01-27 12:42:46,705 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:46,705 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:46,705 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:46,709 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742343_1519{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000012_1
2019-01-27 12:42:46,750 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:46,750 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:46,750 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:46,750 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742344_1520{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000012_1
2019-01-27 12:42:46,751 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:46,751 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:46,751 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:46,751 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742345_1521{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000012_1
2019-01-27 12:42:46,820 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:46,820 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:46,820 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:46,820 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742346_1522{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000012_1
2019-01-27 12:42:46,842 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:46,842 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:46,842 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:46,843 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742347_1523{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000012_1
2019-01-27 12:42:46,859 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:46,859 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:46,859 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:46,859 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742348_1524{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000012_1
2019-01-27 12:42:46,867 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:46,867 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:46,867 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:46,867 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742349_1525{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000016_0
2019-01-27 12:42:46,868 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:46,868 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:46,868 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:46,868 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742350_1526{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000012_1
2019-01-27 12:42:46,869 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:46,869 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:46,869 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:46,869 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742351_1527{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000016_0
2019-01-27 12:42:46,875 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:46,875 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:46,875 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:46,875 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:46,875 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:46,875 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:46,875 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742352_1528{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000016_0
2019-01-27 12:42:46,875 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:46,875 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:46,876 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:46,876 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742353_1529{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000016_0
2019-01-27 12:42:46,876 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742354_1530{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000016_0
2019-01-27 12:42:46,880 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:46,880 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:46,880 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:46,880 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52378 Call#114 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000016_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:46,893 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:46,893 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:46,893 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:46,893 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52378 Call#115 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000016_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:47,196 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:47,196 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:47,197 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:47,197 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52402 Call#101 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000010_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:47,316 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:47,316 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:47,317 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:47,317 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52402 Call#102 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000010_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:47,372 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742349_1525{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:42:47,518 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000016_0 is closed by DFSClient_attempt_1548592229700_0001_m_000016_0_1312245754_1
2019-01-27 12:42:47,524 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:47,524 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:47,524 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:47,524 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52378 Call#117 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000016_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:47,553 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:47,553 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:47,553 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:47,553 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52402 Call#105 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000010_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:47,561 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:47,561 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:47,561 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:47,561 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52402 Call#106 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000010_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:47,564 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:47,565 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:47,565 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:47,565 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52402 Call#107 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000010_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:47,567 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:47,567 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:47,567 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:47,568 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52402 Call#108 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000010_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:47,571 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:47,571 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:47,571 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:47,571 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52402 Call#109 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000010_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:47,577 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:47,577 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:47,577 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:47,577 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52402 Call#110 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000010_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:47,592 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:47,592 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:47,592 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:47,592 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52402 Call#111 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000010_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:47,600 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:47,600 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:47,600 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:47,600 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52402 Call#112 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000010_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:47,612 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:47,612 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:47,612 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:47,612 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52402 Call#113 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000010_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:47,619 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:47,620 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:47,620 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:47,620 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52402 Call#114 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000010_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:47,624 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:47,624 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:47,624 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:47,624 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52402 Call#115 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000010_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:47,627 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:47,627 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:47,627 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:47,627 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52402 Call#116 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000010_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:47,634 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:47,634 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:47,634 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:47,634 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52402 Call#117 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000010_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:47,638 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:47,638 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:47,638 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:47,639 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52402 Call#118 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000010_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:47,648 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:47,648 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:47,648 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:47,649 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52402 Call#119 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000010_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:47,651 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:47,651 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:47,651 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:47,652 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52402 Call#120 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000010_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:48,327 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:48,327 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:48,328 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:48,328 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52380 Call#129 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000012_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:49,152 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742329_1505{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:49,155 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742329_1505{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:49,414 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000012_1 is closed by DFSClient_attempt_1548592229700_0001_m_000012_1_-1766875255_1
2019-01-27 12:42:49,419 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742337_1513{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:42:49,494 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742349_1525 to datanode(s) 172.18.0.3:50010
2019-01-27 12:42:49,494 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742337_1513 to datanode(s) 172.18.0.3:50010
2019-01-27 12:42:49,525 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:49,525 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:49,525 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:49,525 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52420 Call#92 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000002_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:49,552 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:49,552 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:49,552 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:49,552 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52420 Call#93 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000002_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:49,611 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:49,611 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:49,611 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:49,611 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52420 Call#96 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000002_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:49,628 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:49,628 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:49,628 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:49,628 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52420 Call#97 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000002_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:49,634 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:49,634 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:49,634 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:49,634 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52420 Call#98 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000002_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:49,639 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:49,640 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:49,640 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:49,640 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52420 Call#99 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000002_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:49,642 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:49,642 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:49,642 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:49,642 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52420 Call#100 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000002_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:49,645 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:49,646 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:49,646 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:49,646 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52420 Call#101 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000002_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:49,648 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:49,648 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:49,648 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:49,649 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000012_1 is closed by DFSClient_attempt_1548592229700_0001_m_000012_1_-1766875255_1
2019-01-27 12:42:49,649 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52420 Call#102 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000002_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:49,652 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:49,652 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:49,652 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:49,652 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52420 Call#103 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000002_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:49,659 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742322_1498{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:49,660 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:49,660 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:49,660 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:49,660 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52420 Call#104 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000002_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:49,663 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:49,663 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:49,663 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:49,663 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52420 Call#105 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000002_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:49,667 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:49,668 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:49,668 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:49,668 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52420 Call#106 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000002_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:49,673 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:49,674 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:49,674 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:49,674 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52420 Call#107 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000002_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:49,693 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:49,698 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:49,699 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:49,699 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52420 Call#108 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000002_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:49,707 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:49,707 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:49,707 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:49,707 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52420 Call#109 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000002_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:49,737 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:49,737 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:49,737 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:49,737 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52420 Call#110 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000002_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:49,739 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:49,739 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:49,739 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:49,740 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52420 Call#111 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000002_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:49,742 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:49,742 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:49,742 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:49,742 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52420 Call#112 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000002_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:49,768 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:49,768 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:49,768 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:49,768 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52420 Call#113 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000002_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:49,773 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:49,773 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:49,773 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:49,773 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52420 Call#114 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000002_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:49,777 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:49,778 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:49,778 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:49,778 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52420 Call#115 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000002_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:49,783 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:49,783 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:49,783 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:49,783 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52420 Call#116 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000002_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:49,786 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:49,786 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:49,786 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:49,786 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52420 Call#117 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000002_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:49,790 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:49,790 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:49,790 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:49,791 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52420 Call#118 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000002_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:49,793 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:49,793 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:49,793 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:49,794 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52420 Call#119 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000002_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:49,797 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:49,798 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:49,798 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:49,798 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52420 Call#120 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000002_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:49,800 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:49,800 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:49,800 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:49,800 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52420 Call#121 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000002_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:49,933 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742322_1498{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:50,046 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000012_1 is closed by DFSClient_attempt_1548592229700_0001_m_000012_1_-1766875255_1
2019-01-27 12:42:50,055 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742331_1507{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:50,059 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742331_1507{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:50,104 WARN org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.delete from 172.18.0.2:52420 Call#125 Retry#0: output error
2019-01-27 12:42:50,104 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000 caught an exception
java.nio.channels.ClosedChannelException
	at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:270)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:479)
	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:2583)
	at org.apache.hadoop.ipc.Server.access$1900(Server.java:136)
	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:986)
	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1051)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2104)
2019-01-27 12:42:50,422 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000012_1 is closed by DFSClient_attempt_1548592229700_0001_m_000012_1_-1766875255_1
2019-01-27 12:42:50,434 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742344_1520{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:42:50,602 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000012_1 is closed by DFSClient_attempt_1548592229700_0001_m_000012_1_-1766875255_1
2019-01-27 12:42:50,606 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742335_1511{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:42:50,807 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000012_1 is closed by DFSClient_attempt_1548592229700_0001_m_000012_1_-1766875255_1
2019-01-27 12:42:50,811 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742342_1518{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:42:50,860 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000012_1 is closed by DFSClient_attempt_1548592229700_0001_m_000012_1_-1766875255_1
2019-01-27 12:42:50,863 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742348_1524{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:42:50,991 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000012_1 is closed by DFSClient_attempt_1548592229700_0001_m_000012_1_-1766875255_1
2019-01-27 12:42:50,996 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:50,996 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:50,996 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:50,996 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52380 Call#139 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000012_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:42:51,478 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742327_1503{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:51,479 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742327_1503{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:51,597 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000012_1 is closed by DFSClient_attempt_1548592229700_0001_m_000012_1_-1766875255_1
2019-01-27 12:42:51,600 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742349_1525 size 80441
2019-01-27 12:42:51,602 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742325_1501{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:51,604 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742337_1513 size 376813
2019-01-27 12:42:51,605 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742325_1501{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:51,808 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000012_1 is closed by DFSClient_attempt_1548592229700_0001_m_000012_1_-1766875255_1
2019-01-27 12:42:51,813 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742320_1496{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:51,815 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742320_1496{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:51,995 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000012_1 is closed by DFSClient_attempt_1548592229700_0001_m_000012_1_-1766875255_1
2019-01-27 12:42:51,998 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742347_1523{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:42:52,240 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000012_1 is closed by DFSClient_attempt_1548592229700_0001_m_000012_1_-1766875255_1
2019-01-27 12:42:52,245 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742326_1502{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:52,266 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742326_1502 size 754082
2019-01-27 12:42:52,495 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742344_1520 to datanode(s) 172.18.0.3:50010
2019-01-27 12:42:52,495 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742335_1511 to datanode(s) 172.18.0.3:50010
2019-01-27 12:42:52,495 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742342_1518 to datanode(s) 172.18.0.3:50010
2019-01-27 12:42:52,495 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742348_1524 to datanode(s) 172.18.0.3:50010
2019-01-27 12:42:52,565 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000012_1 is closed by DFSClient_attempt_1548592229700_0001_m_000012_1_-1766875255_1
2019-01-27 12:42:52,568 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742333_1509{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:42:53,106 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000012_1 is closed by DFSClient_attempt_1548592229700_0001_m_000012_1_-1766875255_1
2019-01-27 12:42:53,118 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742324_1500{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:53,118 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742324_1500{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:53,448 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000012_1 is closed by DFSClient_attempt_1548592229700_0001_m_000012_1_-1766875255_1
2019-01-27 12:42:53,450 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:53,450 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:53,450 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:53,450 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742355_1531{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000012_1
2019-01-27 12:42:53,778 WARN org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52380 Call#149 Retry#0: output error
2019-01-27 12:42:53,779 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000 caught an exception
java.nio.channels.ClosedChannelException
	at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:270)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:479)
	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:2583)
	at org.apache.hadoop.ipc.Server.access$1900(Server.java:136)
	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:986)
	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1051)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2104)
2019-01-27 12:42:54,617 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742344_1520 size 264683
2019-01-27 12:42:54,634 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742335_1511 size 483627
2019-01-27 12:42:55,495 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742347_1523 to datanode(s) 172.18.0.3:50010
2019-01-27 12:42:55,496 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742333_1509 to datanode(s) 172.18.0.3:50010
2019-01-27 12:42:57,617 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742348_1524 size 170120
2019-01-27 12:42:57,618 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742342_1518 size 239858
2019-01-27 12:42:57,850 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742356_1532{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000000_1
2019-01-27 12:42:58,067 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742117_1293{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:58,073 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742117_1293{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:58,488 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000004_0 is closed by DFSClient_attempt_1548592229700_0001_m_000004_0_628121016_1
2019-01-27 12:42:59,017 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742357_1533{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000000_1
2019-01-27 12:42:59,055 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742358_1534{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000000_1
2019-01-27 12:42:59,338 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742359_1535{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000000_1
2019-01-27 12:42:59,341 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742360_1536{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000000_1
2019-01-27 12:42:59,360 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742361_1537{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000000_1
2019-01-27 12:42:59,362 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742362_1538{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000000_1
2019-01-27 12:42:59,364 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742363_1539{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000000_1
2019-01-27 12:42:59,378 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742364_1540{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000000_1
2019-01-27 12:42:59,382 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:59,382 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:59,382 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:59,382 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742365_1541{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000000_1
2019-01-27 12:42:59,385 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:59,385 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:59,385 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:59,385 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742366_1542{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000000_1
2019-01-27 12:42:59,504 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:59,505 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:59,505 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:59,505 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742367_1543{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000000_1
2019-01-27 12:42:59,543 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:59,543 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:59,543 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:59,543 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742368_1544{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000000_1
2019-01-27 12:42:59,543 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:59,544 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:59,544 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:59,544 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742369_1545{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000000_1
2019-01-27 12:42:59,544 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:59,544 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:59,544 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:59,545 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:59,545 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:59,545 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742370_1546{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000000_1
2019-01-27 12:42:59,545 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:59,545 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:59,543 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:42:59,545 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742371_1547{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000000_1
2019-01-27 12:42:59,544 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:59,545 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:59,545 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:42:59,545 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:42:59,545 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742372_1548{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000000_1
2019-01-27 12:42:59,545 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742373_1549{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000000_1
2019-01-27 12:42:59,693 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742120_1296{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:42:59,697 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742120_1296{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:00,054 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000004_0 is closed by DFSClient_attempt_1548592229700_0001_m_000004_0_628121016_1
2019-01-27 12:43:00,574 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742099_1275{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:00,576 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742099_1275{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:00,607 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:00,607 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:00,607 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:00,607 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742374_1550{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000014_1
2019-01-27 12:43:00,686 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000004_0 is closed by DFSClient_attempt_1548592229700_0001_m_000004_0_628121016_1
2019-01-27 12:43:01,878 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742347_1523 size 177663
2019-01-27 12:43:01,885 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742333_1509 size 546651
2019-01-27 12:43:02,077 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742116_1292{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:02,080 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742116_1292{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:02,237 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:02,237 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:02,237 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:02,237 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52678 Call#114 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000000_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:02,422 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000004_0 is closed by DFSClient_attempt_1548592229700_0001_m_000004_0_628121016_1
2019-01-27 12:43:02,630 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742148_1324{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:02,634 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742148_1324{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:02,793 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000004_0 is closed by DFSClient_attempt_1548592229700_0001_m_000004_0_628121016_1
2019-01-27 12:43:02,982 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742109_1285{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:02,986 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742109_1285 size 6653132
2019-01-27 12:43:03,082 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000004_0 is closed by DFSClient_attempt_1548592229700_0001_m_000004_0_628121016_1
2019-01-27 12:43:03,206 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742365_1541{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:03,300 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742141_1317{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:03,315 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742141_1317 size 3361789
2019-01-27 12:43:03,341 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:03,341 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:03,341 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:03,341 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52690 Call#97 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000014_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:03,421 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000000_1 is closed by DFSClient_attempt_1548592229700_0001_m_000000_1_625055943_1
2019-01-27 12:43:03,427 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742369_1545{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:03,440 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:03,440 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:03,440 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:03,440 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52690 Call#98 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000014_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:03,527 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:03,527 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:03,527 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:03,527 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52690 Call#102 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000014_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:03,542 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000004_0 is closed by DFSClient_attempt_1548592229700_0001_m_000004_0_628121016_1
2019-01-27 12:43:03,556 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:03,556 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:03,556 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:03,556 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52690 Call#103 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000014_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:03,569 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:03,569 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:03,569 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:03,569 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52690 Call#104 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000014_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:03,572 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:03,572 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:03,572 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:03,573 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52690 Call#105 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000014_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:03,575 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:03,575 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:03,575 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:03,575 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52690 Call#106 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000014_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:03,582 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:03,582 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:03,582 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:03,582 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52690 Call#107 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000014_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:03,585 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:03,585 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:03,585 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:03,585 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52690 Call#108 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000014_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:03,589 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:03,590 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:03,590 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:03,590 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52690 Call#109 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000014_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:03,607 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:03,607 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:03,607 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:03,607 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52690 Call#110 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000014_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:03,615 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:03,615 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:03,615 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:03,615 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52690 Call#111 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000014_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:03,623 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:03,623 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:03,623 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:03,623 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52690 Call#112 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000014_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:03,628 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:03,629 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:03,629 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:03,629 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52690 Call#113 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000014_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:03,644 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:03,645 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:03,645 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:03,645 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52690 Call#114 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000014_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:03,653 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:03,653 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:03,653 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:03,653 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52690 Call#115 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000014_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:03,656 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:03,656 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:03,657 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:03,657 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52690 Call#116 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000014_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:03,660 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:03,660 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:03,660 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:03,660 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52690 Call#117 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000014_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:03,665 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:03,665 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:03,665 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:03,665 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52690 Call#118 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000014_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:03,693 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:03,693 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:03,693 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:03,693 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52690 Call#119 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000014_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:03,704 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:03,705 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:03,705 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:03,705 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52690 Call#120 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000014_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:03,716 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:03,716 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:03,716 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:03,717 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742375_1551{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000014_1
2019-01-27 12:43:03,771 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000000_1 is closed by DFSClient_attempt_1548592229700_0001_m_000000_1_625055943_1
2019-01-27 12:43:03,781 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742367_1543{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:03,950 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742375_1551{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:04,115 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000000_1 is closed by DFSClient_attempt_1548592229700_0001_m_000000_1_625055943_1
2019-01-27 12:43:04,200 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742358_1534{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:04,214 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742358_1534 size 208994
2019-01-27 12:43:04,414 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000014_1 is closed by DFSClient_attempt_1548592229700_0001_m_000014_1_-107846553_1
2019-01-27 12:43:04,415 WARN org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.complete from 172.18.0.2:52690 Call#122 Retry#0: output error
2019-01-27 12:43:04,415 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000 caught an exception
java.nio.channels.ClosedChannelException
	at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:270)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:479)
	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:2583)
	at org.apache.hadoop.ipc.Server.access$1900(Server.java:136)
	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:986)
	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1051)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2104)
2019-01-27 12:43:04,474 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742170_1346{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:04,482 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742170_1346{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:04,497 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742365_1541 to datanode(s) 172.18.0.3:50010
2019-01-27 12:43:04,498 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742369_1545 to datanode(s) 172.18.0.3:50010
2019-01-27 12:43:04,498 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742367_1543 to datanode(s) 172.18.0.3:50010
2019-01-27 12:43:04,498 INFO BlockStateChange: BLOCK* ask 172.18.0.3:50010 to replicate blk_1073742375_1551 to datanode(s) 172.18.0.2:50010
2019-01-27 12:43:04,668 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000000_1 is closed by DFSClient_attempt_1548592229700_0001_m_000000_1_625055943_1
2019-01-27 12:43:04,672 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742368_1544{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:04,748 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:04,748 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:04,748 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:04,749 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742376_1552{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000006_1
2019-01-27 12:43:04,832 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000004_0 is closed by DFSClient_attempt_1548592229700_0001_m_000004_0_628121016_1
2019-01-27 12:43:04,957 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000000_1 is closed by DFSClient_attempt_1548592229700_0001_m_000000_1_625055943_1
2019-01-27 12:43:04,962 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:04,962 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:04,962 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:04,962 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52678 Call#120 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000000_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:05,375 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742373_1549{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:05,387 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:05,387 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:05,387 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:05,388 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742377_1553{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000004_0
2019-01-27 12:43:05,529 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000000_1 is closed by DFSClient_attempt_1548592229700_0001_m_000000_1_625055943_1
2019-01-27 12:43:05,532 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:05,532 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:05,532 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:05,532 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52678 Call#125 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000000_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:05,536 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742366_1542{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:05,568 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742377_1553{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:05,743 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000004_0 is closed by DFSClient_attempt_1548592229700_0001_m_000004_0_628121016_1
2019-01-27 12:43:05,744 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000000_1 is closed by DFSClient_attempt_1548592229700_0001_m_000000_1_625055943_1
2019-01-27 12:43:05,744 WARN org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.complete from 172.18.0.2:52678 Call#126 Retry#0: output error
2019-01-27 12:43:05,744 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000 caught an exception
java.nio.channels.ClosedChannelException
	at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:270)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:479)
	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:2583)
	at org.apache.hadoop.ipc.Server.access$1900(Server.java:136)
	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:986)
	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1051)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2104)
2019-01-27 12:43:06,265 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742112_1288{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:06,266 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742112_1288{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:06,371 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000004_0 is closed by DFSClient_attempt_1548592229700_0001_m_000004_0_628121016_1
2019-01-27 12:43:06,935 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742145_1321{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:06,936 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742145_1321{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:06,938 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742365_1541 size 121768
2019-01-27 12:43:07,145 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742369_1545 size 96604
2019-01-27 12:43:07,146 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742375_1551 size 15105
2019-01-27 12:43:07,433 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000004_0 is closed by DFSClient_attempt_1548592229700_0001_m_000004_0_628121016_1
2019-01-27 12:43:07,499 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742368_1544 to datanode(s) 172.18.0.3:50010
2019-01-27 12:43:07,499 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742373_1549 to datanode(s) 172.18.0.3:50010
2019-01-27 12:43:07,499 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742366_1542 to datanode(s) 172.18.0.3:50010
2019-01-27 12:43:08,506 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742122_1298{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:08,516 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742122_1298{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:08,792 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000004_0 is closed by DFSClient_attempt_1548592229700_0001_m_000004_0_628121016_1
2019-01-27 12:43:09,240 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742106_1282{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:09,244 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742106_1282{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:09,463 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000004_0 is closed by DFSClient_attempt_1548592229700_0001_m_000004_0_628121016_1
2019-01-27 12:43:10,625 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742378_1554{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000008_1
2019-01-27 12:43:10,681 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742367_1543 size 115040
2019-01-27 12:43:10,690 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742368_1544 size 114259
2019-01-27 12:43:10,744 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742091_1267{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:10,749 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742091_1267 size 98195987
2019-01-27 12:43:11,080 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2603 Total time for transactions(ms): 216 Number of transactions batched in Syncs: 384 Number of syncs: 1071 SyncTimes(ms): 71403 
2019-01-27 12:43:11,871 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000004_0 is closed by DFSClient_attempt_1548592229700_0001_m_000004_0_628121016_1
2019-01-27 12:43:12,577 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742379_1555{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000008_1
2019-01-27 12:43:12,785 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742172_1348{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:12,786 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742172_1348{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:12,867 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000004_0 is closed by DFSClient_attempt_1548592229700_0001_m_000004_0_628121016_1
2019-01-27 12:43:13,349 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742380_1556{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000008_1
2019-01-27 12:43:13,354 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742381_1557{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000008_1
2019-01-27 12:43:13,468 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742382_1558{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000010_2
2019-01-27 12:43:13,475 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742383_1559{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000008_1
2019-01-27 12:43:13,500 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742377_1553 to datanode(s) 172.18.0.3:50010
2019-01-27 12:43:13,574 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742102_1278{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:13,575 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742102_1278{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:13,853 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742373_1549 size 108873
2019-01-27 12:43:13,853 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742366_1542 size 133849
2019-01-27 12:43:13,863 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000004_0 is closed by DFSClient_attempt_1548592229700_0001_m_000004_0_628121016_1
2019-01-27 12:43:13,901 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742384_1560{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000008_1
2019-01-27 12:43:14,067 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742385_1561{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000008_1
2019-01-27 12:43:14,102 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742386_1562{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000008_1
2019-01-27 12:43:14,156 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742387_1563{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000010_2
2019-01-27 12:43:14,222 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742388_1564{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000010_2
2019-01-27 12:43:14,244 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742389_1565{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000010_2
2019-01-27 12:43:14,273 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742390_1566{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000010_2
2019-01-27 12:43:14,457 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742391_1567{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000010_2
2019-01-27 12:43:14,459 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742392_1568{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000010_2
2019-01-27 12:43:14,460 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742393_1569{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000008_1
2019-01-27 12:43:14,662 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742394_1570{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000010_2
2019-01-27 12:43:14,684 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742395_1571{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000010_2
2019-01-27 12:43:14,685 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742396_1572{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000008_1
2019-01-27 12:43:14,686 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742397_1573{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000008_1
2019-01-27 12:43:14,687 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742398_1574{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000008_1
2019-01-27 12:43:14,688 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:14,688 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:14,688 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:14,688 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742399_1575{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000008_1
2019-01-27 12:43:14,694 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:14,694 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:14,694 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:14,694 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742400_1576{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000008_1
2019-01-27 12:43:14,961 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:14,961 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:14,961 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:14,961 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:14,962 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:14,962 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742401_1577{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000008_1
2019-01-27 12:43:14,962 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:14,962 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742402_1578{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000010_2
2019-01-27 12:43:15,405 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:15,405 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:15,405 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:15,405 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742403_1579{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000010_2
2019-01-27 12:43:15,407 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:15,407 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:15,407 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:15,407 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742404_1580{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000010_2
2019-01-27 12:43:15,407 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:15,407 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:15,407 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:15,408 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742405_1581{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000010_2
2019-01-27 12:43:15,417 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:15,417 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:15,417 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:15,418 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742406_1582{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000008_1
2019-01-27 12:43:15,418 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:15,418 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:15,418 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:15,419 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742407_1583{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000010_2
2019-01-27 12:43:15,419 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:15,419 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:15,419 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:15,419 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52718 Call#115 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000010_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:15,420 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:15,420 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:15,420 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:15,420 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52720 Call#118 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000008_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:15,423 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:15,423 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:15,423 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:15,423 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:15,424 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:15,424 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:15,424 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52718 Call#116 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000010_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:15,424 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:15,425 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:15,425 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:15,425 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52718 Call#124 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000010_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:15,426 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:15,426 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:15,426 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:15,426 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52718 Call#122 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000010_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:15,427 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:15,427 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:15,427 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:15,427 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52718 Call#120 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000010_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:15,428 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:15,428 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:15,428 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:15,428 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52718 Call#123 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000010_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:15,429 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:15,429 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:15,429 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:15,429 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52718 Call#121 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000010_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:15,430 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:15,430 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:15,430 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:15,430 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52718 Call#119 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000010_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:15,430 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:15,431 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:15,431 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:15,431 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52718 Call#118 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000010_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:15,431 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:15,431 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:15,431 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:15,431 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52718 Call#117 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000010_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:15,432 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:15,432 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:15,432 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:15,432 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52720 Call#120 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000008_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:15,417 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:15,442 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:15,442 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:15,442 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742408_1584{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000008_1
2019-01-27 12:43:15,442 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52720 Call#119 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000008_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:15,443 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:15,444 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:15,444 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:15,444 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52720 Call#122 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000008_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:15,445 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:15,445 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:15,445 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:15,445 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52718 Call#125 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000010_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:15,446 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:15,446 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:15,446 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:15,447 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52718 Call#126 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000010_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:15,448 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:15,448 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:15,449 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:15,449 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52720 Call#124 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000008_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:15,450 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:15,450 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:15,450 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:15,450 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52720 Call#121 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000008_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:15,672 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:15,672 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:15,673 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:15,673 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742163_1339{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:15,673 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52718 Call#127 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000010_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:15,673 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:15,673 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:15,673 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742163_1339{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:15,673 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:15,673 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52720 Call#125 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000008_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:15,682 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:15,683 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:15,683 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:15,683 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742409_1585{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000008_1
2019-01-27 12:43:15,683 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:15,683 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:15,683 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:15,683 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52720 Call#126 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000008_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:15,689 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:15,689 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:15,689 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:15,689 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52720 Call#123 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000008_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:15,969 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000004_0 is closed by DFSClient_attempt_1548592229700_0001_m_000004_0_628121016_1
2019-01-27 12:43:16,414 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742104_1280{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:16,535 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742104_1280{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:16,755 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742408_1584{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:16,827 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000004_0 is closed by DFSClient_attempt_1548592229700_0001_m_000004_0_628121016_1
2019-01-27 12:43:17,090 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742404_1580{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000010_2
2019-01-27 12:43:17,090 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742404_1580{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 342397
2019-01-27 12:43:17,160 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000008_1 is closed by DFSClient_attempt_1548592229700_0001_m_000008_1_846242391_1
2019-01-27 12:43:17,188 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742377_1553 size 7905
2019-01-27 12:43:17,195 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:17,195 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:17,195 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:17,195 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52808 Call#93 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000016_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:17,278 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742396_1572{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:17,280 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742396_1572{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:17,444 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000008_1 is closed by DFSClient_attempt_1548592229700_0001_m_000008_1_846242391_1
2019-01-27 12:43:17,472 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742229_1405{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:17,481 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742398_1574{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:17,485 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742229_1405{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:17,485 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742398_1574{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:17,694 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000010_2 is closed by DFSClient_attempt_1548592229700_0001_m_000010_2_1507685115_1
2019-01-27 12:43:17,694 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000004_0 is closed by DFSClient_attempt_1548592229700_0001_m_000004_0_628121016_1
2019-01-27 12:43:17,695 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000008_1 is closed by DFSClient_attempt_1548592229700_0001_m_000008_1_846242391_1
2019-01-27 12:43:17,699 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742381_1557{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:17,711 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741966_1142{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:17,714 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742381_1557{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:17,714 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741966_1142{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:17,738 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:17,738 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:17,738 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:17,738 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52808 Call#97 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000016_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:17,987 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000008_1 is closed by DFSClient_attempt_1548592229700_0001_m_000008_1_846242391_1
2019-01-27 12:43:17,996 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742399_1575{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:18,082 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742405_1581{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:18,130 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:18,130 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:18,130 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:18,130 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52808 Call#100 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000016_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:18,134 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:18,134 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:18,134 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:18,134 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52808 Call#101 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000016_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:18,140 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:18,140 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:18,140 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:18,140 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52808 Call#102 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000016_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:18,143 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:18,143 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:18,143 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:18,143 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52808 Call#103 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000016_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:18,146 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:18,146 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:18,146 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:18,146 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52808 Call#104 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000016_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:18,149 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:18,149 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:18,149 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:18,149 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52808 Call#105 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000016_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:18,154 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:18,155 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:18,155 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:18,155 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52808 Call#106 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000016_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:18,157 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:18,157 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:18,157 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:18,157 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52808 Call#107 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000016_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:18,160 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:18,160 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:18,160 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:18,160 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52808 Call#108 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000016_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:18,162 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:18,163 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:18,163 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:18,163 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52808 Call#109 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000016_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:18,165 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:18,165 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:18,165 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:18,165 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52808 Call#110 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000016_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:18,168 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:18,168 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:18,168 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:18,168 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52808 Call#111 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000016_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:18,170 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:18,170 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:18,170 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:18,170 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52808 Call#112 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000016_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:18,173 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:18,173 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:18,173 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:18,173 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52808 Call#113 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000016_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:18,177 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:18,177 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:18,177 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:18,177 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52808 Call#114 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000016_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:18,180 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:18,180 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:18,180 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:18,180 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52808 Call#115 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000016_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:18,183 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:18,183 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:18,184 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:18,184 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52808 Call#116 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000016_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:18,186 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:18,186 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:18,186 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:18,186 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52808 Call#117 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000016_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:18,189 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:18,189 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:18,189 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:18,189 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52808 Call#118 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000016_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:18,192 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:18,192 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:18,192 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:18,192 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52808 Call#119 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000016_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:18,194 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:18,194 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:18,195 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:18,195 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52808 Call#120 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000016_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:18,197 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:18,197 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:18,197 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:18,197 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52808 Call#121 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000016_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:18,200 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:18,200 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:18,201 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:18,201 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52808 Call#122 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000016_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:18,205 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:18,205 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:18,205 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:18,205 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52808 Call#123 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000016_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:18,230 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:18,230 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:18,230 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:18,230 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52808 Call#124 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000016_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:18,234 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:18,234 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:18,234 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:18,234 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52808 Call#125 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000016_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:18,236 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:18,236 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:18,237 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:18,237 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52808 Call#126 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000016_1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:18,296 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000010_2 is closed by DFSClient_attempt_1548592229700_0001_m_000010_2_1507685115_1
2019-01-27 12:43:18,301 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000008_1 is closed by DFSClient_attempt_1548592229700_0001_m_000008_1_846242391_1
2019-01-27 12:43:18,303 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:18,313 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:18,313 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:18,313 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:18,313 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52712 Call#242 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=out/_tmp.000004_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:18,317 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742392_1568{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:18,319 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742392_1568{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:18,463 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742383_1559{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:18,469 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742383_1559{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:18,725 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000008_1 is closed by DFSClient_attempt_1548592229700_0001_m_000008_1_846242391_1
2019-01-27 12:43:18,726 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000010_2 is closed by DFSClient_attempt_1548592229700_0001_m_000010_2_1507685115_1
2019-01-27 12:43:18,731 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742402_1578{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:18,750 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742380_1556{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:18,759 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742380_1556 size 444034
2019-01-27 12:43:18,791 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742011_1187{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:18,794 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742173_1349{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:18,796 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742173_1349 size 1960886
2019-01-27 12:43:18,797 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742011_1187{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:19,000 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:19,000 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000004_0 is closed by DFSClient_attempt_1548592229700_0001_m_000004_0_628121016_1
2019-01-27 12:43:19,001 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000008_1 is closed by DFSClient_attempt_1548592229700_0001_m_000008_1_846242391_1
2019-01-27 12:43:19,001 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000010_2 is closed by DFSClient_attempt_1548592229700_0001_m_000010_2_1507685115_1
2019-01-27 12:43:19,001 WARN org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.complete from 172.18.0.2:52718 Call#135 Retry#0: output error
2019-01-27 12:43:19,001 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000 caught an exception
java.nio.channels.ClosedChannelException
	at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:270)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:479)
	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:2583)
	at org.apache.hadoop.ipc.Server.access$1900(Server.java:136)
	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:986)
	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1051)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2104)
2019-01-27 12:43:19,016 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742406_1582{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:19,042 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742101_1277{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:19,043 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742101_1277{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:19,093 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741960_1136{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:19,094 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741960_1136{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:19,270 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000004_0 is closed by DFSClient_attempt_1548592229700_0001_m_000004_0_628121016_1
2019-01-27 12:43:19,271 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000008_1 is closed by DFSClient_attempt_1548592229700_0001_m_000008_1_846242391_1
2019-01-27 12:43:19,278 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742401_1577{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000008_1
2019-01-27 12:43:19,278 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741981_1157{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000005_0
2019-01-27 12:43:19,279 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741981_1157{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 6492257
2019-01-27 12:43:19,279 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742401_1577{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 267361
2019-01-27 12:43:19,280 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741981_1157 size 6492257
2019-01-27 12:43:19,287 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742282_1458{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:19,287 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742282_1458{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:19,335 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:19,335 WARN org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.delete from 172.18.0.2:52808 Call#134 Retry#0: output error
2019-01-27 12:43:19,335 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000 caught an exception
java.nio.channels.ClosedChannelException
	at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:270)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:479)
	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:2583)
	at org.apache.hadoop.ipc.Server.access$1900(Server.java:136)
	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:986)
	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1051)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2104)
2019-01-27 12:43:19,420 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000004_0 is closed by DFSClient_attempt_1548592229700_0001_m_000004_0_628121016_1
2019-01-27 12:43:19,501 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742408_1584 to datanode(s) 172.18.0.3:50010
2019-01-27 12:43:19,501 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742404_1580 to datanode(s) 172.18.0.3:50010
2019-01-27 12:43:19,501 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742399_1575 to datanode(s) 172.18.0.3:50010
2019-01-27 12:43:19,501 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742405_1581 to datanode(s) 172.18.0.3:50010
2019-01-27 12:43:19,651 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742408_1584 size 236591
2019-01-27 12:43:19,653 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742404_1580 size 342397
2019-01-27 12:43:19,687 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741993_1169{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:19,688 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741993_1169{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:19,769 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:19,870 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:19,939 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742410_1586{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000007_0
2019-01-27 12:43:20,128 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741933_1109{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:20,128 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741933_1109{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:20,161 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741944_1120{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:20,166 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741944_1120 size 11182251
2019-01-27 12:43:20,441 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741983_1159{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:20,443 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741983_1159{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:20,470 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:20,471 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:20,565 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:20,653 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741977_1153{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:20,655 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741977_1153{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:20,666 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742020_1196{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000005_0
2019-01-27 12:43:20,666 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742020_1196{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 3530900
2019-01-27 12:43:20,667 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742020_1196 size 3530900
2019-01-27 12:43:20,714 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742025_1201{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:20,715 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742025_1201{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:20,742 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742411_1587{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000002_2
2019-01-27 12:43:20,790 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:20,867 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:20,895 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742028_1204{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000011_0
2019-01-27 12:43:20,903 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742028_1204{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 3525334
2019-01-27 12:43:20,904 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742028_1204 size 3525334
2019-01-27 12:43:20,953 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741939_1115{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:20,955 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741939_1115 size 11244725
2019-01-27 12:43:21,052 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:21,163 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:21,178 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741989_1165{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:21,182 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741989_1165{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:21,238 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742412_1588{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000002_2
2019-01-27 12:43:21,255 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742413_1589{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000002_2
2019-01-27 12:43:21,259 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742414_1590{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000002_2
2019-01-27 12:43:21,271 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742415_1591{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000002_2
2019-01-27 12:43:21,282 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742416_1592{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000002_2
2019-01-27 12:43:21,292 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:21,298 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742417_1593{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000002_2
2019-01-27 12:43:21,306 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741973_1149{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:21,308 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741973_1149 size 6632136
2019-01-27 12:43:21,359 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742418_1594{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000002_2
2019-01-27 12:43:21,360 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742419_1595{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000002_2
2019-01-27 12:43:21,375 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742030_1206{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:21,381 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742030_1206 size 5078613
2019-01-27 12:43:21,409 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742420_1596{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000002_2
2019-01-27 12:43:21,417 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742421_1597{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000002_2
2019-01-27 12:43:21,472 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742422_1598{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000002_2
2019-01-27 12:43:21,474 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:21,474 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:21,477 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742423_1599{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000002_2
2019-01-27 12:43:21,481 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741998_1174{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:21,486 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741998_1174{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:21,615 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742424_1600{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000002_2
2019-01-27 12:43:21,617 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:21,617 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:21,617 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:21,617 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742425_1601{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000002_2
2019-01-27 12:43:21,617 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:21,618 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:21,618 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:21,618 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:21,620 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742426_1602{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000002_2
2019-01-27 12:43:21,621 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:21,621 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:21,621 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:21,621 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742427_1603{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000002_2
2019-01-27 12:43:21,636 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:21,636 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:21,636 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:21,636 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742428_1604{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000002_2
2019-01-27 12:43:21,724 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:21,725 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:21,725 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:21,725 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742429_1605{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000002_2
2019-01-27 12:43:21,726 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742430_1606{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000002_2
2019-01-27 12:43:21,726 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:21,731 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741985_1161{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:21,733 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741985_1161{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:21,739 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742027_1203{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:21,740 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742027_1203{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:21,890 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742431_1607{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000002_2
2019-01-27 12:43:21,893 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742432_1608{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000002_2
2019-01-27 12:43:21,894 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742433_1609{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000002_2
2019-01-27 12:43:21,895 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742434_1610{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000002_2
2019-01-27 12:43:21,895 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742435_1611{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000002_2
2019-01-27 12:43:22,049 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:22,049 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:22,274 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742436_1612{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000002_2
2019-01-27 12:43:22,353 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742437_1613{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000002_2
2019-01-27 12:43:22,394 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742060_1236{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:22,394 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742005_1181{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:22,396 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742005_1181{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:22,398 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742032_1208{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:22,406 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742032_1208 size 3201392
2019-01-27 12:43:22,406 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742060_1236{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:22,418 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742050_1226{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000005_0
2019-01-27 12:43:22,419 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742050_1226{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 2355417
2019-01-27 12:43:22,420 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742050_1226 size 2355417
2019-01-27 12:43:22,502 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742402_1578 to datanode(s) 172.18.0.3:50010
2019-01-27 12:43:22,502 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742406_1582 to datanode(s) 172.18.0.3:50010
2019-01-27 12:43:22,527 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:22,529 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:22,690 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:22,796 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742055_1231{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:22,797 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742055_1231{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:22,800 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742045_1221{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:22,802 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742045_1221 size 2359652
2019-01-27 12:43:22,811 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742438_1614{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000002_2
2019-01-27 12:43:22,862 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742399_1575 size 289239
2019-01-27 12:43:22,863 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742405_1581 size 331990
2019-01-27 12:43:22,878 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742006_1182{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:22,879 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742006_1182 size 6650494
2019-01-27 12:43:23,022 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:23,023 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:23,023 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:23,126 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:23,254 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742439_1615{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000011_0
2019-01-27 12:43:23,254 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742440_1616{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000005_0
2019-01-27 12:43:23,314 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741947_1123{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:23,315 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741947_1123{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:23,356 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742064_1240{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:23,359 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742064_1240 size 2417128
2019-01-27 12:43:23,531 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:23,531 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:23,607 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742307_1483{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:23,611 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742307_1483{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:23,656 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742439_1615{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:23,656 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742439_1615{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:23,694 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:23,698 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742441_1617{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000001_0
2019-01-27 12:43:23,760 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742440_1616{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:23,760 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742440_1616 size 5633
2019-01-27 12:43:23,868 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:24,019 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:24,027 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742293_1469{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:24,030 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742293_1469{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:24,159 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742023_1199{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:24,161 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742023_1199 size 3531713
2019-01-27 12:43:24,217 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742053_1229{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:24,219 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742053_1229{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:24,224 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:24,245 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742018_1194{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:24,246 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742018_1194 size 3628295
2019-01-27 12:43:24,338 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:24,339 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:24,426 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:24,434 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742298_1474{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:24,435 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742298_1474{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:24,548 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741995_1171{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:24,548 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741995_1171{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:24,606 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742441_1617{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:24,609 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742441_1617 size 15872
2019-01-27 12:43:24,616 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741982_1158{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000003_0
2019-01-27 12:43:24,616 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741982_1158{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 6591543
2019-01-27 12:43:24,617 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741982_1158 size 6591543
2019-01-27 12:43:24,632 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:24,709 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:24,709 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:24,746 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742041_1217{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:24,746 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742041_1217{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:24,746 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741976_1152{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:24,747 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741976_1152{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:24,747 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741987_1163{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:24,747 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741987_1163{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:24,787 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742288_1464{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:24,788 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742288_1464{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:24,898 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:24,898 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:24,898 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:24,990 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:25,006 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742004_1180{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:25,006 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742004_1180{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:25,007 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741992_1168{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000011_0
2019-01-27 12:43:25,008 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741992_1168{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 5388592
2019-01-27 12:43:25,008 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741992_1168 size 5388592
2019-01-27 12:43:25,082 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742075_1251{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:25,083 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742075_1251{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:25,160 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:25,160 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:25,165 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742304_1480{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:25,189 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:25,195 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742026_1202{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:25,196 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742026_1202{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:25,199 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742304_1480{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:25,201 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741990_1166{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:25,202 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741990_1166 size 5546043
2019-01-27 12:43:25,329 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742044_1220{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000001_0
2019-01-27 12:43:25,330 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742044_1220{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 3582120
2019-01-27 12:43:25,331 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742044_1220 size 3582120
2019-01-27 12:43:25,332 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742068_1244{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:25,333 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742068_1244{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:25,417 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:25,417 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:25,418 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:25,427 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742049_1225{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:25,427 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742049_1225{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:25,548 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:25,551 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:25,560 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742442_1618{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000012_2
2019-01-27 12:43:25,670 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:25,677 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742295_1471{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:25,710 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742402_1578 size 383180
2019-01-27 12:43:25,735 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742406_1582 size 237776
2019-01-27 12:43:25,742 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741961_1137{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000005_0
2019-01-27 12:43:25,742 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741953_1129{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000009_0
2019-01-27 12:43:25,743 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742295_1471 size 6615966
2019-01-27 12:43:25,743 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741953_1129{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 11129974
2019-01-27 12:43:25,743 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741961_1137{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 7581455
2019-01-27 12:43:25,743 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741953_1129 size 11129974
2019-01-27 12:43:25,743 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741961_1137 size 7581455
2019-01-27 12:43:25,794 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741956_1132{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:25,808 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741956_1132{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:25,814 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742077_1253{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:25,815 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742077_1253{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:25,890 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:25,890 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:25,897 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742002_1178{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:25,898 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742002_1178{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:25,992 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:25,993 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:26,094 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:26,099 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742308_1484{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:26,101 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742308_1484{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:26,106 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742015_1191{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000001_0
2019-01-27 12:43:26,107 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742015_1191{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 5533891
2019-01-27 12:43:26,108 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742015_1191 size 5533891
2019-01-27 12:43:26,258 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742443_1619{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000003_0
2019-01-27 12:43:26,261 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741927_1103{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:26,261 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741927_1103{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:26,362 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:26,363 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:26,365 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:26,466 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:26,504 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742311_1487{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:26,508 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741928_1104{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000005_0
2019-01-27 12:43:26,510 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742052_1228{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:26,510 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741928_1104{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 99523140
2019-01-27 12:43:26,511 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742007_1183{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000009_0
2019-01-27 12:43:26,512 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742052_1228{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:26,512 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741928_1104 size 99523140
2019-01-27 12:43:26,512 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742007_1183{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 6400437
2019-01-27 12:43:26,513 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742007_1183 size 6400437
2019-01-27 12:43:26,514 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742311_1487{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:26,521 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742443_1619{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:26,522 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742443_1619 size 6173
2019-01-27 12:43:26,627 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742042_1218{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000011_0
2019-01-27 12:43:26,627 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742042_1218{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 2283571
2019-01-27 12:43:26,627 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742042_1218 size 2283571
2019-01-27 12:43:26,879 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:26,879 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:26,879 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:26,879 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:26,995 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742444_1620{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000012_2
2019-01-27 12:43:27,129 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742445_1621{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000012_2
2019-01-27 12:43:27,146 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:27,283 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:27,284 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:27,288 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742446_1622{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000006_1
2019-01-27 12:43:27,395 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742057_1233{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:27,398 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742057_1233 size 3603259
2019-01-27 12:43:27,551 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:27,568 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742447_1623{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000012_2
2019-01-27 12:43:27,574 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742448_1624{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000012_2
2019-01-27 12:43:27,575 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742449_1625{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000012_2
2019-01-27 12:43:27,604 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742450_1626{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000012_2
2019-01-27 12:43:27,633 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742451_1627{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000012_2
2019-01-27 12:43:27,695 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742452_1628{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000012_2
2019-01-27 12:43:27,737 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742081_1257{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:27,737 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741999_1175{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:27,738 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741999_1175{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:27,741 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742033_1209{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000003_0
2019-01-27 12:43:27,741 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742033_1209{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 5946477
2019-01-27 12:43:27,754 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742054_1230{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:27,754 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741994_1170{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:27,756 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741938_1114{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:27,761 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742033_1209 size 5946477
2019-01-27 12:43:27,761 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742054_1230 size 2229530
2019-01-27 12:43:27,761 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741938_1114 size 10703665
2019-01-27 12:43:27,761 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742081_1257 size 3508393
2019-01-27 12:43:27,761 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741994_1170 size 6651142
2019-01-27 12:43:27,784 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742446_1622{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:27,786 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742446_1622{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:27,918 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:27,918 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:27,994 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:27,994 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:27,994 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:27,994 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:28,192 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742306_1482{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:28,274 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:28,286 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741930_1106{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:28,290 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741930_1106 size 98111810
2019-01-27 12:43:28,293 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741946_1122{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:28,293 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742062_1238{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000009_0
2019-01-27 12:43:28,294 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742062_1238{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 3582739
2019-01-27 12:43:28,294 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741946_1122{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:28,294 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742306_1482{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:28,294 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742084_1260{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:28,295 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742062_1238 size 3582739
2019-01-27 12:43:28,295 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742084_1260 size 3331215
2019-01-27 12:43:28,459 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:28,459 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:28,459 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742036_1212{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:28,459 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:28,459 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:28,459 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742036_1212{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:28,516 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741943_1119{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:28,516 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741943_1119{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:28,573 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:28,687 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:28,698 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742087_1263{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:28,698 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742087_1263{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:28,806 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742067_1243{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:28,808 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742067_1243 size 2220282
2019-01-27 12:43:28,810 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741955_1131{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:28,811 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741980_1156{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:28,812 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741955_1131 size 9024740
2019-01-27 12:43:28,812 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741980_1156 size 7692042
2019-01-27 12:43:28,865 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:28,865 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:28,877 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742024_1200{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:28,878 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742024_1200{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:28,906 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742303_1479{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:28,906 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742303_1479{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:28,965 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:28,966 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:28,966 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:29,098 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:29,099 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:29,105 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742058_1234{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:29,106 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742058_1234{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:29,108 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742410_1586{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:29,154 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742453_1629{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000012_2
2019-01-27 12:43:29,180 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741975_1151{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:29,183 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741929_1105{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:29,184 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742034_1210{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:29,233 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:29,278 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742410_1586{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:29,281 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742302_1478{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:29,282 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742034_1210{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:29,282 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741929_1105 size 98097155
2019-01-27 12:43:29,282 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741975_1151 size 10726158
2019-01-27 12:43:29,387 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:29,388 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:29,388 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:29,388 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:29,421 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742454_1630{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000014_2
2019-01-27 12:43:29,727 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742074_1250{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000009_0
2019-01-27 12:43:29,727 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742079_1255{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:29,728 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742079_1255 size 924270
2019-01-27 12:43:29,728 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742074_1250{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 2413752
2019-01-27 12:43:29,728 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742080_1256{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000003_0
2019-01-27 12:43:29,729 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742080_1256{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 2235525
2019-01-27 12:43:29,733 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742061_1237{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:29,734 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742455_1631{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000012_2
2019-01-27 12:43:29,734 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742061_1237 size 5906361
2019-01-27 12:43:29,734 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742074_1250 size 2413752
2019-01-27 12:43:29,734 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742080_1256 size 2235525
2019-01-27 12:43:29,735 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741979_1155{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:29,737 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741979_1155{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:29,738 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742302_1478{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:29,744 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742456_1632{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000012_2
2019-01-27 12:43:29,748 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742457_1633{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000012_2
2019-01-27 12:43:29,760 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742092_1268{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:29,761 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742092_1268{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:29,900 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742458_1634{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000014_2
2019-01-27 12:43:29,928 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:29,976 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742459_1635{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000012_2
2019-01-27 12:43:30,020 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:30,021 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742460_1636{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000012_2
2019-01-27 12:43:30,021 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:30,021 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:30,022 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:30,022 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742461_1637{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000012_2
2019-01-27 12:43:30,060 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742462_1638{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=out/_tmp.000011_0
2019-01-27 12:43:30,130 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742463_1639{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=drp1/_tmp.000005_0
2019-01-27 12:43:30,135 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742056_1232{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:30,136 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742056_1232{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:30,138 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742305_1481{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:30,140 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742305_1481{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:30,208 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742464_1640{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000012_2
2019-01-27 12:43:30,224 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:30,224 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:30,224 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:30,224 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:30,245 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742083_1259{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:30,246 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742083_1259{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:30,365 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:30,367 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742465_1641{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000009_0
2019-01-27 12:43:30,369 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742284_1460{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:30,419 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741964_1140{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000003_0
2019-01-27 12:43:30,421 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741964_1140{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 10661186
2019-01-27 12:43:30,426 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741964_1140 size 10661186
2019-01-27 12:43:30,430 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742284_1460{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:30,430 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742086_1262{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:30,434 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742086_1262{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:30,443 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742462_1638{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:30,449 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742462_1638 size 275
2019-01-27 12:43:30,475 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742465_1641{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:30,475 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742465_1641{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:30,486 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742463_1639{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:30,489 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742463_1639 size 1185
2019-01-27 12:43:30,584 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=out/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:30,585 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:30,585 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:30,595 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742071_1247{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:30,600 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742071_1247 size 5343324
2019-01-27 12:43:30,601 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742466_1642{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000012_2
2019-01-27 12:43:30,622 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=drp1/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:30,622 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:30,642 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:30,644 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742467_1643{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=drp1/_tmp.000001_0
2019-01-27 12:43:30,654 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742310_1486{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:30,659 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742310_1486{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:30,673 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742468_1644{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000012_2
2019-01-27 12:43:30,710 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742040_1216{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000009_0
2019-01-27 12:43:30,711 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742040_1216{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 3534142
2019-01-27 12:43:30,711 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742040_1216 size 3534142
2019-01-27 12:43:30,753 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:30,769 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742469_1645{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=drp1/_tmp.000011_0
2019-01-27 12:43:30,774 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742039_1215{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000007_0
2019-01-27 12:43:30,774 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742039_1215{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 7587051
2019-01-27 12:43:30,774 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742039_1215 size 7587051
2019-01-27 12:43:30,818 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742470_1646{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000014_2
2019-01-27 12:43:30,834 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742467_1643{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:30,839 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742467_1643{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:30,856 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742035_1211{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000005_0
2019-01-27 12:43:30,857 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742035_1211{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 2510970
2019-01-27 12:43:30,858 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742035_1211 size 2510970
2019-01-27 12:43:30,879 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742287_1463{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:30,883 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742287_1463{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:30,885 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742471_1647{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000014_2
2019-01-27 12:43:30,908 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742472_1648{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000014_2
2019-01-27 12:43:30,915 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742473_1649{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000014_2
2019-01-27 12:43:30,917 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=drp1/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:30,917 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:30,962 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:30,996 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742469_1645{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:30,998 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742469_1645{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:31,016 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741970_1146{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000003_0
2019-01-27 12:43:31,018 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741970_1146{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 9026179
2019-01-27 12:43:31,019 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741970_1146 size 9026179
2019-01-27 12:43:31,024 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742065_1241{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000001_0
2019-01-27 12:43:31,025 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742065_1241{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 2462835
2019-01-27 12:43:31,025 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742065_1241 size 2462835
2019-01-27 12:43:31,056 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=drp1/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:31,074 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742299_1475{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:31,074 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742299_1475{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:31,075 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742474_1650{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000014_2
2019-01-27 12:43:31,113 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742046_1222{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:31,114 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742046_1222{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:31,153 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:31,172 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742475_1651{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000012_2
2019-01-27 12:43:31,186 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742476_1652{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000012_2
2019-01-27 12:43:31,211 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:31,211 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:31,251 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742477_1653{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000012_2
2019-01-27 12:43:31,253 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:31,259 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742292_1468{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:31,261 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742292_1468{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:31,268 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742478_1654{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000014_2
2019-01-27 12:43:31,275 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742479_1655{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000012_2
2019-01-27 12:43:31,284 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742010_1186{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:31,288 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742010_1186 size 5978546
2019-01-27 12:43:31,318 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742480_1656{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000014_2
2019-01-27 12:43:31,337 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:31,337 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:31,346 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741932_1108{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:31,349 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741932_1108{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:31,355 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742481_1657{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000014_2
2019-01-27 12:43:31,367 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:31,374 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741972_1148{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:31,377 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741972_1148 size 7433266
2019-01-27 12:43:31,403 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:31,418 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742482_1658{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000014_2
2019-01-27 12:43:31,425 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742315_1491{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:31,432 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742315_1491 size 836845
2019-01-27 12:43:31,433 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741962_1138{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:31,438 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741962_1138 size 7399756
2019-01-27 12:43:31,485 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:31,499 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742014_1190{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000009_0
2019-01-27 12:43:31,499 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742014_1190{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 5367122
2019-01-27 12:43:31,503 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742014_1190 size 5367122
2019-01-27 12:43:31,512 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:31,512 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:31,512 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:31,513 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:31,517 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742483_1659{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000012_2
2019-01-27 12:43:31,522 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742088_1264{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000007_0
2019-01-27 12:43:31,523 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742088_1264{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 2262957
2019-01-27 12:43:31,524 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742088_1264 size 2262957
2019-01-27 12:43:31,553 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742484_1660{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000014_2
2019-01-27 12:43:31,584 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742485_1661{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000014_2
2019-01-27 12:43:31,617 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742486_1662{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000012_2
2019-01-27 12:43:31,640 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741978_1154{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000001_0
2019-01-27 12:43:31,641 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741978_1154{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 7369278
2019-01-27 12:43:31,643 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742487_1663{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=out/_tmp.000006_1
2019-01-27 12:43:31,643 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741978_1154 size 7369278
2019-01-27 12:43:31,649 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742059_1235{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:31,651 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742059_1235 size 3356453
2019-01-27 12:43:31,656 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741954_1130{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000005_0
2019-01-27 12:43:31,656 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741954_1130{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 9472290
2019-01-27 12:43:31,657 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741954_1130 size 9472290
2019-01-27 12:43:31,662 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742488_1664{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000014_2
2019-01-27 12:43:31,688 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742489_1665{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000014_2
2019-01-27 12:43:31,694 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741948_1124{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000011_0
2019-01-27 12:43:31,695 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741948_1124{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 9535214
2019-01-27 12:43:31,695 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741948_1124 size 9535214
2019-01-27 12:43:31,729 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742490_1666{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000014_2
2019-01-27 12:43:31,745 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742491_1667{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000014_2
2019-01-27 12:43:31,760 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:31,824 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742487_1663{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:31,826 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742492_1668{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000012_2
2019-01-27 12:43:31,826 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742487_1663{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:31,863 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742094_1270{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:31,865 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742094_1270{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:31,873 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742493_1669{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000012_2
2019-01-27 12:43:31,882 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=out/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:31,944 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:31,985 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:31,985 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:31,993 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742494_1670{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=drp1/_tmp.000006_1
2019-01-27 12:43:32,045 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742495_1671{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000014_2
2019-01-27 12:43:32,054 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742496_1672{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=drp1/_tmp.000003_0
2019-01-27 12:43:32,107 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:32,114 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741986_1162{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,114 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742003_1179{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,115 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741986_1162 size 7548697
2019-01-27 12:43:32,115 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742003_1179 size 10607163
2019-01-27 12:43:32,163 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:32,164 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742497_1673{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000014_2
2019-01-27 12:43:32,164 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:32,180 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742498_1674{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000014_2
2019-01-27 12:43:32,198 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742499_1675{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000014_2
2019-01-27 12:43:32,219 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:32,220 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:32,224 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742496_1672{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,226 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742496_1672{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,229 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741967_1143{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,232 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741967_1143 size 9427796
2019-01-27 12:43:32,239 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742494_1670{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:32,262 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742494_1670 size 413
2019-01-27 12:43:32,288 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741997_1173{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,294 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741997_1173 size 5349981
2019-01-27 12:43:32,326 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=drp1/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:32,327 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:32,327 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=drp1/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:32,332 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742500_1676{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000014_2
2019-01-27 12:43:32,335 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742085_1261{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,336 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:32,356 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742085_1261 size 3257132
2019-01-27 12:43:32,356 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742000_1176{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,359 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741931_1107{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,360 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742000_1176 size 5407152
2019-01-27 12:43:32,364 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741931_1107{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,384 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742309_1485{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:32,385 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742309_1485{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:32,388 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:32,388 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:32,389 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742012_1188{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000001_0
2019-01-27 12:43:32,389 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:32,389 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742012_1188{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 5440070
2019-01-27 12:43:32,389 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742073_1249{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,389 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742012_1188 size 5440070
2019-01-27 12:43:32,392 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742073_1249 size 2513105
2019-01-27 12:43:32,395 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:32,400 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:32,407 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742019_1195{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,408 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742019_1195{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,411 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742022_1198{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,412 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742013_1189{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,413 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742022_1198{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,413 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742013_1189{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,414 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:32,415 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742296_1472{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:32,415 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742069_1245{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,416 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742069_1245 size 2299006
2019-01-27 12:43:32,418 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742501_1677{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000014_2
2019-01-27 12:43:32,420 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742296_1472 size 7297182
2019-01-27 12:43:32,421 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:32,422 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741991_1167{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,422 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:32,423 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741991_1167{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,428 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:32,428 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:32,440 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:32,448 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742063_1239{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,450 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742063_1239{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,453 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741950_1126{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,455 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741940_1116{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,467 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742097_1273{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,468 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742290_1466{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000006_1
2019-01-27 12:43:32,469 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742290_1466{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 9369535
2019-01-27 12:43:32,472 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742290_1466 size 9369535
2019-01-27 12:43:32,473 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741950_1126 size 10497373
2019-01-27 12:43:32,473 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741940_1116 size 9790708
2019-01-27 12:43:32,473 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742097_1273 size 862257
2019-01-27 12:43:32,473 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741969_1145{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,473 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741969_1145{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,477 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:32,483 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:32,483 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:32,483 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:32,483 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:32,510 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742502_1678{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=drp1/_tmp.000007_0
2019-01-27 12:43:32,512 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742016_1192{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,514 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742016_1192{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,516 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742048_1224{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,518 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742048_1224 size 3325193
2019-01-27 12:43:32,567 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:32,568 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741945_1121{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,569 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:32,569 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741945_1121 size 9895517
2019-01-27 12:43:32,574 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:32,580 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742051_1227{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,581 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742051_1227 size 1856065
2019-01-27 12:43:32,589 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742503_1679{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000014_2
2019-01-27 12:43:32,594 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742502_1678{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,599 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742502_1678 size 517
2019-01-27 12:43:32,600 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742037_1213{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,602 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742037_1213 size 4077262
2019-01-27 12:43:32,603 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741958_1134{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,603 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:32,607 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741958_1134 size 8908568
2019-01-27 12:43:32,619 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:32,619 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=drp1/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:32,626 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:32,627 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741971_1147{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,627 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741971_1147{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,632 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741965_1141{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,632 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741965_1141 size 6961196
2019-01-27 12:43:32,645 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:32,646 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:32,648 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742082_1258{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,652 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742082_1258 size 1919525
2019-01-27 12:43:32,667 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:32,677 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742504_1680{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000014_2
2019-01-27 12:43:32,700 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742089_1265{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,704 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742093_1269{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,711 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:32,717 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:32,737 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741937_1113{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,742 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:32,748 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742505_1681{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=out/_tmp.000009_0
2019-01-27 12:43:32,749 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741951_1127{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,769 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:32,773 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741952_1128{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000003_0
2019-01-27 12:43:32,784 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742038_1214{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000007_0
2019-01-27 12:43:32,784 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742038_1214{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 7426868
2019-01-27 12:43:32,784 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741951_1127 size 10055667
2019-01-27 12:43:32,784 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741937_1113 size 10089613
2019-01-27 12:43:32,785 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741952_1128{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 10070199
2019-01-27 12:43:32,785 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742089_1265 size 2563397
2019-01-27 12:43:32,785 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742093_1269 size 954089
2019-01-27 12:43:32,785 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742038_1214 size 7426868
2019-01-27 12:43:32,785 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741952_1128 size 10070199
2019-01-27 12:43:32,790 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742506_1682{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000014_2
2019-01-27 12:43:32,810 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:32,813 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742279_1455{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,823 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742278_1454{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,824 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742279_1455{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,841 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:32,841 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:32,846 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742043_1219{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000001_0
2019-01-27 12:43:32,847 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742043_1219{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 4127995
2019-01-27 12:43:32,848 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742043_1219 size 4127995
2019-01-27 12:43:32,869 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741934_1110{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,871 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741934_1110 size 12652355
2019-01-27 12:43:32,877 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742507_1683{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=ill/_tmp.000005_0
2019-01-27 12:43:32,882 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742505_1681{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,883 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742505_1681{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,888 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000011_0 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:32,894 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:32,894 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=out/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:32,920 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742508_1684{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000014_2
2019-01-27 12:43:32,926 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742301_1477{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:32,928 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742301_1477{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:32,932 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742509_1685{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=drp1/_tmp.000009_0
2019-01-27 12:43:32,947 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:32,956 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742507_1683{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,959 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742507_1683 size 125
2019-01-27 12:43:32,964 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=ill/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:32,980 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742300_1476{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:32,983 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742300_1476{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:32,986 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742510_1686{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/-ext-10001/tmpstats-11
2019-01-27 12:43:32,992 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742509_1685{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:32,998 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742509_1685{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,000 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742511_1687{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000012_2
2019-01-27 12:43:33,006 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:33,016 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=drp1/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:33,024 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741935_1111{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,036 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000005_0 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:33,049 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741935_1111 size 12657604
2019-01-27 12:43:33,054 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742291_1467{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:33,055 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742072_1248{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,060 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742072_1248 size 2586078
2019-01-27 12:43:33,062 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:33,067 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:33,076 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742291_1467 size 9952119
2019-01-27 12:43:33,087 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742510_1686{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,088 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742510_1686{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,092 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741988_1164{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,096 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/-ext-10001/tmpstats-11 is closed by DFSClient_attempt_1548592229700_0001_m_000011_0_1657519779_1
2019-01-27 12:43:33,097 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741988_1164 size 7522559
2019-01-27 12:43:33,100 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742312_1488{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:33,112 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:33,115 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742312_1488 size 1946342
2019-01-27 12:43:33,120 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:33,124 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742512_1688{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/-ext-10001/tmpstats-5
2019-01-27 12:43:33,155 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741974_1150{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,159 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741974_1150{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,165 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742289_1465{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:33,169 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742289_1465 size 10103218
2019-01-27 12:43:33,172 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:33,182 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:33,191 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742031_1207{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,193 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:33,193 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742031_1207{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,200 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742512_1688{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,202 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:33,207 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742512_1688{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,212 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742294_1470{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:33,216 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742294_1470{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:33,218 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:33,225 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741959_1135{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,229 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741959_1135 size 10382982
2019-01-27 12:43:33,230 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:33,230 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/-ext-10001/tmpstats-5 is closed by DFSClient_attempt_1548592229700_0001_m_000005_0_1835654388_1
2019-01-27 12:43:33,236 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:33,240 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742021_1197{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,242 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742021_1197{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,252 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742047_1223{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,254 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:33,255 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742047_1223 size 4121455
2019-01-27 12:43:33,259 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:33,264 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742376_1552{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000006_1
2019-01-27 12:43:33,264 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742376_1552{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 125062
2019-01-27 12:43:33,268 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:33,269 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742009_1185{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,272 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742009_1185 size 6987632
2019-01-27 12:43:33,277 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:33,279 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742066_1242{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000007_0
2019-01-27 12:43:33,279 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742066_1242{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 5357125
2019-01-27 12:43:33,281 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742066_1242 size 5357125
2019-01-27 12:43:33,288 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742076_1252{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,290 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742076_1252{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,299 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:33,315 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742070_1246{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,316 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742070_1246{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,319 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741957_1133{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000009_0
2019-01-27 12:43:33,320 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741957_1133{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 9906579
2019-01-27 12:43:33,320 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741957_1133 size 9906579
2019-01-27 12:43:33,326 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742297_1473{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,327 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742297_1473{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,327 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:33,342 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:33,352 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741968_1144{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,352 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741968_1144{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,363 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:33,371 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741936_1112{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,374 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741936_1112{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,380 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000003_0 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:33,387 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741963_1139{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000001_0
2019-01-27 12:43:33,388 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741963_1139{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 10394624
2019-01-27 12:43:33,389 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741963_1139 size 10394624
2019-01-27 12:43:33,476 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742513_1689{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/-ext-10001/tmpstats-3
2019-01-27 12:43:33,517 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742513_1689{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,527 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/-ext-10001/tmpstats-3 is closed by DFSClient_attempt_1548592229700_0001_m_000003_0_805193377_1
2019-01-27 12:43:33,527 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742513_1689 size 2129
2019-01-27 12:43:33,553 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742514_1690{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000014_2
2019-01-27 12:43:33,677 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:33,694 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:33,696 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742515_1691{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=ill/_tmp.000006_1
2019-01-27 12:43:33,729 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742078_1254{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,730 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:33,738 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742078_1254 size 4119139
2019-01-27 12:43:33,753 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:33,983 WARN org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo from 172.18.0.3:49140 Call#286 Retry#0: output error
2019-01-27 12:43:33,983 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000 caught an exception
java.nio.channels.ClosedChannelException
	at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:270)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:479)
	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:2583)
	at org.apache.hadoop.ipc.Server.access$1900(Server.java:136)
	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:986)
	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1051)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2104)
2019-01-27 12:43:33,984 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741996_1172{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:33,984 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741996_1172{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:34,112 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742515_1691{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:34,113 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742515_1691{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:34,142 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:34,142 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:34,440 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=ill/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:34,450 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742008_1184{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:34,450 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742008_1184{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:34,461 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742516_1692{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000002_2
2019-01-27 12:43:34,503 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742278_1454 to datanode(s) 172.18.0.3:50010
2019-01-27 12:43:34,504 INFO BlockStateChange: BLOCK* ask 172.18.0.3:50010 to replicate blk_1073742376_1552 to datanode(s) 172.18.0.2:50010
2019-01-27 12:43:34,560 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:34,642 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741949_1125{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:34,643 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741949_1125{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:34,750 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:34,875 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742090_1266{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:34,875 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742090_1266 size 1907546
2019-01-27 12:43:34,876 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741984_1160{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000001_0
2019-01-27 12:43:34,877 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741984_1160{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 7007785
2019-01-27 12:43:34,877 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742278_1454 size 123522
2019-01-27 12:43:34,878 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741984_1160 size 7007785
2019-01-27 12:43:35,113 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742286_1462{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:35,114 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742286_1462 size 12656541
2019-01-27 12:43:35,124 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:35,136 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742283_1459{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:35,137 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742283_1459{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:35,161 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000006_1 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:35,230 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:35,241 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742029_1205{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:35,246 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742029_1205 size 6967301
2019-01-27 12:43:35,458 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:35,575 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:35,804 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742017_1193{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:35,804 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742017_1193{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:35,805 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742517_1693{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=ill/_tmp.000009_0
2019-01-27 12:43:35,948 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742285_1461{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:35,949 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742285_1461{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:36,016 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:36,164 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742517_1693{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:36,165 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742517_1693{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:36,289 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:36,322 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=ill/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:36,393 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742518_1694{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/-ext-10001/tmpstats-6
2019-01-27 12:43:36,593 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741942_1118{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:36,596 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741942_1118{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:36,824 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073741941_1117{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:36,825 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742518_1694{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/-ext-10001/tmpstats-6
2019-01-27 12:43:36,826 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742280_1456{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:36,826 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742518_1694{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 2251
2019-01-27 12:43:36,856 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742280_1456 size 141192
2019-01-27 12:43:36,856 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073741941_1117 size 12762327
2019-01-27 12:43:36,856 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742518_1694 size 2251
2019-01-27 12:43:36,917 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000009_0 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:36,949 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:36,949 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000001_0 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:37,174 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742376_1552 size 125062
2019-01-27 12:43:37,370 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/-ext-10001/tmpstats-6 is closed by DFSClient_attempt_1548592229700_0001_m_000006_1_650024531_1
2019-01-27 12:43:37,633 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742519_1695{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/-ext-10001/tmpstats-9
2019-01-27 12:43:37,805 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742001_1177{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:37,820 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742001_1177{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:37,988 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742520_1696{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/-ext-10001/tmpstats-1
2019-01-27 12:43:38,254 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742519_1695{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:38,255 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742519_1695{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:38,360 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000007_0 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:38,400 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/-ext-10001/tmpstats-9 is closed by DFSClient_attempt_1548592229700_0001_m_000009_0_-1856591426_1
2019-01-27 12:43:39,044 WARN org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo from 172.18.0.3:49150 Call#308 Retry#0: output error
2019-01-27 12:43:39,044 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000 caught an exception
java.nio.channels.ClosedChannelException
	at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:270)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:479)
	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:2583)
	at org.apache.hadoop.ipc.Server.access$1900(Server.java:136)
	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:986)
	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1051)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2104)
2019-01-27 12:43:39,044 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742520_1696{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:39,044 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742520_1696{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:39,074 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742521_1697{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/-ext-10001/tmpstats-7
2019-01-27 12:43:39,130 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/-ext-10001/tmpstats-1 is closed by DFSClient_attempt_1548592229700_0001_m_000001_0_-1148118986_1
2019-01-27 12:43:39,534 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742521_1697{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:39,536 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742521_1697{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:43:39,703 WARN org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo from 172.18.0.3:49144 Call#292 Retry#0: output error
2019-01-27 12:43:39,703 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000 caught an exception
java.nio.channels.ClosedChannelException
	at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:270)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:479)
	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:2583)
	at org.apache.hadoop.ipc.Server.access$1900(Server.java:136)
	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:986)
	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1051)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2104)
2019-01-27 12:43:39,829 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/-ext-10001/tmpstats-7 is closed by DFSClient_attempt_1548592229700_0001_m_000007_0_-1690715424_1
2019-01-27 12:43:47,795 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742522_1698{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000004_1
2019-01-27 12:43:48,424 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742523_1699{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000000_2
2019-01-27 12:43:48,866 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742524_1700{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000015_0
2019-01-27 12:43:48,942 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742525_1701{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000013_0
2019-01-27 12:43:49,261 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742526_1702{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000004_1
2019-01-27 12:43:49,303 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742527_1703{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000004_1
2019-01-27 12:43:49,459 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742528_1704{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000004_1
2019-01-27 12:43:49,477 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742529_1705{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000010_3
2019-01-27 12:43:49,503 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742530_1706{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000004_1
2019-01-27 12:43:49,812 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742531_1707{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000004_1
2019-01-27 12:43:49,873 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742532_1708{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000004_1
2019-01-27 12:43:49,944 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742533_1709{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000004_1
2019-01-27 12:43:50,065 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742534_1710{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000004_1
2019-01-27 12:43:50,250 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742535_1711{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000004_1
2019-01-27 12:43:50,267 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742536_1712{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000004_1
2019-01-27 12:43:50,492 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742537_1713{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000015_0
2019-01-27 12:43:50,543 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742538_1714{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000004_1
2019-01-27 12:43:50,571 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742539_1715{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000016_2
2019-01-27 12:43:50,596 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742540_1716{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000015_0
2019-01-27 12:43:50,604 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742541_1717{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000004_1
2019-01-27 12:43:50,612 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742542_1718{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000004_1
2019-01-27 12:43:50,616 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742543_1719{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000004_1
2019-01-27 12:43:50,756 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742544_1720{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000015_0
2019-01-27 12:43:50,835 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742545_1721{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000004_1
2019-01-27 12:43:50,857 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742546_1722{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000000_2
2019-01-27 12:43:50,893 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742547_1723{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000004_1
2019-01-27 12:43:50,955 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742548_1724{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000013_0
2019-01-27 12:43:50,966 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742549_1725{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000013_0
2019-01-27 12:43:50,967 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742550_1726{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000015_0
2019-01-27 12:43:51,012 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742551_1727{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000004_1
2019-01-27 12:43:51,154 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742552_1728{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000015_0
2019-01-27 12:43:51,197 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742553_1729{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000015_0
2019-01-27 12:43:51,208 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742554_1730{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000015_0
2019-01-27 12:43:51,210 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742555_1731{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000015_0
2019-01-27 12:43:51,273 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742556_1732{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000015_0
2019-01-27 12:43:51,302 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742557_1733{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000008_2
2019-01-27 12:43:51,431 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742558_1734{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000000_2
2019-01-27 12:43:51,437 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742559_1735{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000013_0
2019-01-27 12:43:51,473 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742560_1736{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000013_0
2019-01-27 12:43:51,502 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742561_1737{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000013_0
2019-01-27 12:43:51,504 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742562_1738{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000013_0
2019-01-27 12:43:51,531 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742563_1739{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000015_0
2019-01-27 12:43:51,550 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742564_1740{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000013_0
2019-01-27 12:43:51,591 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742565_1741{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000015_0
2019-01-27 12:43:51,610 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742566_1742{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000015_0
2019-01-27 12:43:51,622 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742567_1743{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000015_0
2019-01-27 12:43:51,632 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742568_1744{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000000_2
2019-01-27 12:43:51,640 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742569_1745{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000004_1
2019-01-27 12:43:51,666 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742570_1746{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000004_1
2019-01-27 12:43:51,667 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742571_1747{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000004_1
2019-01-27 12:43:51,745 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742572_1748{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000000_2
2019-01-27 12:43:51,758 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742573_1749{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000013_0
2019-01-27 12:43:51,759 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742574_1750{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000015_0
2019-01-27 12:43:51,781 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742575_1751{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000004_1
2019-01-27 12:43:51,813 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742576_1752{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000010_3
2019-01-27 12:43:52,074 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742577_1753{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000013_0
2019-01-27 12:43:52,075 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742578_1754{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000004_1
2019-01-27 12:43:52,303 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742579_1755{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000015_0
2019-01-27 12:43:52,309 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742580_1756{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000013_0
2019-01-27 12:43:52,525 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742581_1757{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000013_0
2019-01-27 12:43:52,766 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742582_1758{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000015_0
2019-01-27 12:43:52,767 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742583_1759{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000013_0
2019-01-27 12:43:52,768 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742584_1760{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000013_0
2019-01-27 12:43:52,769 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742585_1761{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000010_3
2019-01-27 12:43:52,769 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742586_1762{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000013_0
2019-01-27 12:43:52,783 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742587_1763{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000010_3
2019-01-27 12:43:52,787 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742588_1764{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000010_3
2019-01-27 12:43:53,003 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742589_1765{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000010_3
2019-01-27 12:43:53,062 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742590_1766{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000010_3
2019-01-27 12:43:53,063 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742591_1767{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000010_3
2019-01-27 12:43:53,293 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742592_1768{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000010_3
2019-01-27 12:43:53,534 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742593_1769{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000010_3
2019-01-27 12:43:53,534 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742594_1770{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000010_3
2019-01-27 12:43:53,535 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742595_1771{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000010_3
2019-01-27 12:43:53,535 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742596_1772{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000010_3
2019-01-27 12:43:53,535 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742597_1773{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000000_2
2019-01-27 12:43:53,536 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742598_1774{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000000_2
2019-01-27 12:43:53,536 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742599_1775{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000010_3
2019-01-27 12:43:53,537 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742600_1776{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000000_2
2019-01-27 12:43:53,537 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742601_1777{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000000_2
2019-01-27 12:43:53,576 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742602_1778{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000010_3
2019-01-27 12:43:53,853 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742603_1779{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000010_3
2019-01-27 12:43:53,854 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742604_1780{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000010_3
2019-01-27 12:43:53,855 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742605_1781{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000010_3
2019-01-27 12:43:53,855 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742606_1782{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000010_3
2019-01-27 12:43:53,855 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742607_1783{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000010_3
2019-01-27 12:43:53,856 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742608_1784{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000010_3
2019-01-27 12:43:53,856 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742609_1785{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000010_3
2019-01-27 12:43:53,856 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742610_1786{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000010_3
2019-01-27 12:43:54,083 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742611_1787{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000010_3
2019-01-27 12:43:54,084 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742612_1788{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000010_3
2019-01-27 12:43:54,523 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742613_1789{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000010_3
2019-01-27 12:43:54,524 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742614_1790{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000010_3
2019-01-27 12:43:54,525 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742615_1791{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000012_2
2019-01-27 12:43:54,527 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742616_1792{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000000_2
2019-01-27 12:43:54,529 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742617_1793{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000000_2
2019-01-27 12:43:54,533 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742618_1794{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000004_1
2019-01-27 12:43:54,534 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742619_1795{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000000_2
2019-01-27 12:43:54,534 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742620_1796{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000004_1
2019-01-27 12:43:54,534 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742621_1797{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000000_2
2019-01-27 12:43:54,535 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742622_1798{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000000_2
2019-01-27 12:43:54,731 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742623_1799{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000004_1
2019-01-27 12:43:55,274 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742624_1800{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000004_1
2019-01-27 12:43:55,504 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742625_1801{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000015_0
2019-01-27 12:43:55,505 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742626_1802{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000015_0
2019-01-27 12:43:55,505 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742627_1803{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000015_0
2019-01-27 12:43:55,506 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742628_1804{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000015_0
2019-01-27 12:43:55,506 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742629_1805{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000015_0
2019-01-27 12:43:55,506 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742630_1806{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000015_0
2019-01-27 12:43:55,506 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742631_1807{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000015_0
2019-01-27 12:43:55,507 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742632_1808{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000015_0
2019-01-27 12:43:55,507 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742633_1809{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000015_0
2019-01-27 12:43:55,615 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742634_1810{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000000_2
2019-01-27 12:43:55,617 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742635_1811{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000000_2
2019-01-27 12:43:55,617 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742636_1812{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000000_2
2019-01-27 12:43:55,617 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742637_1813{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000013_0
2019-01-27 12:43:55,618 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742638_1814{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000015_0
2019-01-27 12:43:55,618 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742639_1815{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000013_0
2019-01-27 12:43:55,843 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742640_1816{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000000_2
2019-01-27 12:43:55,845 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742641_1817{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000013_0
2019-01-27 12:43:55,845 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742642_1818{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000013_0
2019-01-27 12:43:55,845 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742643_1819{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000013_0
2019-01-27 12:43:55,846 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742644_1820{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000013_0
2019-01-27 12:43:55,846 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742645_1821{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000013_0
2019-01-27 12:43:56,063 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742646_1822{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000013_0
2019-01-27 12:43:56,064 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742647_1823{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000013_0
2019-01-27 12:43:56,064 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742648_1824{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000013_0
2019-01-27 12:43:56,065 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742649_1825{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000013_0
2019-01-27 12:43:56,127 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742650_1826{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000013_0
2019-01-27 12:43:56,354 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742651_1827{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000000_2
2019-01-27 12:43:56,357 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742652_1828{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000000_2
2019-01-27 12:43:56,357 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742653_1829{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000000_2
2019-01-27 12:43:56,357 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742654_1830{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000000_2
2019-01-27 12:43:56,358 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742655_1831{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000000_2
2019-01-27 12:43:56,743 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742656_1832{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000000_2
2019-01-27 12:43:56,743 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742657_1833{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000000_2
2019-01-27 12:43:56,965 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742658_1834{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000000_2
2019-01-27 12:43:56,965 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742659_1835{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000016_2
2019-01-27 12:43:56,965 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742660_1836{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000016_2
2019-01-27 12:43:56,966 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742661_1837{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000016_2
2019-01-27 12:43:56,966 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742662_1838{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000016_2
2019-01-27 12:43:57,118 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742663_1839{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000016_2
2019-01-27 12:43:57,737 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742664_1840{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000008_2
2019-01-27 12:43:57,754 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742665_1841{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000000_2
2019-01-27 12:43:57,804 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742666_1842{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000008_2
2019-01-27 12:43:57,842 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742667_1843{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000016_2
2019-01-27 12:43:57,855 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742668_1844{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000016_2
2019-01-27 12:43:57,931 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742669_1845{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000016_2
2019-01-27 12:43:57,967 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:57,967 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:57,967 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:57,967 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742670_1846{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000008_2
2019-01-27 12:43:57,987 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:57,988 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:57,988 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:57,988 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742671_1847{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000016_2
2019-01-27 12:43:58,007 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:58,089 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:58,089 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:58,089 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742672_1848{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000008_2
2019-01-27 12:43:58,042 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:58,033 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:58,092 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:58,092 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:58,092 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:58,092 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:58,092 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742673_1849{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000016_2
2019-01-27 12:43:58,092 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742674_1850{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000016_2
2019-01-27 12:43:58,287 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:58,287 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:58,287 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:58,288 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742675_1851{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000004_1
2019-01-27 12:43:58,289 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:58,290 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:58,290 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:58,290 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742676_1852{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000016_2
2019-01-27 12:43:58,332 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:58,332 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:58,332 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:58,332 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742677_1853{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000016_2
2019-01-27 12:43:58,406 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:58,406 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:58,406 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:58,406 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742678_1854{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000010_3
2019-01-27 12:43:58,432 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:58,432 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:58,432 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:58,432 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742679_1855{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000016_2
2019-01-27 12:43:58,516 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:58,516 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:58,516 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:58,518 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742680_1856{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000016_2
2019-01-27 12:43:58,766 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:58,766 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:58,766 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:58,767 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742681_1857{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000015_0
2019-01-27 12:43:58,973 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:58,973 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:58,973 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:58,974 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742682_1858{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000013_0
2019-01-27 12:43:58,986 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:58,986 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:58,986 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:58,986 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742683_1859{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000016_2
2019-01-27 12:43:58,986 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:58,986 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:58,986 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:58,986 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742684_1860{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000008_2
2019-01-27 12:43:58,987 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:58,987 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:58,988 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:58,988 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:58,988 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:58,988 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:58,988 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:58,988 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:58,988 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:58,988 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:58,988 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:59,214 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742685_1861{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000008_2
2019-01-27 12:43:59,214 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:59,214 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742686_1862{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000008_2
2019-01-27 12:43:59,214 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742687_1863{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000008_2
2019-01-27 12:43:59,215 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:59,215 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:59,215 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:59,215 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:59,215 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:59,215 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:59,215 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:59,215 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:59,215 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:59,215 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:59,215 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:59,216 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:59,216 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:59,216 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742688_1864{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000008_2
2019-01-27 12:43:59,215 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:59,216 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:59,216 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:59,216 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:59,216 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:59,385 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742689_1865{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000008_2
2019-01-27 12:43:59,386 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742690_1866{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000008_2
2019-01-27 12:43:59,386 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742691_1867{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000008_2
2019-01-27 12:43:59,386 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742692_1868{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000008_2
2019-01-27 12:43:59,386 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742693_1869{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000008_2
2019-01-27 12:43:59,387 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742694_1870{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000008_2
2019-01-27 12:43:59,387 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:59,387 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:59,387 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:59,388 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:53520 Call#117 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000008_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:59,388 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:59,388 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:59,388 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:59,389 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:53520 Call#118 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000008_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:59,389 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:59,389 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:59,389 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:59,390 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:53520 Call#119 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000008_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:59,390 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:59,390 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:59,390 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:59,390 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:53462 Call#119 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000016_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:59,391 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:59,391 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:59,391 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:59,392 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:53520 Call#120 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000008_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:59,393 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:59,393 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:59,393 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:59,393 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:53520 Call#121 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000008_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:59,394 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:59,394 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:59,394 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:59,394 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:53520 Call#122 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000008_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:59,395 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:59,395 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:59,396 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:59,396 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:53520 Call#123 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000008_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:59,397 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:59,397 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:59,397 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:59,397 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:53520 Call#124 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000008_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:59,398 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:59,398 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:59,398 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:59,398 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:53520 Call#125 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000008_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:59,399 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:59,399 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:59,399 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:59,399 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:53520 Call#126 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000008_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:59,614 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:59,614 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:59,614 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:59,614 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:53462 Call#120 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000016_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:59,615 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:59,615 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:59,615 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:59,615 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:53520 Call#127 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000008_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:59,682 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:43:59,682 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:43:59,682 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:43:59,682 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52988 Call#146 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000014_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:43:59,827 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742491_1667{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:43:59,843 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742491_1667{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:00,088 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000014_2 is closed by DFSClient_attempt_1548592229700_0001_m_000014_2_1368573603_1
2019-01-27 12:44:00,244 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:00,244 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:00,244 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:00,244 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742695_1871{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000000_2
2019-01-27 12:44:01,000 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742480_1656{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:01,004 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742480_1656{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:01,284 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000014_2 is closed by DFSClient_attempt_1548592229700_0001_m_000014_2_1368573603_1
2019-01-27 12:44:01,875 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742470_1646{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:02,036 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742470_1646{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:02,643 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000014_2 is closed by DFSClient_attempt_1548592229700_0001_m_000014_2_1368573603_1
2019-01-27 12:44:02,679 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742484_1660{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:02,748 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742484_1660{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:02,874 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000014_2 is closed by DFSClient_attempt_1548592229700_0001_m_000014_2_1368573603_1
2019-01-27 12:44:03,014 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742488_1664{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:03,178 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742488_1664 size 3150261
2019-01-27 12:44:03,348 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000014_2 is closed by DFSClient_attempt_1548592229700_0001_m_000014_2_1368573603_1
2019-01-27 12:44:03,404 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742499_1675{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:03,628 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742499_1675{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:04,009 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000014_2 is closed by DFSClient_attempt_1548592229700_0001_m_000014_2_1368573603_1
2019-01-27 12:44:04,326 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742501_1677{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:04,326 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742501_1677{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:04,564 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000014_2 is closed by DFSClient_attempt_1548592229700_0001_m_000014_2_1368573603_1
2019-01-27 12:44:04,974 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742506_1682{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:04,974 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742506_1682{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:05,094 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000014_2 is closed by DFSClient_attempt_1548592229700_0001_m_000014_2_1368573603_1
2019-01-27 12:44:05,177 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:05,177 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:05,177 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:05,178 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:52988 Call#157 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000014_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:05,377 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742458_1634{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:05,382 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742458_1634{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:05,650 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000014_2 is closed by DFSClient_attempt_1548592229700_0001_m_000014_2_1368573603_1
2019-01-27 12:44:05,695 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742482_1658{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:05,889 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742482_1658{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:05,899 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742694_1870{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:06,125 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742694_1870 to datanode(s) 172.18.0.3:50010
2019-01-27 12:44:06,238 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000008_2 is closed by DFSClient_attempt_1548592229700_0001_m_000008_2_-1880743960_1
2019-01-27 12:44:06,242 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742692_1868{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:06,370 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742668_1844{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:06,595 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742668_1844 size 114170
2019-01-27 12:44:06,596 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742426_1602{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:06,824 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000008_2 is closed by DFSClient_attempt_1548592229700_0001_m_000008_2_-1880743960_1
2019-01-27 12:44:06,827 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742689_1865{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:06,944 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000016_2 is closed by DFSClient_attempt_1548592229700_0001_m_000016_2_268781814_1
2019-01-27 12:44:06,952 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742683_1859{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:07,011 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000008_2 is closed by DFSClient_attempt_1548592229700_0001_m_000008_2_-1880743960_1
2019-01-27 12:44:07,012 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:07,070 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000016_2 is closed by DFSClient_attempt_1548592229700_0001_m_000016_2_268781814_1
2019-01-27 12:44:07,293 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742662_1838{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:07,352 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742666_1842{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:07,365 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742662_1838{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:07,369 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742666_1842{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:07,521 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000016_2 is closed by DFSClient_attempt_1548592229700_0001_m_000016_2_268781814_1
2019-01-27 12:44:07,521 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000008_2 is closed by DFSClient_attempt_1548592229700_0001_m_000008_2_-1880743960_1
2019-01-27 12:44:07,684 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742677_1853{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:07,809 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742690_1866{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000008_2
2019-01-27 12:44:07,809 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742690_1866{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 226468
2019-01-27 12:44:07,824 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742424_1600{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:07,905 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000016_2 is closed by DFSClient_attempt_1548592229700_0001_m_000016_2_268781814_1
2019-01-27 12:44:08,044 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742424_1600{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:08,100 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742673_1849{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000016_2
2019-01-27 12:44:08,264 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:08,353 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742673_1849{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 105634
2019-01-27 12:44:08,630 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742694_1870 size 192438
2019-01-27 12:44:08,759 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000008_2 is closed by DFSClient_attempt_1548592229700_0001_m_000008_2_-1880743960_1
2019-01-27 12:44:08,850 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000016_2 is closed by DFSClient_attempt_1548592229700_0001_m_000016_2_268781814_1
2019-01-27 12:44:08,996 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742421_1597{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:08,996 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742421_1597{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:09,074 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:09,075 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:09,075 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:09,075 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:53462 Call#130 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000016_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:09,085 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:09,086 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:09,086 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:09,086 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:53520 Call#139 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000008_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:09,126 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742692_1868 to datanode(s) 172.18.0.3:50010
2019-01-27 12:44:09,126 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742426_1602 to datanode(s) 172.18.0.3:50010
2019-01-27 12:44:09,126 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742689_1865 to datanode(s) 172.18.0.3:50010
2019-01-27 12:44:09,126 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742683_1859 to datanode(s) 172.18.0.3:50010
2019-01-27 12:44:09,190 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742557_1733{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:09,195 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742557_1733{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:09,310 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:09,394 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000008_2 is closed by DFSClient_attempt_1548592229700_0001_m_000008_2_-1880743960_1
2019-01-27 12:44:09,407 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742663_1839{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:09,407 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742663_1839{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:09,407 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742687_1863{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000008_2
2019-01-27 12:44:09,410 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742687_1863{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 261474
2019-01-27 12:44:09,596 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000016_2 is closed by DFSClient_attempt_1548592229700_0001_m_000016_2_268781814_1
2019-01-27 12:44:09,601 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742413_1589{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:09,803 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742674_1850{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:09,804 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742413_1589 size 11392822
2019-01-27 12:44:09,954 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:10,073 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000016_2 is closed by DFSClient_attempt_1548592229700_0001_m_000016_2_268781814_1
2019-01-27 12:44:10,073 WARN org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.complete from 172.18.0.2:53462 Call#135 Retry#0: output error
2019-01-27 12:44:10,073 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000 caught an exception
java.nio.channels.ClosedChannelException
	at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:270)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:479)
	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:2583)
	at org.apache.hadoop.ipc.Server.access$1900(Server.java:136)
	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:986)
	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1051)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2104)
2019-01-27 12:44:10,212 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742422_1598{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:10,219 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742422_1598 size 6650881
2019-01-27 12:44:10,466 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:10,888 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742432_1608{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:10,890 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742432_1608{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:11,116 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 4518 Total time for transactions(ms): 480 Number of transactions batched in Syncs: 1051 Number of syncs: 1733 SyncTimes(ms): 119031 
2019-01-27 12:44:11,129 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742692_1868 size 258860
2019-01-27 12:44:11,167 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:11,168 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:11,168 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:11,168 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742696_1872{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000017_0
2019-01-27 12:44:11,259 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:11,578 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742433_1609{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:11,584 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742433_1609{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:11,787 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:12,127 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742677_1853 to datanode(s) 172.18.0.3:50010
2019-01-27 12:44:12,127 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742690_1866 to datanode(s) 172.18.0.3:50010
2019-01-27 12:44:12,260 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:12,260 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:12,260 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:12,260 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742697_1873{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000021_0
2019-01-27 12:44:12,932 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742435_1611{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:13,076 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742435_1611{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:13,325 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:13,492 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:13,492 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:13,703 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:13,703 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742698_1874{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000002_2
2019-01-27 12:44:13,970 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:13,970 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:13,970 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:13,970 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742699_1875{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000019_0
2019-01-27 12:44:14,135 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742689_1865 size 245876
2019-01-27 12:44:14,275 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742698_1874{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:14,592 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:14,725 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742425_1601{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:14,928 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:15,215 INFO BlockStateChange: BLOCK* ask 172.18.0.3:50010 to replicate blk_1073742698_1874 to datanode(s) 172.18.0.2:50010
2019-01-27 12:44:15,215 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742425_1601 to datanode(s) 172.18.0.3:50010
2019-01-27 12:44:15,476 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742426_1602 size 5067203
2019-01-27 12:44:15,535 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742430_1606{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:15,537 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742430_1606{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:15,748 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:15,810 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742427_1603{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:15,962 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:15,984 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:15,984 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:15,984 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:15,985 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742700_1876{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000021_0
2019-01-27 12:44:16,157 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:16,157 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:16,157 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:16,158 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742701_1877{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000017_0
2019-01-27 12:44:16,164 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742698_1874 size 9938
2019-01-27 12:44:16,182 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742702_1878{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000021_0
2019-01-27 12:44:16,186 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742419_1595{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:16,250 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742419_1595{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:16,251 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742703_1879{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000021_0
2019-01-27 12:44:16,281 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:16,281 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:16,281 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:16,281 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742704_1880{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000021_0
2019-01-27 12:44:16,383 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:16,383 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:16,383 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:16,383 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742705_1881{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000021_0
2019-01-27 12:44:16,435 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:16,596 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:16,596 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:16,596 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:16,596 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742706_1882{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000017_0
2019-01-27 12:44:16,797 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:16,797 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:16,797 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:16,797 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:16,797 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:16,797 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:16,798 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742411_1587{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:16,798 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742707_1883{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000017_0
2019-01-27 12:44:16,798 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742708_1884{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000017_0
2019-01-27 12:44:16,803 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:16,944 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:16,944 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:16,945 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742709_1885{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000017_0
2019-01-27 12:44:16,948 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742411_1587 size 98271166
2019-01-27 12:44:17,090 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:17,148 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742710_1886{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000019_0
2019-01-27 12:44:17,240 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742711_1887{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000021_0
2019-01-27 12:44:17,263 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742712_1888{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000021_0
2019-01-27 12:44:17,263 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:17,263 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:17,263 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:17,263 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.3:49866 Call#112 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000021_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:17,311 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:17,311 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:17,312 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:17,312 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.3:49740 Call#111 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000017_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:17,333 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742436_1612{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:17,334 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742436_1612 size 2211189
2019-01-27 12:44:17,365 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742713_1889{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000017_0
2019-01-27 12:44:17,456 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:17,456 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:17,456 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:17,456 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.3:49764 Call#107 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000019_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:17,474 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:17,474 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:17,474 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:17,474 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.3:49764 Call#108 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000019_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:17,481 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:17,534 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:17,534 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:17,534 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:17,534 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.3:49764 Call#109 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000019_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:17,536 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:17,536 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:17,536 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:17,536 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.3:49866 Call#113 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000021_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:17,565 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742677_1853 size 99257
2019-01-27 12:44:17,681 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742414_1590{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:17,683 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742414_1590{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:17,720 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742683_1859 size 75825
2019-01-27 12:44:17,845 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:17,939 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742714_1890{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000019_0
2019-01-27 12:44:18,153 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:18,184 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:18,184 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:18,185 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742715_1891{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000021_0
2019-01-27 12:44:18,216 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742673_1849 to datanode(s) 172.18.0.3:50010
2019-01-27 12:44:18,216 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742674_1850 to datanode(s) 172.18.0.3:50010
2019-01-27 12:44:18,225 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742417_1593{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:18,317 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742417_1593{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:18,609 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:18,618 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742715_1891{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:18,651 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742714_1890{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:18,652 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742714_1890{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:18,813 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742716_1892{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000018_0
2019-01-27 12:44:18,827 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000019_0 is closed by DFSClient_attempt_1548592229700_0001_m_000019_0_381378038_1
2019-01-27 12:44:18,827 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000021_0 is closed by DFSClient_attempt_1548592229700_0001_m_000021_0_-1977733260_1
2019-01-27 12:44:18,828 WARN org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.complete from 172.18.0.3:49866 Call#118 Retry#0: output error
2019-01-27 12:44:18,828 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000 caught an exception
java.nio.channels.ClosedChannelException
	at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:270)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:479)
	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:2583)
	at org.apache.hadoop.ipc.Server.access$1900(Server.java:136)
	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:986)
	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1051)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2104)
2019-01-27 12:44:18,831 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742717_1893{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000019_0
2019-01-27 12:44:18,853 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742431_1607{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:18,855 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742431_1607{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:18,867 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:18,867 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:18,867 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:18,868 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742718_1894{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000000_2
2019-01-27 12:44:19,065 WARN org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.3:49764 Call#114 Retry#0: output error
2019-01-27 12:44:19,065 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:19,065 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000 caught an exception
java.nio.channels.ClosedChannelException
	at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:270)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:479)
	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:2583)
	at org.apache.hadoop.ipc.Server.access$1900(Server.java:136)
	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:986)
	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1051)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2104)
2019-01-27 12:44:19,150 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:19,151 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:19,151 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:19,151 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.3:49740 Call#118 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000017_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:19,234 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:19,234 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:19,234 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:19,234 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.3:49740 Call#117 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000017_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:19,234 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:19,234 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:19,234 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:19,234 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:19,235 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.3:49740 Call#116 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000017_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:19,235 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:19,235 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:19,235 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:19,235 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.3:49740 Call#114 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000017_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:19,236 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:19,236 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:19,236 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.3:49740 Call#115 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000017_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:19,236 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:19,236 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:19,236 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:19,237 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.3:49740 Call#112 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000017_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:19,236 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:19,237 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:19,237 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:19,237 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.3:49740 Call#113 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000017_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:19,261 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742438_1614{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:19,280 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742438_1614{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:19,280 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:19,281 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:19,281 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:19,281 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742719_1895{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000017_0
2019-01-27 12:44:19,286 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:19,286 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:19,286 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:19,286 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.3:49740 Call#122 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000017_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:19,287 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:19,287 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:19,287 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:19,287 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.3:49740 Call#121 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000017_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:19,532 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:19,532 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:19,533 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:19,533 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.3:49740 Call#125 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000017_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:19,544 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:19,544 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:19,544 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:19,544 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.3:49740 Call#126 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000017_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:19,548 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:19,548 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:19,548 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:19,549 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.3:49740 Call#127 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000017_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:19,553 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742696_1872{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:19,575 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:19,745 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000017_0 is closed by DFSClient_attempt_1548592229700_0001_m_000017_0_1407604150_1
2019-01-27 12:44:19,752 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742701_1877{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:19,934 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:19,934 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:19,935 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:19,935 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742720_1896{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=out/_tmp.000002_2
2019-01-27 12:44:19,958 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000017_0 is closed by DFSClient_attempt_1548592229700_0001_m_000017_0_1407604150_1
2019-01-27 12:44:19,958 WARN org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.complete from 172.18.0.3:49740 Call#129 Retry#0: output error
2019-01-27 12:44:19,958 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000 caught an exception
java.nio.channels.ClosedChannelException
	at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:270)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:479)
	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:2583)
	at org.apache.hadoop.ipc.Server.access$1900(Server.java:136)
	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:986)
	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1051)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2104)
2019-01-27 12:44:20,140 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742690_1866 size 226468
2019-01-27 12:44:20,346 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742720_1896{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:20,554 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=out/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:20,667 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742721_1897{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=drp1/_tmp.000002_2
2019-01-27 12:44:21,000 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742721_1897{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:21,000 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742721_1897{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:21,096 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742722_1898{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000018_0
2019-01-27 12:44:21,186 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=drp1/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:21,217 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742427_1603 to datanode(s) 172.18.0.3:50010
2019-01-27 12:44:21,217 INFO BlockStateChange: BLOCK* ask 172.18.0.3:50010 to replicate blk_1073742715_1891 to datanode(s) 172.18.0.2:50010
2019-01-27 12:44:21,217 INFO BlockStateChange: BLOCK* ask 172.18.0.3:50010 to replicate blk_1073742696_1872 to datanode(s) 172.18.0.2:50010
2019-01-27 12:44:21,217 INFO BlockStateChange: BLOCK* ask 172.18.0.3:50010 to replicate blk_1073742701_1877 to datanode(s) 172.18.0.2:50010
2019-01-27 12:44:21,248 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:21,248 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:21,248 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:21,248 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742723_1899{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000018_0
2019-01-27 12:44:21,363 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742434_1610{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:21,365 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742434_1610 size 2542370
2019-01-27 12:44:21,509 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:21,651 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742420_1596{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000002_2
2019-01-27 12:44:21,652 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742420_1596{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 7403891
2019-01-27 12:44:21,652 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742420_1596 size 7403891
2019-01-27 12:44:22,151 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742724_1900{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000018_0
2019-01-27 12:44:22,165 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:22,165 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:22,165 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:22,165 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742725_1901{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000018_0
2019-01-27 12:44:22,166 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:22,166 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:22,166 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:22,166 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742726_1902{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000018_0
2019-01-27 12:44:22,188 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:22,305 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742415_1591{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:22,310 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742415_1591{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:22,316 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742715_1891 size 41253
2019-01-27 12:44:22,393 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742727_1903{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000018_0
2019-01-27 12:44:22,407 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742728_1904{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000018_0
2019-01-27 12:44:22,465 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:22,797 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:22,797 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:22,797 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:22,797 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:22,797 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:22,797 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:22,797 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742729_1905{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000018_0
2019-01-27 12:44:22,798 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742730_1906{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000018_0
2019-01-27 12:44:22,915 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742428_1604{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:22,933 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742731_1907{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000018_0
2019-01-27 12:44:23,015 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:23,335 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742429_1605{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:23,354 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742425_1601 size 5956918
2019-01-27 12:44:23,356 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742673_1849 size 105634
2019-01-27 12:44:23,359 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742732_1908{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000010_3
2019-01-27 12:44:23,442 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:23,545 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742696_1872 size 1507546
2019-01-27 12:44:23,585 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742733_1909{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000018_0
2019-01-27 12:44:23,699 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742437_1613{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:23,699 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742437_1613{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:23,766 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wal/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:23,930 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742416_1592{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:23,932 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742416_1592 size 10153256
2019-01-27 12:44:24,047 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:24,218 INFO BlockStateChange: BLOCK* ask 172.18.0.3:50010 to replicate blk_1073742720_1896 to datanode(s) 172.18.0.2:50010
2019-01-27 12:44:24,218 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742428_1604 to datanode(s) 172.18.0.3:50010
2019-01-27 12:44:24,218 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742429_1605 to datanode(s) 172.18.0.3:50010
2019-01-27 12:44:24,319 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742418_1594{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:24,319 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742418_1594{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:24,457 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:24,536 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742423_1599{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:24,536 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742423_1599{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:24,699 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:24,943 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742701_1877 size 192498
2019-01-27 12:44:24,948 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742516_1692{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:24,954 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742516_1692{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:25,019 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742734_1910{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000018_0
2019-01-27 12:44:25,020 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742735_1911{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000018_0
2019-01-27 12:44:25,020 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742736_1912{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000018_0
2019-01-27 12:44:25,021 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:25,021 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:25,021 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:25,021 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742737_1913{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000018_0
2019-01-27 12:44:25,050 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742720_1896 size 332
2019-01-27 12:44:25,100 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:25,100 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:25,101 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:25,101 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742738_1914{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000015_0
2019-01-27 12:44:25,126 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:25,126 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:25,126 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:25,126 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742739_1915{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000013_0
2019-01-27 12:44:25,274 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:25,495 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:25,495 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:25,495 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:25,495 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:54220 Call#258 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=ill/_tmp.000002_2 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:25,783 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:25,783 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:25,783 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:25,783 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:54222 Call#113 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000018_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:25,794 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:25,794 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:25,794 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:25,795 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:54222 Call#114 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000018_0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:25,991 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742412_1588{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:25,999 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742412_1588{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:26,123 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742730_1906{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:26,177 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000002_2 is closed by DFSClient_attempt_1548592229700_0001_m_000002_2_-1395590438_1
2019-01-27 12:44:26,195 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742674_1850 size 120774
2019-01-27 12:44:26,303 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000018_0 is closed by DFSClient_attempt_1548592229700_0001_m_000018_0_-532230749_1
2019-01-27 12:44:26,318 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742735_1911{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:26,327 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742735_1911{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:26,501 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000018_0 is closed by DFSClient_attempt_1548592229700_0001_m_000018_0_-532230749_1
2019-01-27 12:44:26,530 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742723_1899{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:26,545 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742427_1603 size 5643639
2019-01-27 12:44:26,575 INFO org.apache.hadoop.ipc.Server: Socket Reader #1 for port 9000: readAndProcess from client 172.18.0.2 threw exception [java.io.IOException: Connection reset by peer]
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:384)
	at org.apache.hadoop.ipc.Server.channelRead(Server.java:2603)
	at org.apache.hadoop.ipc.Server.access$2800(Server.java:136)
	at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1481)
	at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:771)
	at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:637)
	at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
2019-01-27 12:44:26,667 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000018_0 is closed by DFSClient_attempt_1548592229700_0001_m_000018_0_-532230749_1
2019-01-27 12:44:27,013 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742716_1892{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:27,014 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742716_1892{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:27,167 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000018_0 is closed by DFSClient_attempt_1548592229700_0001_m_000018_0_-532230749_1
2019-01-27 12:44:27,190 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742724_1900{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:27,197 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742724_1900{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:27,218 INFO BlockStateChange: BLOCK* ask 172.18.0.3:50010 to replicate blk_1073742730_1906 to datanode(s) 172.18.0.2:50010
2019-01-27 12:44:27,219 INFO BlockStateChange: BLOCK* ask 172.18.0.3:50010 to replicate blk_1073742723_1899 to datanode(s) 172.18.0.2:50010
2019-01-27 12:44:27,378 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000018_0 is closed by DFSClient_attempt_1548592229700_0001_m_000018_0_-532230749_1
2019-01-27 12:44:27,383 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:27,383 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:27,383 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:27,383 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742740_1916{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mob/_tmp.000018_0
2019-01-27 12:44:27,567 INFO org.apache.hadoop.ipc.Server: Socket Reader #1 for port 9000: readAndProcess from client 172.18.0.2 threw exception [java.io.IOException: Connection reset by peer]
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:384)
	at org.apache.hadoop.ipc.Server.channelRead(Server.java:2603)
	at org.apache.hadoop.ipc.Server.access$2800(Server.java:136)
	at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1481)
	at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:771)
	at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:637)
	at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
2019-01-27 12:44:27,650 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:27,650 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:27,650 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:27,650 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742741_1917{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000004_1
2019-01-27 12:44:28,182 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742723_1899 size 183339
2019-01-27 12:44:28,187 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742730_1906 size 119105
2019-01-27 12:44:31,216 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742429_1605 size 4040026
2019-01-27 12:44:32,741 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742428_1604 size 5437298
2019-01-27 12:44:33,990 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742452_1628{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:34,000 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742452_1628{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:34,186 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000012_2 is closed by DFSClient_attempt_1548592229700_0001_m_000012_2_2119739676_1
2019-01-27 12:44:34,317 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742461_1637{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:34,319 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742461_1637{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:34,631 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000012_2 is closed by DFSClient_attempt_1548592229700_0001_m_000012_2_2119739676_1
2019-01-27 12:44:34,852 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742742_1918{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000014_3
2019-01-27 12:44:34,952 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742445_1621{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:34,968 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742445_1621{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:35,513 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000012_2 is closed by DFSClient_attempt_1548592229700_0001_m_000012_2_2119739676_1
2019-01-27 12:44:35,737 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742453_1629{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:35,737 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742453_1629{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:35,917 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000012_2 is closed by DFSClient_attempt_1548592229700_0001_m_000012_2_2119739676_1
2019-01-27 12:44:36,302 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742477_1653{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:36,303 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742477_1653{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:36,500 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000012_2 is closed by DFSClient_attempt_1548592229700_0001_m_000012_2_2119739676_1
2019-01-27 12:44:36,764 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742456_1632{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:36,767 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742456_1632{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:37,072 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000012_2 is closed by DFSClient_attempt_1548592229700_0001_m_000012_2_2119739676_1
2019-01-27 12:44:37,156 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742743_1919{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000014_3
2019-01-27 12:44:37,641 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742744_1920{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000014_3
2019-01-27 12:44:37,657 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742745_1921{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000014_3
2019-01-27 12:44:37,658 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742746_1922{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000014_3
2019-01-27 12:44:37,775 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742747_1923{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000014_3
2019-01-27 12:44:37,779 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742475_1651{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:37,786 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742475_1651 size 3192245
2019-01-27 12:44:37,842 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742748_1924{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000014_3
2019-01-27 12:44:38,110 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000012_2 is closed by DFSClient_attempt_1548592229700_0001_m_000012_2_2119739676_1
2019-01-27 12:44:38,235 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742749_1925{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000014_3
2019-01-27 12:44:38,236 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742750_1926{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000014_3
2019-01-27 12:44:38,237 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742751_1927{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000014_3
2019-01-27 12:44:38,366 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742752_1928{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000014_3
2019-01-27 12:44:38,369 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742753_1929{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000014_3
2019-01-27 12:44:38,369 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742754_1930{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000014_3
2019-01-27 12:44:38,378 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742755_1931{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000014_3
2019-01-27 12:44:38,389 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742756_1932{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000014_3
2019-01-27 12:44:38,394 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742492_1668{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:38,476 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742492_1668{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:38,592 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=idc/_tmp.000012_2 is closed by DFSClient_attempt_1548592229700_0001_m_000012_2_2119739676_1
2019-01-27 12:44:38,772 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742757_1933{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000012_2
2019-01-27 12:44:38,811 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742758_1934{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000014_3
2019-01-27 12:44:39,174 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742759_1935{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000014_3
2019-01-27 12:44:40,625 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742757_1933{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:40,635 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:40,636 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:40,636 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:40,637 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742760_1936{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000014_3
2019-01-27 12:44:40,640 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:40,640 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:40,640 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:40,640 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:40,640 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:40,640 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:40,640 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742761_1937{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000014_3
2019-01-27 12:44:40,640 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742762_1938{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000014_3
2019-01-27 12:44:40,641 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:40,641 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:40,641 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:40,641 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:54440 Call#117 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=hip/_tmp.000014_3 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:40,641 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:40,641 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:40,641 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:40,641 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:54440 Call#116 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000014_3 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:40,642 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:40,642 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:40,642 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:40,642 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:54440 Call#119 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=fre/_tmp.000014_3 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:40,643 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:40,643 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:40,643 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:40,643 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:54440 Call#118 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000014_3 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:40,737 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742757_1933{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:40,798 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:40,798 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:40,798 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:40,798 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:54440 Call#124 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mgm/_tmp.000014_3 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:40,960 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000012_2 is closed by DFSClient_attempt_1548592229700_0001_m_000012_2_2119739676_1
2019-01-27 12:44:41,224 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742476_1652{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:41,226 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742476_1652{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:41,254 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742758_1934{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:41,255 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742758_1934{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:41,392 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000014_3 is closed by DFSClient_attempt_1548592229700_0001_m_000014_3_1631332570_1
2019-01-27 12:44:41,392 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mon/_tmp.000012_2 is closed by DFSClient_attempt_1548592229700_0001_m_000012_2_2119739676_1
2019-01-27 12:44:41,402 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:41,402 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:41,402 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:41,402 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742763_1939{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000008_3
2019-01-27 12:44:41,507 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742750_1926{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:41,508 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742750_1926{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:41,678 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742455_1631{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:41,688 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742455_1631 size 6166653
2019-01-27 12:44:41,826 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000014_3 is closed by DFSClient_attempt_1548592229700_0001_m_000014_3_1631332570_1
2019-01-27 12:44:41,841 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742744_1920{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:41,843 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742744_1920{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:41,978 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=uni/_tmp.000012_2 is closed by DFSClient_attempt_1548592229700_0001_m_000012_2_2119739676_1
2019-01-27 12:44:42,147 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000014_3 is closed by DFSClient_attempt_1548592229700_0001_m_000014_3_1631332570_1
2019-01-27 12:44:42,176 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742753_1929{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:42,180 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742753_1929{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:42,503 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742468_1644{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:42,504 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742468_1644{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:42,572 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000014_3 is closed by DFSClient_attempt_1548592229700_0001_m_000014_3_1631332570_1
2019-01-27 12:44:42,624 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742755_1931{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:42,632 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742755_1931{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:42,744 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=glk/_tmp.000012_2 is closed by DFSClient_attempt_1548592229700_0001_m_000012_2_2119739676_1
2019-01-27 12:44:42,931 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000014_3 is closed by DFSClient_attempt_1548592229700_0001_m_000014_3_1631332570_1
2019-01-27 12:44:43,027 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:43,027 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:43,027 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:43,027 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742764_1940{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000008_3
2019-01-27 12:44:43,144 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:43,144 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:43,144 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:43,145 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742765_1941{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000008_3
2019-01-27 12:44:43,152 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:43,152 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:43,152 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:43,152 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742766_1942{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000008_3
2019-01-27 12:44:43,195 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:43,195 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:43,195 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:43,196 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742767_1943{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000008_3
2019-01-27 12:44:43,352 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:43,352 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:43,352 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:43,353 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742768_1944{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000019_1
2019-01-27 12:44:43,709 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:43,709 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:43,710 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:43,710 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742769_1945{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000008_3
2019-01-27 12:44:43,712 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:43,713 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:43,713 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:43,713 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742770_1946{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000008_3
2019-01-27 12:44:43,724 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:43,724 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:43,724 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:43,725 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742771_1947{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000008_3
2019-01-27 12:44:43,785 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742459_1635{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:43,785 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742459_1635{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:43,813 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742752_1928{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000014_3
2019-01-27 12:44:43,814 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742752_1928{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 194779
2019-01-27 12:44:43,820 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742752_1928 size 194779
2019-01-27 12:44:43,881 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742631_1807{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:43,883 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742631_1807{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:43,893 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:43,893 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:43,894 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:43,894 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742772_1948{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000008_3
2019-01-27 12:44:43,908 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000012_2 is closed by DFSClient_attempt_1548592229700_0001_m_000012_2_2119739676_1
2019-01-27 12:44:44,022 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742617_1793{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:44,026 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742617_1793{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:44,035 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:44,035 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:44,035 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:44,036 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742773_1949{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000008_3
2019-01-27 12:44:44,056 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:44,056 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:44,056 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:44,057 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742774_1950{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000008_3
2019-01-27 12:44:44,057 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:44,057 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:44,057 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:44,057 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742775_1951{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000008_3
2019-01-27 12:44:44,071 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dlr/_tmp.000015_0 is closed by DFSClient_attempt_1548592229700_0001_m_000015_0_-234333406_1
2019-01-27 12:44:44,088 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:44,088 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:44,089 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:44,089 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742776_1952{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=net/_tmp.000019_1
2019-01-27 12:44:44,092 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:44,092 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:44,092 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:44,092 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742777_1953{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000008_3
2019-01-27 12:44:44,094 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:44,094 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:44,094 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:44,095 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:54454 Call#114 Retry#0
java.io.IOException: File /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000008_3 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:421)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2019-01-27 12:44:44,176 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:44,176 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:44,176 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:44,176 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742778_1954{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=out/_tmp.000015_0
2019-01-27 12:44:44,205 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:44,205 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:44,206 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:44,206 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742779_1955{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=gwd/_tmp.000019_1
2019-01-27 12:44:44,211 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000000_2 is closed by DFSClient_attempt_1548592229700_0001_m_000000_2_1920271100_1
2019-01-27 12:44:44,215 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742625_1801{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:44,219 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:44,219 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:44,219 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:44,220 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742780_1956{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=GWD/_tmp.000010_3
2019-01-27 12:44:44,223 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742625_1801 size 3352857
2019-01-27 12:44:44,232 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742640_1816{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:44,239 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742640_1816{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:44,259 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742781_1957{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000008_3
2019-01-27 12:44:44,260 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742442_1618{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:44,260 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742782_1958{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bal/_tmp.000019_1
2019-01-27 12:44:44,329 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742442_1618 size 99318087
2019-01-27 12:44:44,332 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742635_1811{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:44,344 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742635_1811{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:44,348 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742483_1659{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:44,348 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742483_1659{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:44,445 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000000_2 is closed by DFSClient_attempt_1548592229700_0001_m_000000_2_1920271100_1
2019-01-27 12:44:44,448 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000015_0 is closed by DFSClient_attempt_1548592229700_0001_m_000015_0_-234333406_1
2019-01-27 12:44:44,449 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cen/_tmp.000012_2 is closed by DFSClient_attempt_1548592229700_0001_m_000012_2_2119739676_1
2019-01-27 12:44:44,451 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742273_1449{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 31716
2019-01-27 12:44:44,452 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742783_1959{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=ill/_tmp.000000_2
2019-01-27 12:44:44,471 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742784_1960{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000019_1
2019-01-27 12:44:44,519 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742620_1796{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:44,528 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742620_1796{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:44,534 INFO org.apache.hadoop.ipc.Server: Socket Reader #1 for port 9000: readAndProcess from client 172.18.0.3 threw exception [java.io.IOException: Connection reset by peer]
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:384)
	at org.apache.hadoop.ipc.Server.channelRead(Server.java:2603)
	at org.apache.hadoop.ipc.Server.access$2800(Server.java:136)
	at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1481)
	at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:771)
	at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:637)
	at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
2019-01-27 12:44:44,543 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000000_2 is closed by DFSClient_attempt_1548592229700_0001_m_000000_2_1920271100_1
2019-01-27 12:44:44,544 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000012_2 is closed by DFSClient_attempt_1548592229700_0001_m_000012_2_2119739676_1
2019-01-27 12:44:44,547 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742785_1961{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=drp1/_tmp.000012_2
2019-01-27 12:44:44,601 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742786_1962{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=lcy/_tmp.000019_1
2019-01-27 12:44:44,619 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742787_1963{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bro/_tmp.000019_1
2019-01-27 12:44:44,638 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:44,638 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:44,638 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:44,638 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742788_1964{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000019_1
2019-01-27 12:44:44,663 INFO org.apache.hadoop.ipc.Server: Socket Reader #1 for port 9000: readAndProcess from client 172.18.0.2 threw exception [java.io.IOException: Connection reset by peer]
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:384)
	at org.apache.hadoop.ipc.Server.channelRead(Server.java:2603)
	at org.apache.hadoop.ipc.Server.access$2800(Server.java:136)
	at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1481)
	at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:771)
	at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:637)
	at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
2019-01-27 12:44:44,683 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nhy/_tmp.000004_1 is closed by DFSClient_attempt_1548592229700_0001_m_000004_1_976787802_1
2019-01-27 12:44:44,684 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1548592229700_0001/job_1548592229700_0001_1.jhist is closed by DFSClient_NONMAPREDUCE_-254055627_1
2019-01-27 12:44:44,684 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742781_1957{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:44,684 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742781_1957{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:44,685 WARN org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.2:54572 Call#161 Retry#0: output error
2019-01-27 12:44:44,685 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000 caught an exception
java.nio.channels.ClosedChannelException
	at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:270)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:479)
	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:2583)
	at org.apache.hadoop.ipc.Server.access$1900(Server.java:136)
	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:986)
	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1051)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2104)
2019-01-27 12:44:44,688 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742789_1965{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=tcs/_tmp.000008_3
2019-01-27 12:44:44,729 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:44,729 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:44,729 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:44,730 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742790_1966{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=dth/_tmp.000019_1
2019-01-27 12:44:44,744 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742535_1711{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:44,744 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:44,744 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:44,745 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:44,745 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742791_1967{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=col/_tmp.000019_1
2019-01-27 12:44:44,820 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742535_1711{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:44,821 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742549_1725{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:44,822 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742549_1725{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:44,822 INFO org.apache.hadoop.ipc.Server: Socket Reader #1 for port 9000: readAndProcess from client 172.18.0.2 threw exception [java.io.IOException: Connection reset by peer]
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:384)
	at org.apache.hadoop.ipc.Server.channelRead(Server.java:2603)
	at org.apache.hadoop.ipc.Server.access$2800(Server.java:136)
	at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1481)
	at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:771)
	at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:637)
	at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
2019-01-27 12:44:44,823 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742792_1968{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=cap/_tmp.000019_1
2019-01-27 12:44:44,825 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742542_1718{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:44,916 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000004_1 is closed by DFSClient_attempt_1548592229700_0001_m_000004_1_976787802_1
2019-01-27 12:44:44,917 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742542_1718 size 5496120
2019-01-27 12:44:44,917 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742793_1969{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=mag/_tmp.000019_1
2019-01-27 12:44:44,925 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=qna/_tmp.000008_3 is closed by DFSClient_attempt_1548592229700_0001_m_000008_3_-1339507634_1
2019-01-27 12:44:44,926 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742610_1786{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:44,926 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742794_1970{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=bea/_tmp.000019_1
2019-01-27 12:44:44,927 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742795_1971{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=rbe/_tmp.000019_1
2019-01-27 12:44:44,927 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742610_1786 size 2753520
2019-01-27 12:44:44,928 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:44,928 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:44,928 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:44,928 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742796_1972{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000019_1
2019-01-27 12:44:44,939 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742773_1949{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} size 0
2019-01-27 12:44:44,945 INFO org.apache.hadoop.ipc.Server: Socket Reader #1 for port 9000: readAndProcess from client 172.18.0.3 threw exception [java.io.IOException: Connection reset by peer]
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:384)
	at org.apache.hadoop.ipc.Server.channelRead(Server.java:2603)
	at org.apache.hadoop.ipc.Server.access$2800(Server.java:136)
	at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1481)
	at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:771)
	at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:637)
	at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
2019-01-27 12:44:44,956 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742797_1973{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1548592229700_0001.summary_tmp
2019-01-27 12:44:44,988 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:44,989 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:44,989 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:44,989 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742798_1974{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000021_1
2019-01-27 12:44:44,989 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=nga/_tmp.000004_1 is closed by DFSClient_attempt_1548592229700_0001_m_000004_1_976787802_1
2019-01-27 12:44:44,989 WARN org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.complete from 172.18.0.3:50750 Call#159 Retry#0: output error
2019-01-27 12:44:44,989 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000 caught an exception
java.nio.channels.ClosedChannelException
	at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:270)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:479)
	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:2583)
	at org.apache.hadoop.ipc.Server.access$1900(Server.java:136)
	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:986)
	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1051)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2104)
2019-01-27 12:44:44,989 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000013_0 is closed by DFSClient_attempt_1548592229700_0001_m_000013_0_-1145020382_1
2019-01-27 12:44:45,008 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000010_3 is closed by DFSClient_attempt_1548592229700_0001_m_000010_3_822406631_1
2019-01-27 12:44:45,014 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:45,014 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:45,014 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:45,014 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742799_1975{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=drp1/_tmp.000010_3
2019-01-27 12:44:45,054 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742799_1975{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:45,066 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=drp1/_tmp.000010_3 is closed by DFSClient_attempt_1548592229700_0001_m_000010_3_822406631_1
2019-01-27 12:44:45,068 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:45,068 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:45,069 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:45,069 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742800_1976{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=ill/_tmp.000010_3
2019-01-27 12:44:45,085 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742796_1972{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:45,101 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=wts/_tmp.000019_1 is closed by DFSClient_attempt_1548592229700_0001_m_000019_1_-2135786924_1
2019-01-27 12:44:45,104 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742800_1976{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:45,108 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:45,108 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:45,109 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:45,109 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742801_1977{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000019_1
2019-01-27 12:44:45,133 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=ill/_tmp.000010_3 is closed by DFSClient_attempt_1548592229700_0001_m_000010_3_822406631_1
2019-01-27 12:44:45,154 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742801_1977{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:45,168 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=spa/_tmp.000019_1 is closed by DFSClient_attempt_1548592229700_0001_m_000019_1_-2135786924_1
2019-01-27 12:44:45,184 INFO org.apache.hadoop.ipc.Server: Socket Reader #1 for port 9000: readAndProcess from client 172.18.0.2 threw exception [java.io.IOException: Connection reset by peer]
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:384)
	at org.apache.hadoop.ipc.Server.channelRead(Server.java:2603)
	at org.apache.hadoop.ipc.Server.access$2800(Server.java:136)
	at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1481)
	at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:771)
	at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:637)
	at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
2019-01-27 12:44:45,217 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742798_1974{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:45,222 INFO BlockStateChange: BLOCK* ask 172.18.0.3:50010 to replicate blk_1073742273_1449 to datanode(s) 172.18.0.2:50010
2019-01-27 12:44:45,222 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742799_1975 to datanode(s) 172.18.0.3:50010
2019-01-27 12:44:45,222 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742796_1972 to datanode(s) 172.18.0.3:50010
2019-01-27 12:44:45,222 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742800_1976 to datanode(s) 172.18.0.3:50010
2019-01-27 12:44:45,232 INFO org.apache.hadoop.ipc.Server: Socket Reader #1 for port 9000: readAndProcess from client 172.18.0.3 threw exception [java.io.IOException: Connection reset by peer]
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:384)
	at org.apache.hadoop.ipc.Server.channelRead(Server.java:2603)
	at org.apache.hadoop.ipc.Server.access$2800(Server.java:136)
	at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1481)
	at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:771)
	at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:637)
	at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
2019-01-27 12:44:45,235 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/library.db/bookspartitioned/.hive-staging_hive_2019-01-27_12-41-09_601_8214468303066055363-1/_task_tmp.-ext-10002/itemlocation=swt/_tmp.000021_1 is closed by DFSClient_attempt_1548592229700_0001_m_000021_1_-2032089212_1
2019-01-27 12:44:45,292 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742797_1973{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:45,295 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742797_1973{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW], ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:45,301 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1548592229700_0001.summary_tmp is closed by DFSClient_NONMAPREDUCE_-254055627_1
2019-01-27 12:44:45,340 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:45,340 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:45,340 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:45,340 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742802_1978{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1548592229700_0001-1548592876055-root-INSERT+OVERWRITE+TABLE+Libra...Library.Books%28Stage-1548593083337-7-0-FAILED-default-1548592887964.jhist_tmp
2019-01-27 12:44:45,352 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742802_1978{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:45,356 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1548592229700_0001-1548592876055-root-INSERT+OVERWRITE+TABLE+Libra...Library.Books%28Stage-1548593083337-7-0-FAILED-default-1548592887964.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-254055627_1
2019-01-27 12:44:45,375 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2019-01-27 12:44:45,375 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-01-27 12:44:45,375 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-01-27 12:44:45,376 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742803_1979{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1548592229700_0001_conf.xml_tmp
2019-01-27 12:44:45,383 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742803_1979{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW]]} size 0
2019-01-27 12:44:45,387 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1548592229700_0001_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-254055627_1
2019-01-27 12:44:45,845 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.2:50010 is added to blk_1073742273_1449 size 410140
2019-01-27 12:44:46,438 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741922_1098 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:46,438 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741923_1099 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:46,438 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741924_1100 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:46,438 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741925_1101 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:46,438 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742273_1449 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:46,438 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741926_1102 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,121 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742799_1975 size 517
2019-01-27 12:44:47,122 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742796_1972 size 75559
2019-01-27 12:44:47,525 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741921_1097 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,551 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742441_1617 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,551 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742698_1874 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,551 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742443_1619 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,551 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742377_1553 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,551 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742440_1616 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,551 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742446_1622 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,551 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742410_1586 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,551 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742465_1641 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,551 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742439_1615 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,551 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742757_1933 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,551 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741939_1115 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,551 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742413_1589 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,551 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741947_1123 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,551 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742099_1275 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,552 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741944_1120 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,552 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742288_1464 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,552 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742002_1178 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,552 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741953_1129 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,552 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741933_1109 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,552 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742445_1621 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,552 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741978_1154 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,552 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742420_1596 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,552 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741991_1167 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,552 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741962_1138 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,552 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742296_1472 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,552 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742038_1214 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,552 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741988_1164 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,552 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741972_1148 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,552 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741979_1155 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,552 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742417_1593 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,552 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741970_1146 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,552 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742104_1280 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,552 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741955_1131 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,553 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742292_1468 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,553 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742022_1198 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,553 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741958_1134 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,553 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741943_1119 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,553 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741999_1175 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,553 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742419_1595 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,553 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741980_1156 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,553 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742106_1282 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,553 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741961_1137 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,553 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742305_1481 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,553 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742039_1215 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,553 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741986_1162 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,553 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741956_1132 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,553 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742459_1635 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,553 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741930_1106 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,553 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742411_1587 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,553 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741929_1105 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,553 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742091_1267 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,553 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741928_1104 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,554 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742284_1460 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,554 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741932_1108 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,554 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741931_1107 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,554 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741927_1103 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,554 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742442_1618 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,554 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741998_1174 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,554 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742422_1598 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,554 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741982_1158 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,554 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742109_1285 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,554 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741973_1149 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,554 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742295_1471 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,554 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742052_1228 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,554 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741994_1170 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,554 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741985_1161 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,554 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742456_1632 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,554 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742055_1231 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,554 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742433_1609 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,554 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742068_1244 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,554 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742141_1317 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,554 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742027_1203 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,555 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742308_1484 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,555 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742084_1260 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,555 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742058_1234 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,555 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742032_1208 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,555 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742475_1651 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,555 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742467_1643 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,555 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742721_1897 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,555 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742496_1672 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,555 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742463_1639 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,555 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742494_1670 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,555 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742502_1678 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,555 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742509_1685 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,555 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742469_1645 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,555 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741967_1143 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,555 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742415_1591 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,555 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741969_1145 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,555 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741954_1130 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,555 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742290_1466 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,555 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742021_1197 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,555 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741974_1150 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,556 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741948_1124 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,556 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742060_1236 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,556 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742432_1608 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,556 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742053_1229 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,556 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742148_1324 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,556 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742020_1196 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,556 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742304_1480 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,556 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742081_1257 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,556 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742062_1238 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,556 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742028_1204 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,556 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742477_1653 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,556 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742015_1191 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,556 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742427_1603 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,556 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742036_1212 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,556 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742122_1298 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,556 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741990_1166 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,556 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742302_1478 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,556 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742071_1247 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,556 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742014_1190 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,556 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741992_1168 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,557 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742468_1644 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,557 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741975_1151 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,557 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742414_1590 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,557 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741964_1140 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,557 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742102_1278 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,557 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741946_1122 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,557 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742287_1463 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,557 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742003_1179 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,557 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741950_1126 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,557 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741938_1114 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,557 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742043_1219 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,557 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742429_1605 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,557 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742037_1213 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,557 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742013_1189 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,557 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742300_1476 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,557 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742078_1254 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,557 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742047_1223 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,557 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742019_1195 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,557 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742064_1240 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,557 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742435_1611 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,557 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742077_1253 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,558 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742170_1346 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,558 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742050_1226 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,558 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742311_1487 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,558 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742087_1263 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,558 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742074_1250 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,558 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742045_1221 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,558 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742492_1668 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,558 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742507_1683 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,558 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742515_1691 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,558 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742517_1693 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,558 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741968_1144 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,558 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742416_1592 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,558 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741952_1128 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,558 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741945_1121 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,558 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742291_1467 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,558 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742008_1184 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,558 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741957_1133 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,558 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741940_1116 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,558 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742012_1188 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,559 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742428_1604 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,559 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742016_1192 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,559 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741997_1173 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,559 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742301_1477 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,559 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742066_1242 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,559 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742031_1207 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,559 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742000_1176 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,559 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742067_1243 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,559 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742436_1612 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,559 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742080_1256 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,559 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742172_1348 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,559 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742054_1230 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,559 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742310_1486 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,559 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742088_1264 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,559 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742069_1245 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,559 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742042_1218 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,559 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742086_1262 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,559 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742438_1614 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,559 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742094_1270 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,559 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742229_1405 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,560 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742092_1268 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,560 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742315_1491 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,560 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742097_1273 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,560 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742093_1269 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,560 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742079_1255 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,560 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742044_1220 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,560 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742430_1606 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,560 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742057_1233 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,560 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742145_1321 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,560 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742018_1194 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,560 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742306_1482 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,560 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742083_1259 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,560 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742040_1216 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,560 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742023_1199 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,560 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742476_1652 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,560 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741941_1117 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,560 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741936_1112 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,560 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741935_1111 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,560 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742286_1462 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,560 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742001_1177 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,561 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741942_1118 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,561 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741934_1110 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,561 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741983_1159 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,561 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742424_1600 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,561 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742005_1181 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,561 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742117_1293 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,561 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741966_1142 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,561 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742293_1469 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,561 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742041_1217 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,561 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741976_1152 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,561 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741960_1136 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,561 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742452_1628 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,561 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742065_1241 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,561 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742434_1610 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,561 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742073_1249 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,561 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742035_1211 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,561 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742309_1485 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,561 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742089_1265 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,561 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742072_1248 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,561 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742046_1222 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,561 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742720_1896 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,562 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742487_1663 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,562 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742505_1681 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,562 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742462_1638 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,562 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742025_1201 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,562 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742426_1602 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,562 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742030_1206 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,562 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742120_1296 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,562 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742011_1187 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,562 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742307_1483 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,562 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742075_1251 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,562 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742026_1202 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,562 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741993_1169 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,562 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742461_1637 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,562 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741989_1165 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,562 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742421_1597 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,562 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742006_1182 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,562 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742116_1292 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,562 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741981_1157 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,562 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742298_1474 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,563 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742049_1225 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,563 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742007_1183 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,563 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741977_1153 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,563 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742453_1629 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,563 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742056_1232 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,563 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742431_1607 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,563 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742059_1235 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,563 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742163_1339 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,563 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742034_1210 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,563 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742299_1475 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,563 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742085_1261 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,563 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742048_1224 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,563 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742024_1200 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,563 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741963_1139 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,563 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742418_1594 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,563 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741959_1135 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,563 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741951_1127 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,563 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742289_1465 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,563 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742017_1193 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,563 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741949_1125 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,564 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741937_1113 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,564 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742285_1461 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,564 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742516_1692 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,564 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742297_1473 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,564 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742278_1454 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,564 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742376_1552 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,564 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742280_1456 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,564 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742283_1459 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,564 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742279_1455 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,564 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742004_1180 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,564 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742425_1601 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,564 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742033_1209 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,564 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742112_1288 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,564 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741987_1163 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,564 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742303_1479 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,564 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742061_1237 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,564 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742010_1186 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,564 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741995_1171 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,564 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742455_1631 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,564 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742070_1246 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,564 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742437_1613 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,564 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742082_1258 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,564 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742063_1239 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,565 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742312_1488 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,565 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742090_1266 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,565 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742076_1252 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,565 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742051_1227 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,565 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741984_1160 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,565 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742423_1599 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,565 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742009_1185 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,565 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741971_1147 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,565 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742294_1470 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,565 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742029_1205 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,565 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741996_1172 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,565 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741965_1141 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,578 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742277_1453 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,579 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742275_1451 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,579 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742181_1357 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,579 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742358_1534 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,579 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742197_1373 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,579 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742123_1299 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,579 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742381_1557 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,579 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742666_1842 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,579 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742115_1291 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,579 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742322_1498 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,579 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742121_1297 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,579 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742470_1646 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,579 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742744_1920 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,580 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742662_1838 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,580 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742723_1899 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,580 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742535_1711 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,580 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742157_1333 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,580 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742687_1863 172.18.0.2:50010 
2019-01-27 12:44:47,580 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742402_1578 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,580 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742333_1509 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,580 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742135_1311 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,580 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742324_1500 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,580 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742146_1322 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,580 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742366_1542 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,580 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742139_1315 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,581 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742154_1330 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,581 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742482_1658{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,581 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742098_1274 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,581 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742557_1733 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,581 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742096_1272 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,581 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742696_1872 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,581 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742716_1892 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,581 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742220_1396 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,581 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742368_1544 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,581 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742231_1407 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,581 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742161_1337 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,582 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742399_1575 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,582 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742690_1866 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,582 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742405_1581 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,582 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742151_1327 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,582 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742335_1511 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,582 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742162_1338 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,582 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742488_1664 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,582 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742755_1931 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,582 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742673_1849 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,582 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742246_1422 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,582 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742176_1352 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,582 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742342_1518 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,582 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742200_1376 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,582 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742501_1677 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,582 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742631_1807 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,582 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742799_1975 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,582 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742327_1503 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,583 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742128_1304{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,583 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742245_1421 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,583 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742178_1354 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,583 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742344_1520 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,583 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742195_1371 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,583 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742375_1551 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,583 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742499_1675 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,583 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742190_1366 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,583 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742406_1582 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,583 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742257_1433 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,583 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742194_1370 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,583 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742715_1891 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,584 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742127_1303 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,584 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742383_1559 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,584 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742126_1302 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,584 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742663_1839 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,584 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742243_1419 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,584 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742252_1428 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,584 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742274_1450 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,584 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742184_1360 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,584 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742348_1524 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,584 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742206_1382 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,584 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742506_1682 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,584 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742800_1976 172.18.0.2:50010 
2019-01-27 12:44:47,584 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742208_1384{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,584 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742101_1277 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,585 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742392_1568 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,585 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742325_1501 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,585 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742640_1816 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,585 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742169_1345 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,585 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742254_1430 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,585 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742347_1523 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,585 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742205_1381 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,585 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742244_1420 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,585 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742196_1372 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,585 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742412_1588 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,586 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742320_1496 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,586 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742458_1634 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,586 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742701_1877 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,586 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742217_1393 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,586 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742365_1541 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,586 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742617_1793 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,586 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742234_1410 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,586 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742542_1718 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,586 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742156_1332 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,586 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742396_1572 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,586 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742692_1868 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,586 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742773_1949{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]} 172.18.0.3:50010 
2019-01-27 12:44:47,586 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742248_1424 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,586 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742404_1580 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,587 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742142_1318 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,587 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742329_1505 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,587 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742149_1325 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,587 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742480_1656 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,587 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742750_1926 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,587 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742349_1525 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,587 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742668_1844 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,587 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742730_1906 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,587 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742620_1796 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,587 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742483_1659 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,587 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742204_1380 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,587 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742241_1417 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,587 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742369_1545 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,587 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742635_1811 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,587 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742236_1412 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,587 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742180_1356 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,587 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742408_1584 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,587 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742694_1870 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,587 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742781_1957 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,588 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742155_1331 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,588 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742337_1513 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,588 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742192_1368 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,588 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742491_1667 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,588 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742758_1934 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,588 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742625_1801 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,588 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742683_1859 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,588 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742735_1911 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,588 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742219_1395 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,588 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742367_1543 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,588 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742225_1401 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,588 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742160_1336 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,588 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742398_1574 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,588 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742689_1865 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,588 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742144_1320 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,588 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742331_1507 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,588 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742158_1334 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,588 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742484_1660 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,589 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742753_1929 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,589 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742677_1853 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,589 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742251_1427 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,589 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742610_1786 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,589 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742199_1375 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,589 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742801_1977 172.18.0.2:50010 
2019-01-27 12:44:47,589 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742125_1301 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,589 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742380_1556 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,589 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742326_1502 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,589 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742549_1725 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,589 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742724_1900 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,589 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742798_1974 172.18.0.2:50010 
2019-01-27 12:44:47,589 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742282_1458 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,589 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742714_1890 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,589 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742373_1549 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,590 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742168_1344 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,590 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742401_1577 172.18.0.2:50010 
2019-01-27 12:44:47,590 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742165_1341 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,590 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742173_1349 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,590 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742207_1383 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,590 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742752_1928 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,590 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742674_1850 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,590 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742796_1972 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,626 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742520_1696 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,626 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742510_1686 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,626 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742513_1689 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,626 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742512_1688 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,626 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742518_1694 172.18.0.2:50010 172.18.0.3:50010 
2019-01-27 12:44:47,626 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742521_1697 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:47,626 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742519_1695 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:44:48,223 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742802_1978 to datanode(s) 172.18.0.3:50010
2019-01-27 12:44:48,223 INFO BlockStateChange: BLOCK* ask 172.18.0.2:50010 to replicate blk_1073742803_1979 to datanode(s) 172.18.0.3:50010
2019-01-27 12:44:48,224 INFO BlockStateChange: BLOCK* BlockManager: ask 172.18.0.3:50010 to delete [blk_1073741921_1097, blk_1073741922_1098, blk_1073741923_1099, blk_1073741924_1100, blk_1073741925_1101, blk_1073741926_1102, blk_1073741927_1103, blk_1073741928_1104, blk_1073741929_1105, blk_1073741930_1106, blk_1073741931_1107, blk_1073741932_1108, blk_1073741933_1109, blk_1073741934_1110, blk_1073741935_1111, blk_1073741936_1112, blk_1073741937_1113, blk_1073741938_1114, blk_1073741939_1115, blk_1073741940_1116, blk_1073741941_1117, blk_1073741942_1118, blk_1073741943_1119, blk_1073741944_1120, blk_1073741945_1121, blk_1073741946_1122, blk_1073741947_1123, blk_1073741948_1124, blk_1073741949_1125, blk_1073741950_1126, blk_1073741951_1127, blk_1073741952_1128, blk_1073741953_1129, blk_1073741954_1130, blk_1073741955_1131, blk_1073741956_1132, blk_1073741957_1133, blk_1073741958_1134, blk_1073741959_1135, blk_1073741960_1136, blk_1073741961_1137, blk_1073741962_1138, blk_1073741963_1139, blk_1073741964_1140, blk_1073741965_1141, blk_1073741966_1142, blk_1073741967_1143, blk_1073741968_1144, blk_1073741969_1145, blk_1073741970_1146, blk_1073741971_1147, blk_1073741972_1148, blk_1073741973_1149, blk_1073741974_1150, blk_1073741975_1151, blk_1073741976_1152, blk_1073741977_1153, blk_1073741978_1154, blk_1073741979_1155, blk_1073741980_1156, blk_1073741981_1157, blk_1073741982_1158, blk_1073741983_1159, blk_1073741984_1160, blk_1073741985_1161, blk_1073741986_1162, blk_1073741987_1163, blk_1073741988_1164, blk_1073741989_1165, blk_1073741990_1166, blk_1073741991_1167, blk_1073741992_1168, blk_1073741993_1169, blk_1073741994_1170, blk_1073741995_1171, blk_1073741996_1172, blk_1073741997_1173, blk_1073741998_1174, blk_1073741999_1175, blk_1073742000_1176, blk_1073742001_1177, blk_1073742002_1178, blk_1073742003_1179, blk_1073742004_1180, blk_1073742005_1181, blk_1073742006_1182, blk_1073742007_1183, blk_1073742008_1184, blk_1073742009_1185, blk_1073742010_1186, blk_1073742011_1187, blk_1073742012_1188, blk_1073742013_1189, blk_1073742014_1190, blk_1073742015_1191, blk_1073742016_1192, blk_1073742017_1193, blk_1073742018_1194, blk_1073742019_1195, blk_1073742020_1196, blk_1073742021_1197, blk_1073742022_1198, blk_1073742023_1199, blk_1073742024_1200, blk_1073742025_1201, blk_1073742026_1202, blk_1073742027_1203, blk_1073742028_1204, blk_1073742029_1205, blk_1073742030_1206, blk_1073742031_1207, blk_1073742032_1208, blk_1073742033_1209, blk_1073742034_1210, blk_1073742035_1211, blk_1073742036_1212, blk_1073742037_1213, blk_1073742038_1214, blk_1073742039_1215, blk_1073742040_1216, blk_1073742041_1217, blk_1073742042_1218, blk_1073742043_1219, blk_1073742044_1220, blk_1073742045_1221, blk_1073742046_1222, blk_1073742047_1223, blk_1073742048_1224, blk_1073742049_1225, blk_1073742050_1226, blk_1073742051_1227, blk_1073742052_1228, blk_1073742053_1229, blk_1073742054_1230, blk_1073742055_1231, blk_1073742056_1232, blk_1073742057_1233, blk_1073742058_1234, blk_1073742059_1235, blk_1073742060_1236, blk_1073742061_1237, blk_1073742062_1238, blk_1073742063_1239, blk_1073742064_1240, blk_1073742065_1241, blk_1073742066_1242, blk_1073742067_1243, blk_1073742068_1244, blk_1073742069_1245, blk_1073742070_1246, blk_1073742071_1247, blk_1073742072_1248, blk_1073742073_1249, blk_1073742074_1250, blk_1073742075_1251, blk_1073742076_1252, blk_1073742077_1253, blk_1073742078_1254, blk_1073742079_1255, blk_1073742080_1256, blk_1073742081_1257, blk_1073742082_1258, blk_1073742083_1259, blk_1073742084_1260, blk_1073742085_1261, blk_1073742086_1262, blk_1073742087_1263, blk_1073742088_1264, blk_1073742089_1265, blk_1073742090_1266, blk_1073742091_1267, blk_1073742092_1268, blk_1073742093_1269, blk_1073742094_1270, blk_1073742096_1272, blk_1073742097_1273, blk_1073742098_1274, blk_1073742099_1275, blk_1073742101_1277, blk_1073742102_1278, blk_1073742104_1280, blk_1073742106_1282, blk_1073742109_1285, blk_1073742112_1288, blk_1073742115_1291, blk_1073742116_1292, blk_1073742117_1293, blk_1073742120_1296, blk_1073742121_1297, blk_1073742122_1298, blk_1073742123_1299, blk_1073742125_1301, blk_1073742126_1302, blk_1073742127_1303, blk_1073742128_1304{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]}, blk_1073742135_1311, blk_1073742139_1315, blk_1073742141_1317, blk_1073742142_1318, blk_1073742144_1320, blk_1073742145_1321, blk_1073742146_1322, blk_1073742148_1324, blk_1073742149_1325, blk_1073742151_1327, blk_1073742154_1330, blk_1073742155_1331, blk_1073742156_1332, blk_1073742157_1333, blk_1073742158_1334, blk_1073742160_1336, blk_1073742161_1337, blk_1073742162_1338, blk_1073742163_1339, blk_1073742165_1341, blk_1073742168_1344, blk_1073742169_1345, blk_1073742170_1346, blk_1073742172_1348, blk_1073742173_1349, blk_1073742176_1352, blk_1073742178_1354, blk_1073742180_1356, blk_1073742181_1357, blk_1073742184_1360, blk_1073742190_1366, blk_1073742192_1368, blk_1073742194_1370, blk_1073742195_1371, blk_1073742196_1372, blk_1073742197_1373, blk_1073742199_1375, blk_1073742200_1376, blk_1073742204_1380, blk_1073742205_1381, blk_1073742206_1382, blk_1073742207_1383, blk_1073742208_1384{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]}, blk_1073742217_1393, blk_1073742219_1395, blk_1073742220_1396, blk_1073742225_1401, blk_1073742229_1405, blk_1073742231_1407, blk_1073742234_1410, blk_1073742236_1412, blk_1073742241_1417, blk_1073742243_1419, blk_1073742244_1420, blk_1073742245_1421, blk_1073742246_1422, blk_1073742248_1424, blk_1073742251_1427, blk_1073742252_1428, blk_1073742254_1430, blk_1073742257_1433, blk_1073742273_1449, blk_1073742274_1450, blk_1073742275_1451, blk_1073742277_1453, blk_1073742278_1454, blk_1073742279_1455, blk_1073742280_1456, blk_1073742282_1458, blk_1073742283_1459, blk_1073742284_1460, blk_1073742285_1461, blk_1073742286_1462, blk_1073742287_1463, blk_1073742288_1464, blk_1073742289_1465, blk_1073742290_1466, blk_1073742291_1467, blk_1073742292_1468, blk_1073742293_1469, blk_1073742294_1470, blk_1073742295_1471, blk_1073742296_1472, blk_1073742297_1473, blk_1073742298_1474, blk_1073742299_1475, blk_1073742300_1476, blk_1073742301_1477, blk_1073742302_1478, blk_1073742303_1479, blk_1073742304_1480, blk_1073742305_1481, blk_1073742306_1482, blk_1073742307_1483, blk_1073742308_1484, blk_1073742309_1485, blk_1073742310_1486, blk_1073742311_1487, blk_1073742312_1488, blk_1073742315_1491, blk_1073742320_1496, blk_1073742322_1498, blk_1073742324_1500, blk_1073742325_1501, blk_1073742326_1502, blk_1073742327_1503, blk_1073742329_1505, blk_1073742331_1507, blk_1073742333_1509, blk_1073742335_1511, blk_1073742337_1513, blk_1073742342_1518, blk_1073742344_1520, blk_1073742347_1523, blk_1073742348_1524, blk_1073742349_1525, blk_1073742358_1534, blk_1073742365_1541, blk_1073742366_1542, blk_1073742367_1543, blk_1073742368_1544, blk_1073742369_1545, blk_1073742373_1549, blk_1073742375_1551, blk_1073742376_1552, blk_1073742377_1553, blk_1073742380_1556, blk_1073742381_1557, blk_1073742383_1559, blk_1073742392_1568, blk_1073742396_1572, blk_1073742398_1574, blk_1073742399_1575, blk_1073742402_1578, blk_1073742404_1580, blk_1073742405_1581, blk_1073742406_1582, blk_1073742408_1584, blk_1073742410_1586, blk_1073742411_1587, blk_1073742412_1588, blk_1073742413_1589, blk_1073742414_1590, blk_1073742415_1591, blk_1073742416_1592, blk_1073742417_1593, blk_1073742418_1594, blk_1073742419_1595, blk_1073742420_1596, blk_1073742421_1597, blk_1073742422_1598, blk_1073742423_1599, blk_1073742424_1600, blk_1073742425_1601, blk_1073742426_1602, blk_1073742427_1603, blk_1073742428_1604, blk_1073742429_1605, blk_1073742430_1606, blk_1073742431_1607, blk_1073742432_1608, blk_1073742433_1609, blk_1073742434_1610, blk_1073742435_1611, blk_1073742436_1612, blk_1073742437_1613, blk_1073742438_1614, blk_1073742439_1615, blk_1073742440_1616, blk_1073742441_1617, blk_1073742442_1618, blk_1073742443_1619, blk_1073742445_1621, blk_1073742446_1622, blk_1073742452_1628, blk_1073742453_1629, blk_1073742455_1631, blk_1073742456_1632, blk_1073742458_1634, blk_1073742459_1635, blk_1073742461_1637, blk_1073742462_1638, blk_1073742463_1639, blk_1073742465_1641, blk_1073742467_1643, blk_1073742468_1644, blk_1073742469_1645, blk_1073742470_1646, blk_1073742475_1651, blk_1073742476_1652, blk_1073742477_1653, blk_1073742480_1656, blk_1073742482_1658{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]}, blk_1073742483_1659, blk_1073742484_1660, blk_1073742487_1663, blk_1073742488_1664, blk_1073742491_1667, blk_1073742492_1668, blk_1073742494_1670, blk_1073742496_1672, blk_1073742499_1675, blk_1073742501_1677, blk_1073742502_1678, blk_1073742505_1681, blk_1073742506_1682, blk_1073742507_1683, blk_1073742509_1685, blk_1073742510_1686, blk_1073742512_1688, blk_1073742513_1689, blk_1073742515_1691, blk_1073742516_1692, blk_1073742517_1693, blk_1073742518_1694, blk_1073742519_1695, blk_1073742520_1696, blk_1073742521_1697, blk_1073742535_1711, blk_1073742542_1718, blk_1073742549_1725, blk_1073742557_1733, blk_1073742610_1786, blk_1073742617_1793, blk_1073742620_1796, blk_1073742625_1801, blk_1073742631_1807, blk_1073742635_1811, blk_1073742640_1816, blk_1073742662_1838, blk_1073742663_1839, blk_1073742666_1842, blk_1073742668_1844, blk_1073742673_1849, blk_1073742674_1850, blk_1073742677_1853, blk_1073742683_1859, blk_1073742689_1865, blk_1073742690_1866, blk_1073742692_1868, blk_1073742694_1870, blk_1073742696_1872, blk_1073742698_1874, blk_1073742701_1877, blk_1073742714_1890, blk_1073742715_1891, blk_1073742716_1892, blk_1073742720_1896, blk_1073742721_1897, blk_1073742723_1899, blk_1073742724_1900, blk_1073742730_1906, blk_1073742735_1911, blk_1073742744_1920, blk_1073742750_1926, blk_1073742752_1928, blk_1073742753_1929, blk_1073742755_1931, blk_1073742757_1933, blk_1073742758_1934, blk_1073742773_1949{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]}, blk_1073742781_1957, blk_1073742796_1972, blk_1073742799_1975]
2019-01-27 12:44:50,123 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742802_1978 size 410140
2019-01-27 12:44:51,226 INFO BlockStateChange: BLOCK* BlockManager: ask 172.18.0.2:50010 to delete [blk_1073741921_1097, blk_1073741922_1098, blk_1073741923_1099, blk_1073741924_1100, blk_1073741925_1101, blk_1073741926_1102, blk_1073741927_1103, blk_1073741928_1104, blk_1073741929_1105, blk_1073741930_1106, blk_1073741931_1107, blk_1073741932_1108, blk_1073741933_1109, blk_1073741934_1110, blk_1073741935_1111, blk_1073741936_1112, blk_1073741937_1113, blk_1073741938_1114, blk_1073741939_1115, blk_1073741940_1116, blk_1073741941_1117, blk_1073741942_1118, blk_1073741943_1119, blk_1073741944_1120, blk_1073741945_1121, blk_1073741946_1122, blk_1073741947_1123, blk_1073741948_1124, blk_1073741949_1125, blk_1073741950_1126, blk_1073741951_1127, blk_1073741952_1128, blk_1073741953_1129, blk_1073741954_1130, blk_1073741955_1131, blk_1073741956_1132, blk_1073741957_1133, blk_1073741958_1134, blk_1073741959_1135, blk_1073741960_1136, blk_1073741961_1137, blk_1073741962_1138, blk_1073741963_1139, blk_1073741964_1140, blk_1073741965_1141, blk_1073741966_1142, blk_1073741967_1143, blk_1073741968_1144, blk_1073741969_1145, blk_1073741970_1146, blk_1073741971_1147, blk_1073741972_1148, blk_1073741973_1149, blk_1073741974_1150, blk_1073741975_1151, blk_1073741976_1152, blk_1073741977_1153, blk_1073741978_1154, blk_1073741979_1155, blk_1073741980_1156, blk_1073741981_1157, blk_1073741982_1158, blk_1073741983_1159, blk_1073741984_1160, blk_1073741985_1161, blk_1073741986_1162, blk_1073741987_1163, blk_1073741988_1164, blk_1073741989_1165, blk_1073741990_1166, blk_1073741991_1167, blk_1073741992_1168, blk_1073741993_1169, blk_1073741994_1170, blk_1073741995_1171, blk_1073741996_1172, blk_1073741997_1173, blk_1073741998_1174, blk_1073741999_1175, blk_1073742000_1176, blk_1073742001_1177, blk_1073742002_1178, blk_1073742003_1179, blk_1073742004_1180, blk_1073742005_1181, blk_1073742006_1182, blk_1073742007_1183, blk_1073742008_1184, blk_1073742009_1185, blk_1073742010_1186, blk_1073742011_1187, blk_1073742012_1188, blk_1073742013_1189, blk_1073742014_1190, blk_1073742015_1191, blk_1073742016_1192, blk_1073742017_1193, blk_1073742018_1194, blk_1073742019_1195, blk_1073742020_1196, blk_1073742021_1197, blk_1073742022_1198, blk_1073742023_1199, blk_1073742024_1200, blk_1073742025_1201, blk_1073742026_1202, blk_1073742027_1203, blk_1073742028_1204, blk_1073742029_1205, blk_1073742030_1206, blk_1073742031_1207, blk_1073742032_1208, blk_1073742033_1209, blk_1073742034_1210, blk_1073742035_1211, blk_1073742036_1212, blk_1073742037_1213, blk_1073742038_1214, blk_1073742039_1215, blk_1073742040_1216, blk_1073742041_1217, blk_1073742042_1218, blk_1073742043_1219, blk_1073742044_1220, blk_1073742045_1221, blk_1073742046_1222, blk_1073742047_1223, blk_1073742048_1224, blk_1073742049_1225, blk_1073742050_1226, blk_1073742051_1227, blk_1073742052_1228, blk_1073742053_1229, blk_1073742054_1230, blk_1073742055_1231, blk_1073742056_1232, blk_1073742057_1233, blk_1073742058_1234, blk_1073742059_1235, blk_1073742060_1236, blk_1073742061_1237, blk_1073742062_1238, blk_1073742063_1239, blk_1073742064_1240, blk_1073742065_1241, blk_1073742066_1242, blk_1073742067_1243, blk_1073742068_1244, blk_1073742069_1245, blk_1073742070_1246, blk_1073742071_1247, blk_1073742072_1248, blk_1073742073_1249, blk_1073742074_1250, blk_1073742075_1251, blk_1073742076_1252, blk_1073742077_1253, blk_1073742078_1254, blk_1073742079_1255, blk_1073742080_1256, blk_1073742081_1257, blk_1073742082_1258, blk_1073742083_1259, blk_1073742084_1260, blk_1073742085_1261, blk_1073742086_1262, blk_1073742087_1263, blk_1073742088_1264, blk_1073742089_1265, blk_1073742090_1266, blk_1073742091_1267, blk_1073742092_1268, blk_1073742093_1269, blk_1073742094_1270, blk_1073742096_1272, blk_1073742097_1273, blk_1073742098_1274, blk_1073742099_1275, blk_1073742101_1277, blk_1073742102_1278, blk_1073742104_1280, blk_1073742106_1282, blk_1073742109_1285, blk_1073742112_1288, blk_1073742115_1291, blk_1073742116_1292, blk_1073742117_1293, blk_1073742120_1296, blk_1073742121_1297, blk_1073742122_1298, blk_1073742123_1299, blk_1073742125_1301, blk_1073742126_1302, blk_1073742127_1303, blk_1073742128_1304{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]}, blk_1073742135_1311, blk_1073742139_1315, blk_1073742141_1317, blk_1073742142_1318, blk_1073742144_1320, blk_1073742145_1321, blk_1073742146_1322, blk_1073742148_1324, blk_1073742149_1325, blk_1073742151_1327, blk_1073742154_1330, blk_1073742155_1331, blk_1073742156_1332, blk_1073742157_1333, blk_1073742158_1334, blk_1073742160_1336, blk_1073742161_1337, blk_1073742162_1338, blk_1073742163_1339, blk_1073742165_1341, blk_1073742168_1344, blk_1073742169_1345, blk_1073742170_1346, blk_1073742172_1348, blk_1073742173_1349, blk_1073742176_1352, blk_1073742178_1354, blk_1073742180_1356, blk_1073742181_1357, blk_1073742184_1360, blk_1073742190_1366, blk_1073742192_1368, blk_1073742194_1370, blk_1073742195_1371, blk_1073742196_1372, blk_1073742197_1373, blk_1073742199_1375, blk_1073742200_1376, blk_1073742204_1380, blk_1073742205_1381, blk_1073742206_1382, blk_1073742207_1383, blk_1073742208_1384{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]}, blk_1073742217_1393, blk_1073742219_1395, blk_1073742220_1396, blk_1073742225_1401, blk_1073742229_1405, blk_1073742231_1407, blk_1073742234_1410, blk_1073742236_1412, blk_1073742241_1417, blk_1073742243_1419, blk_1073742244_1420, blk_1073742245_1421, blk_1073742246_1422, blk_1073742248_1424, blk_1073742251_1427, blk_1073742252_1428, blk_1073742254_1430, blk_1073742257_1433, blk_1073742273_1449, blk_1073742274_1450, blk_1073742275_1451, blk_1073742277_1453, blk_1073742278_1454, blk_1073742279_1455, blk_1073742280_1456, blk_1073742282_1458, blk_1073742283_1459, blk_1073742284_1460, blk_1073742285_1461, blk_1073742286_1462, blk_1073742287_1463, blk_1073742288_1464, blk_1073742289_1465, blk_1073742290_1466, blk_1073742291_1467, blk_1073742292_1468, blk_1073742293_1469, blk_1073742294_1470, blk_1073742295_1471, blk_1073742296_1472, blk_1073742297_1473, blk_1073742298_1474, blk_1073742299_1475, blk_1073742300_1476, blk_1073742301_1477, blk_1073742302_1478, blk_1073742303_1479, blk_1073742304_1480, blk_1073742305_1481, blk_1073742306_1482, blk_1073742307_1483, blk_1073742308_1484, blk_1073742309_1485, blk_1073742310_1486, blk_1073742311_1487, blk_1073742312_1488, blk_1073742315_1491, blk_1073742320_1496, blk_1073742322_1498, blk_1073742324_1500, blk_1073742325_1501, blk_1073742326_1502, blk_1073742327_1503, blk_1073742329_1505, blk_1073742331_1507, blk_1073742333_1509, blk_1073742335_1511, blk_1073742337_1513, blk_1073742342_1518, blk_1073742344_1520, blk_1073742347_1523, blk_1073742348_1524, blk_1073742349_1525, blk_1073742358_1534, blk_1073742365_1541, blk_1073742366_1542, blk_1073742367_1543, blk_1073742368_1544, blk_1073742369_1545, blk_1073742373_1549, blk_1073742375_1551, blk_1073742376_1552, blk_1073742377_1553, blk_1073742380_1556, blk_1073742381_1557, blk_1073742383_1559, blk_1073742392_1568, blk_1073742396_1572, blk_1073742398_1574, blk_1073742399_1575, blk_1073742401_1577, blk_1073742402_1578, blk_1073742404_1580, blk_1073742405_1581, blk_1073742406_1582, blk_1073742408_1584, blk_1073742410_1586, blk_1073742411_1587, blk_1073742412_1588, blk_1073742413_1589, blk_1073742414_1590, blk_1073742415_1591, blk_1073742416_1592, blk_1073742417_1593, blk_1073742418_1594, blk_1073742419_1595, blk_1073742420_1596, blk_1073742421_1597, blk_1073742422_1598, blk_1073742423_1599, blk_1073742424_1600, blk_1073742425_1601, blk_1073742426_1602, blk_1073742427_1603, blk_1073742428_1604, blk_1073742429_1605, blk_1073742430_1606, blk_1073742431_1607, blk_1073742432_1608, blk_1073742433_1609, blk_1073742434_1610, blk_1073742435_1611, blk_1073742436_1612, blk_1073742437_1613, blk_1073742438_1614, blk_1073742439_1615, blk_1073742440_1616, blk_1073742441_1617, blk_1073742442_1618, blk_1073742443_1619, blk_1073742445_1621, blk_1073742446_1622, blk_1073742452_1628, blk_1073742453_1629, blk_1073742455_1631, blk_1073742456_1632, blk_1073742458_1634, blk_1073742459_1635, blk_1073742461_1637, blk_1073742462_1638, blk_1073742463_1639, blk_1073742465_1641, blk_1073742467_1643, blk_1073742468_1644, blk_1073742469_1645, blk_1073742470_1646, blk_1073742475_1651, blk_1073742476_1652, blk_1073742477_1653, blk_1073742480_1656, blk_1073742482_1658{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-00b0d8d1-662a-44cb-9462-5bddd88a0a50:NORMAL:172.18.0.2:50010|RBW], ReplicaUC[[DISK]DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2:NORMAL:172.18.0.3:50010|RBW]]}, blk_1073742483_1659, blk_1073742484_1660, blk_1073742487_1663, blk_1073742488_1664, blk_1073742491_1667, blk_1073742492_1668, blk_1073742494_1670, blk_1073742496_1672, blk_1073742499_1675, blk_1073742501_1677, blk_1073742502_1678, blk_1073742505_1681, blk_1073742506_1682, blk_1073742507_1683, blk_1073742509_1685, blk_1073742510_1686, blk_1073742512_1688, blk_1073742513_1689, blk_1073742515_1691, blk_1073742516_1692, blk_1073742517_1693, blk_1073742518_1694, blk_1073742519_1695, blk_1073742520_1696, blk_1073742521_1697, blk_1073742535_1711, blk_1073742542_1718, blk_1073742549_1725, blk_1073742557_1733, blk_1073742610_1786, blk_1073742617_1793, blk_1073742620_1796, blk_1073742625_1801, blk_1073742631_1807, blk_1073742635_1811, blk_1073742640_1816, blk_1073742662_1838, blk_1073742663_1839, blk_1073742666_1842, blk_1073742668_1844, blk_1073742673_1849, blk_1073742674_1850, blk_1073742677_1853, blk_1073742683_1859, blk_1073742687_1863, blk_1073742689_1865, blk_1073742690_1866, blk_1073742692_1868, blk_1073742694_1870, blk_1073742696_1872, blk_1073742698_1874, blk_1073742701_1877, blk_1073742714_1890, blk_1073742715_1891, blk_1073742716_1892, blk_1073742720_1896, blk_1073742721_1897, blk_1073742723_1899, blk_1073742724_1900, blk_1073742730_1906, blk_1073742735_1911, blk_1073742744_1920, blk_1073742750_1926, blk_1073742752_1928, blk_1073742753_1929, blk_1073742755_1931, blk_1073742757_1933, blk_1073742758_1934, blk_1073742781_1957, blk_1073742796_1972, blk_1073742798_1974, blk_1073742799_1975, blk_1073742800_1976, blk_1073742801_1977]
2019-01-27 12:44:53,127 INFO org.apache.hadoop.hdfs.StateChange: *DIR* reportBadBlocks
2019-01-27 12:44:53,127 INFO BlockStateChange: BLOCK* findAndMarkBlockAsCorrupt: BP-1125863354-172.17.0.2-1548592171015:blk_1073742800_1976 not found
2019-01-27 12:44:53,137 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.3:50010 is added to blk_1073742803_1979 size 260792
2019-01-27 12:45:22,505 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.4
2019-01-27 12:45:22,506 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2019-01-27 12:45:22,506 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2019-01-27 12:45:22,506 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 5122 Total time for transactions(ms): 514 Number of transactions batched in Syncs: 1200 Number of syncs: 1975 SyncTimes(ms): 146302 
2019-01-27 12:45:22,515 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 5122 Total time for transactions(ms): 514 Number of transactions batched in Syncs: 1200 Number of syncs: 1976 SyncTimes(ms): 146311 
2019-01-27 12:45:22,516 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /root/hdfs/namenode/current/edits_inprogress_0000000000000000001 -> /root/hdfs/namenode/current/edits_0000000000000000001-0000000000000005122
2019-01-27 12:45:22,519 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 5123
2019-01-27 12:45:25,442 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.02s at 117.65 KB/s
2019-01-27 12:45:25,443 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000005122 size 2954 bytes.
2019-01-27 12:45:25,463 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2019-01-27 12:45:50,687 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742797_1973 172.18.0.3:50010 172.18.0.2:50010 
2019-01-27 12:45:51,818 INFO BlockStateChange: BLOCK* BlockManager: ask 172.18.0.2:50010 to delete [blk_1073742797_1973]
2019-01-27 12:45:54,818 INFO BlockStateChange: BLOCK* BlockManager: ask 172.18.0.3:50010 to delete [blk_1073742797_1973]
2019-01-27 13:10:49,524 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742175_1351 to 172.18.0.3:50010
2019-01-27 13:10:49,524 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742174_1350 to 172.18.0.3:50010
2019-01-27 13:10:49,524 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742171_1347 to 172.18.0.3:50010
2019-01-27 13:10:49,524 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742167_1343 to 172.18.0.3:50010
2019-01-27 13:10:49,524 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742166_1342 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742164_1340 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742159_1335 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742152_1328 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742153_1329 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742150_1326 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742147_1323 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742201_1377 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742203_1379 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742202_1378 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742198_1374 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742193_1369 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742188_1364 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742189_1365 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742191_1367 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742185_1361 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742186_1362 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742187_1363 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742182_1358 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742183_1359 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742177_1353 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742179_1355 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742107_1283 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742105_1281 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742111_1287 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742110_1286 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742108_1284 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742103_1279 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742100_1276 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742095_1271 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742137_1313 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742136_1312 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742138_1314 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742140_1316 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742143_1319 to 172.18.0.3:50010
2019-01-27 13:10:49,525 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742129_1305 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742131_1307 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742130_1306 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742133_1309 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742132_1308 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742134_1310 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742124_1300 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742113_1289 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742114_1290 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742118_1294 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742119_1295 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742272_1448 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742281_1457 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742321_1497 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742323_1499 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742328_1504 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742330_1506 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742317_1493 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742316_1492 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742319_1495 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742318_1494 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742313_1489 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742314_1490 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742226_1402 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742227_1403 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742224_1400 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742230_1406 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742228_1404 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742235_1411 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742232_1408 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742233_1409 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742238_1414 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742239_1415 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742237_1413 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742211_1387 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742210_1386 to 172.18.0.3:50010
2019-01-27 13:10:49,526 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742209_1385 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742215_1391 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742214_1390 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742213_1389 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742212_1388 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742218_1394 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742216_1392 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742223_1399 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742222_1398 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742221_1397 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742256_1432 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742258_1434 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742259_1435 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742260_1436 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742261_1437 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742262_1438 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742263_1439 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742264_1440 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742265_1441 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742266_1442 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742267_1443 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742268_1444 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742269_1445 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742270_1446 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742271_1447 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742240_1416 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742242_1418 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742247_1423 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742249_1425 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742250_1426 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742253_1429 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742255_1431 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742447_1623 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742444_1620 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742457_1633 to 172.18.0.3:50010
2019-01-27 13:10:49,527 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742460_1636 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742451_1627 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742450_1626 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742449_1625 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742448_1624 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742454_1630 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742409_1585 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742382_1558 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742378_1554 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742379_1555 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742374_1550 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742397_1573 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742395_1571 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742394_1570 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742393_1569 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742391_1567 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742390_1566 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742389_1565 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742388_1564 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742387_1563 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742386_1562 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742385_1561 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742384_1560 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742364_1540 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742361_1537 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742360_1536 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742363_1539 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742362_1538 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742357_1533 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742356_1532 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742359_1535 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742563_1739 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742562_1738 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742561_1737 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742560_1736 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742567_1743 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742566_1742 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742565_1741 to 172.18.0.3:50010
2019-01-27 13:10:49,528 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742564_1740 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742571_1747 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742570_1746 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742569_1745 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742568_1744 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742575_1751 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742574_1750 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742573_1749 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742572_1748 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742578_1754 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742579_1755 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742576_1752 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742577_1753 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742582_1758 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742583_1759 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742580_1756 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742581_1757 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742586_1762 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742587_1763 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742584_1760 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742585_1761 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742590_1766 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742591_1767 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742588_1764 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742589_1765 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742529_1705 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742528_1704 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742531_1707 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742530_1706 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742533_1709 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742532_1708 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742534_1710 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742537_1713 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742536_1712 to 172.18.0.3:50010
2019-01-27 13:10:49,529 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742539_1715 to 172.18.0.3:50010
2019-01-27 13:10:49,530 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742538_1714 to 172.18.0.3:50010
2019-01-27 13:10:49,530 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742541_1717 to 172.18.0.3:50010
2019-01-27 13:10:49,530 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742540_1716 to 172.18.0.3:50010
2019-01-27 13:10:49,530 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742543_1719 to 172.18.0.3:50010
2019-01-27 13:10:49,530 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742544_1720 to 172.18.0.3:50010
2019-01-27 13:10:49,530 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742545_1721 to 172.18.0.3:50010
2019-01-27 13:10:49,530 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742546_1722 to 172.18.0.3:50010
2019-01-27 13:10:49,530 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742547_1723 to 172.18.0.3:50010
2019-01-27 13:10:49,530 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742548_1724 to 172.18.0.3:50010
2019-01-27 13:10:49,530 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742550_1726 to 172.18.0.3:50010
2019-01-27 13:10:49,530 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742551_1727 to 172.18.0.3:50010
2019-01-27 13:10:49,530 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742552_1728 to 172.18.0.3:50010
2019-01-27 13:10:49,530 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742553_1729 to 172.18.0.3:50010
2019-01-27 13:10:49,530 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742554_1730 to 172.18.0.3:50010
2019-01-27 13:10:49,530 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742555_1731 to 172.18.0.3:50010
2019-01-27 13:10:49,530 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742556_1732 to 172.18.0.3:50010
2019-01-27 13:10:49,530 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742558_1734 to 172.18.0.3:50010
2019-01-27 13:10:49,530 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742559_1735 to 172.18.0.3:50010
2019-01-27 13:10:49,530 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742503_1679 to 172.18.0.3:50010
2019-01-27 13:10:49,530 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742500_1676 to 172.18.0.3:50010
2019-01-27 13:10:49,530 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742498_1674 to 172.18.0.3:50010
2019-01-27 13:10:49,530 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742497_1673 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742511_1687 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742508_1684 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742504_1680 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742514_1690 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742526_1702 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742527_1703 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742524_1700 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742525_1701 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742522_1698 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742523_1699 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742471_1647 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742464_1640 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742466_1642 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742479_1655 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742478_1654 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742473_1649 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742472_1648 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742474_1650 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742485_1661 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742486_1662 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742481_1657 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742493_1669 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742495_1671 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742489_1665 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742490_1666 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742713_1889 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742712_1888 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742705_1881 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742704_1880 to 172.18.0.3:50010
2019-01-27 13:10:49,531 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742707_1883 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742706_1882 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742709_1885 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742708_1884 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742711_1887 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742710_1886 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742697_1873 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742699_1875 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742700_1876 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742702_1878 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742703_1879 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742695_1871 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742667_1843 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742664_1840 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742665_1841 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742669_1845 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742658_1834 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742659_1835 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742656_1832 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742657_1833 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742660_1836 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742661_1837 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742653_1829 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742652_1828 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742655_1831 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742654_1830 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742649_1825 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742648_1824 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742651_1827 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742650_1826 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742645_1821 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742644_1820 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742647_1823 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742646_1822 to 172.18.0.3:50010
2019-01-27 13:10:49,532 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742641_1817 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742643_1819 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742642_1818 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742636_1812 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742637_1813 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742638_1814 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742639_1815 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742632_1808 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742633_1809 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742634_1810 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742628_1804 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742629_1805 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742630_1806 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742624_1800 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742626_1802 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742627_1803 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742623_1799 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742622_1798 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742621_1797 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742619_1795 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742618_1794 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742616_1792 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742615_1791 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742614_1790 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742613_1789 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742612_1788 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742611_1787 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742609_1785 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742608_1784 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742606_1782 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742607_1783 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742604_1780 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742605_1781 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742602_1778 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742603_1779 to 172.18.0.3:50010
2019-01-27 13:10:49,533 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742600_1776 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742601_1777 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742598_1774 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742599_1775 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742596_1772 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742597_1773 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742594_1770 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742595_1771 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742592_1768 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742593_1769 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742787_1963 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742786_1962 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742784_1960 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742789_1965 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742795_1971 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742794_1970 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742793_1969 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742792_1968 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742772_1948 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742774_1950 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742775_1951 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742768_1944 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742769_1945 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742770_1946 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742771_1947 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742782_1958 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742776_1952 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742777_1953 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742756_1932 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742759_1935 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742754_1930 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742765_1941 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742764_1940 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742767_1943 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742766_1942 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742763_1939 to 172.18.0.3:50010
2019-01-27 13:10:49,534 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742742_1918 to 172.18.0.3:50010
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742743_1919 to 172.18.0.3:50010
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742741_1917 to 172.18.0.3:50010
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742736_1912 to 172.18.0.3:50010
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742751_1927 to 172.18.0.3:50010
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742748_1924 to 172.18.0.3:50010
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742749_1925 to 172.18.0.3:50010
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742746_1922 to 172.18.0.3:50010
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742747_1923 to 172.18.0.3:50010
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742745_1921 to 172.18.0.3:50010
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742727_1903 to 172.18.0.3:50010
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742726_1902 to 172.18.0.3:50010
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742725_1901 to 172.18.0.3:50010
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742722_1898 to 172.18.0.3:50010
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742734_1910 to 172.18.0.3:50010
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742733_1909 to 172.18.0.3:50010
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742732_1908 to 172.18.0.3:50010
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742731_1907 to 172.18.0.3:50010
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742729_1905 to 172.18.0.3:50010
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* InvalidateBlocks: add blk_1073742728_1904 to 172.18.0.3:50010
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* processReport: blk_1073742175_1351 on node 172.18.0.3:50010 size 322560 does not belong to any file
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* processReport: blk_1073742174_1350 on node 172.18.0.3:50010 size 258048 does not belong to any file
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* processReport: blk_1073742171_1347 on node 172.18.0.3:50010 size 193536 does not belong to any file
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* processReport: blk_1073742167_1343 on node 172.18.0.3:50010 size 2519246 does not belong to any file
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* processReport: blk_1073742166_1342 on node 172.18.0.3:50010 size 322560 does not belong to any file
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* processReport: blk_1073742164_1340 on node 172.18.0.3:50010 size 580608 does not belong to any file
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* processReport: blk_1073742159_1335 on node 172.18.0.3:50010 size 387072 does not belong to any file
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* processReport: blk_1073742152_1328 on node 172.18.0.3:50010 size 451584 does not belong to any file
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* processReport: blk_1073742153_1329 on node 172.18.0.3:50010 size 451584 does not belong to any file
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* processReport: blk_1073742150_1326 on node 172.18.0.3:50010 size 516096 does not belong to any file
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* processReport: blk_1073742147_1323 on node 172.18.0.3:50010 size 516096 does not belong to any file
2019-01-27 13:10:49,535 INFO BlockStateChange: BLOCK* processReport: blk_1073742201_1377 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742203_1379 on node 172.18.0.3:50010 size 258048 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742202_1378 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742198_1374 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742193_1369 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742188_1364 on node 172.18.0.3:50010 size 451584 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742189_1365 on node 172.18.0.3:50010 size 193536 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742191_1367 on node 172.18.0.3:50010 size 193536 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742185_1361 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742186_1362 on node 172.18.0.3:50010 size 193536 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742187_1363 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742182_1358 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742183_1359 on node 172.18.0.3:50010 size 172424 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742177_1353 on node 172.18.0.3:50010 size 193536 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742179_1355 on node 172.18.0.3:50010 size 193536 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742107_1283 on node 172.18.0.3:50010 size 903168 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742105_1281 on node 172.18.0.3:50010 size 9354240 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742111_1287 on node 172.18.0.3:50010 size 774144 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742110_1286 on node 172.18.0.3:50010 size 1870848 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742108_1284 on node 172.18.0.3:50010 size 7354368 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742103_1279 on node 172.18.0.3:50010 size 10257408 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742100_1276 on node 172.18.0.3:50010 size 12773376 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742095_1271 on node 172.18.0.3:50010 size 7096320 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742137_1313 on node 172.18.0.3:50010 size 645120 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742136_1312 on node 172.18.0.3:50010 size 774144 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742138_1314 on node 172.18.0.3:50010 size 4064256 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742140_1316 on node 172.18.0.3:50010 size 1483776 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742143_1319 on node 172.18.0.3:50010 size 451584 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742129_1305 on node 172.18.0.3:50010 size 580608 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742131_1307 on node 172.18.0.3:50010 size 645120 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742130_1306 on node 172.18.0.3:50010 size 580608 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742133_1309 on node 172.18.0.3:50010 size 516096 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742132_1308 on node 172.18.0.3:50010 size 709632 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742134_1310 on node 172.18.0.3:50010 size 774144 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742124_1300 on node 172.18.0.3:50010 size 709632 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742113_1289 on node 172.18.0.3:50010 size 1612800 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742114_1290 on node 172.18.0.3:50010 size 6967296 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742118_1294 on node 172.18.0.3:50010 size 5354496 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742119_1295 on node 172.18.0.3:50010 size 1096704 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742272_1448 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742281_1457 on node 172.18.0.3:50010 size 0 does not belong to any file
2019-01-27 13:10:49,536 INFO BlockStateChange: BLOCK* processReport: blk_1073742321_1497 on node 172.18.0.3:50010 size 0 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742323_1499 on node 172.18.0.3:50010 size 774144 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742328_1504 on node 172.18.0.3:50010 size 0 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742330_1506 on node 172.18.0.3:50010 size 451584 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742317_1493 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742316_1492 on node 172.18.0.3:50010 size 0 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742319_1495 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742318_1494 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742313_1489 on node 172.18.0.3:50010 size 1032192 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742314_1490 on node 172.18.0.3:50010 size 7354368 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742226_1402 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742227_1403 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742224_1400 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742230_1406 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742228_1404 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742235_1411 on node 172.18.0.3:50010 size 0 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742232_1408 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742233_1409 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742238_1414 on node 172.18.0.3:50010 size 0 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742239_1415 on node 172.18.0.3:50010 size 0 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742237_1413 on node 172.18.0.3:50010 size 0 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742211_1387 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742210_1386 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742209_1385 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742215_1391 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742214_1390 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742213_1389 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742212_1388 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742218_1394 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742216_1392 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742223_1399 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742222_1398 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742221_1397 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742256_1432 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742258_1434 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742259_1435 on node 172.18.0.3:50010 size 0 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742260_1436 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742261_1437 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742262_1438 on node 172.18.0.3:50010 size 0 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742263_1439 on node 172.18.0.3:50010 size 0 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742264_1440 on node 172.18.0.3:50010 size 0 does not belong to any file
2019-01-27 13:10:49,537 INFO BlockStateChange: BLOCK* processReport: blk_1073742265_1441 on node 172.18.0.3:50010 size 0 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742266_1442 on node 172.18.0.3:50010 size 0 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742267_1443 on node 172.18.0.3:50010 size 0 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742268_1444 on node 172.18.0.3:50010 size 0 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742269_1445 on node 172.18.0.3:50010 size 0 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742270_1446 on node 172.18.0.3:50010 size 0 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742271_1447 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742240_1416 on node 172.18.0.3:50010 size 387072 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742242_1418 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742247_1423 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742249_1425 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742250_1426 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742253_1429 on node 172.18.0.3:50010 size 193536 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742255_1431 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742447_1623 on node 172.18.0.3:50010 size 9676800 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742444_1620 on node 172.18.0.3:50010 size 12579840 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742457_1633 on node 172.18.0.3:50010 size 6902784 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742460_1636 on node 172.18.0.3:50010 size 7483392 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742451_1627 on node 172.18.0.3:50010 size 9483264 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742450_1626 on node 172.18.0.3:50010 size 10063872 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742449_1625 on node 172.18.0.3:50010 size 8838144 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742448_1624 on node 172.18.0.3:50010 size 10579968 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742454_1630 on node 172.18.0.3:50010 size 46771200 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742409_1585 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742382_1558 on node 172.18.0.3:50010 size 4967424 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742378_1554 on node 172.18.0.3:50010 size 4515840 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742379_1555 on node 172.18.0.3:50010 size 580608 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742374_1550 on node 172.18.0.3:50010 size 387072 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742397_1573 on node 172.18.0.3:50010 size 322560 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742395_1571 on node 172.18.0.3:50010 size 451584 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742394_1570 on node 172.18.0.3:50010 size 387072 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742393_1569 on node 172.18.0.3:50010 size 322560 does not belong to any file
2019-01-27 13:10:49,538 INFO BlockStateChange: BLOCK* processReport: blk_1073742391_1567 on node 172.18.0.3:50010 size 387072 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742390_1566 on node 172.18.0.3:50010 size 451584 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742389_1565 on node 172.18.0.3:50010 size 516096 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742388_1564 on node 172.18.0.3:50010 size 516096 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742387_1563 on node 172.18.0.3:50010 size 580608 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742386_1562 on node 172.18.0.3:50010 size 387072 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742385_1561 on node 172.18.0.3:50010 size 387072 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742384_1560 on node 172.18.0.3:50010 size 387072 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742364_1540 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742361_1537 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742360_1536 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742363_1539 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742362_1538 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742357_1533 on node 172.18.0.3:50010 size 193536 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742356_1532 on node 172.18.0.3:50010 size 1612800 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742359_1535 on node 172.18.0.3:50010 size 193536 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742563_1739 on node 172.18.0.3:50010 size 5289984 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742562_1738 on node 172.18.0.3:50010 size 6902784 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742561_1737 on node 172.18.0.3:50010 size 6515712 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742560_1736 on node 172.18.0.3:50010 size 7547904 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742567_1743 on node 172.18.0.3:50010 size 4580352 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742566_1742 on node 172.18.0.3:50010 size 4128768 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742565_1741 on node 172.18.0.3:50010 size 4451328 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742564_1740 on node 172.18.0.3:50010 size 5392111 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742571_1747 on node 172.18.0.3:50010 size 2709504 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742570_1746 on node 172.18.0.3:50010 size 2515968 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742569_1745 on node 172.18.0.3:50010 size 3096576 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742568_1744 on node 172.18.0.3:50010 size 8838144 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742575_1751 on node 172.18.0.3:50010 size 2709504 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742574_1750 on node 172.18.0.3:50010 size 4580352 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742573_1749 on node 172.18.0.3:50010 size 6193152 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742572_1748 on node 172.18.0.3:50010 size 8709120 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742578_1754 on node 172.18.0.3:50010 size 2451456 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742579_1755 on node 172.18.0.3:50010 size 3612672 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742576_1752 on node 172.18.0.3:50010 size 10708992 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742577_1753 on node 172.18.0.3:50010 size 5160960 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742582_1758 on node 172.18.0.3:50010 size 3612672 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742583_1759 on node 172.18.0.3:50010 size 5354496 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742580_1756 on node 172.18.0.3:50010 size 4322304 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742581_1757 on node 172.18.0.3:50010 size 4902912 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742586_1762 on node 172.18.0.3:50010 size 4773888 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742587_1763 on node 172.18.0.3:50010 size 9612288 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742584_1760 on node 172.18.0.3:50010 size 4644864 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742585_1761 on node 172.18.0.3:50010 size 8967168 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742590_1766 on node 172.18.0.3:50010 size 7547904 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742591_1767 on node 172.18.0.3:50010 size 6451200 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742588_1764 on node 172.18.0.3:50010 size 8451072 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742589_1765 on node 172.18.0.3:50010 size 8257536 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742529_1705 on node 172.18.0.3:50010 size 83994624 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742528_1704 on node 172.18.0.3:50010 size 7612416 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742531_1707 on node 172.18.0.3:50010 size 7870464 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742530_1706 on node 172.18.0.3:50010 size 8128512 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742533_1709 on node 172.18.0.3:50010 size 7160832 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742532_1708 on node 172.18.0.3:50010 size 6838272 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742534_1710 on node 172.18.0.3:50010 size 5806080 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742537_1713 on node 172.18.0.3:50010 size 8644608 does not belong to any file
2019-01-27 13:10:49,539 INFO BlockStateChange: BLOCK* processReport: blk_1073742536_1712 on node 172.18.0.3:50010 size 5031936 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742539_1715 on node 172.18.0.3:50010 size 1548288 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742538_1714 on node 172.18.0.3:50010 size 4515840 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742541_1717 on node 172.18.0.3:50010 size 5031936 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742540_1716 on node 172.18.0.3:50010 size 7225344 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742543_1719 on node 172.18.0.3:50010 size 5354496 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742544_1720 on node 172.18.0.3:50010 size 7547904 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742545_1721 on node 172.18.0.3:50010 size 4064256 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742546_1722 on node 172.18.0.3:50010 size 10708992 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742547_1723 on node 172.18.0.3:50010 size 3870720 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742548_1724 on node 172.18.0.3:50010 size 8967168 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742550_1726 on node 172.18.0.3:50010 size 6580224 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742551_1727 on node 172.18.0.3:50010 size 4257792 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742552_1728 on node 172.18.0.3:50010 size 6838272 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742553_1729 on node 172.18.0.3:50010 size 5160960 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742554_1730 on node 172.18.0.3:50010 size 5031936 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742555_1731 on node 172.18.0.3:50010 size 5870592 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742556_1732 on node 172.18.0.3:50010 size 5999616 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742558_1734 on node 172.18.0.3:50010 size 9418752 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742559_1735 on node 172.18.0.3:50010 size 7999488 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742503_1679 on node 172.18.0.3:50010 size 1161216 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742500_1676 on node 172.18.0.3:50010 size 1548288 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742498_1674 on node 172.18.0.3:50010 size 1677312 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742497_1673 on node 172.18.0.3:50010 size 1935360 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742511_1687 on node 172.18.0.3:50010 size 903168 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742508_1684 on node 172.18.0.3:50010 size 838656 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742504_1680 on node 172.18.0.3:50010 size 1096704 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742514_1690 on node 172.18.0.3:50010 size 387072 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742526_1702 on node 172.18.0.3:50010 size 8580096 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742527_1703 on node 172.18.0.3:50010 size 9805824 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742524_1700 on node 172.18.0.3:50010 size 67415040 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742525_1701 on node 172.18.0.3:50010 size 70898688 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742522_1698 on node 172.18.0.3:50010 size 74769408 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742523_1699 on node 172.18.0.3:50010 size 82059264 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742471_1647 on node 172.18.0.3:50010 size 5031936 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742464_1640 on node 172.18.0.3:50010 size 5289984 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742466_1642 on node 172.18.0.3:50010 size 4193280 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742479_1655 on node 172.18.0.3:50010 size 3225600 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742478_1654 on node 172.18.0.3:50010 size 4128768 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742473_1649 on node 172.18.0.3:50010 size 4451328 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742472_1648 on node 172.18.0.3:50010 size 4644864 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742474_1650 on node 172.18.0.3:50010 size 4386816 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742485_1661 on node 172.18.0.3:50010 size 3225600 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742486_1662 on node 172.18.0.3:50010 size 2322432 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742481_1657 on node 172.18.0.3:50010 size 3483648 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742493_1669 on node 172.18.0.3:50010 size 1806336 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742495_1671 on node 172.18.0.3:50010 size 2515968 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742489_1665 on node 172.18.0.3:50010 size 2903040 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742490_1666 on node 172.18.0.3:50010 size 2451456 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742713_1889 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742712_1888 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,540 INFO BlockStateChange: BLOCK* processReport: blk_1073742705_1881 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742704_1880 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742707_1883 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742706_1882 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742709_1885 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742708_1884 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742711_1887 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742710_1886 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742697_1873 on node 172.18.0.3:50010 size 838656 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742699_1875 on node 172.18.0.3:50010 size 580608 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742700_1876 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742702_1878 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742703_1879 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742695_1871 on node 172.18.0.3:50010 size 774144 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742667_1843 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742664_1840 on node 172.18.0.3:50010 size 451584 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742665_1841 on node 172.18.0.3:50010 size 1612800 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742669_1845 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742658_1834 on node 172.18.0.3:50010 size 1999872 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742659_1835 on node 172.18.0.3:50010 size 193536 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742656_1832 on node 172.18.0.3:50010 size 1870848 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742657_1833 on node 172.18.0.3:50010 size 1806336 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742660_1836 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742661_1837 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742653_1829 on node 172.18.0.3:50010 size 2967552 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742652_1828 on node 172.18.0.3:50010 size 2967552 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742655_1831 on node 172.18.0.3:50010 size 2774016 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742654_1830 on node 172.18.0.3:50010 size 2838528 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742649_1825 on node 172.18.0.3:50010 size 1677312 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742648_1824 on node 172.18.0.3:50010 size 1806336 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742651_1827 on node 172.18.0.3:50010 size 3419136 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742650_1826 on node 172.18.0.3:50010 size 1290240 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742645_1821 on node 172.18.0.3:50010 size 3806208 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742644_1820 on node 172.18.0.3:50010 size 2515968 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742647_1823 on node 172.18.0.3:50010 size 1612800 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742646_1822 on node 172.18.0.3:50010 size 2257920 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742641_1817 on node 172.18.0.3:50010 size 3032064 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742643_1819 on node 172.18.0.3:50010 size 2322432 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742642_1818 on node 172.18.0.3:50010 size 2580480 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742636_1812 on node 172.18.0.3:50010 size 4580352 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742637_1813 on node 172.18.0.3:50010 size 3806208 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742638_1814 on node 172.18.0.3:50010 size 1225728 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742639_1815 on node 172.18.0.3:50010 size 3523938 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742632_1808 on node 172.18.0.3:50010 size 1612800 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742633_1809 on node 172.18.0.3:50010 size 1548288 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742634_1810 on node 172.18.0.3:50010 size 4967424 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742628_1804 on node 172.18.0.3:50010 size 2257920 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742629_1805 on node 172.18.0.3:50010 size 2451456 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742630_1806 on node 172.18.0.3:50010 size 2322432 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742624_1800 on node 172.18.0.3:50010 size 1677312 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742626_1802 on node 172.18.0.3:50010 size 2774016 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742627_1803 on node 172.18.0.3:50010 size 1677312 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742623_1799 on node 172.18.0.3:50010 size 1483776 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742622_1798 on node 172.18.0.3:50010 size 5548032 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742621_1797 on node 172.18.0.3:50010 size 5483520 does not belong to any file
2019-01-27 13:10:49,541 INFO BlockStateChange: BLOCK* processReport: blk_1073742619_1795 on node 172.18.0.3:50010 size 6386688 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742618_1794 on node 172.18.0.3:50010 size 1806336 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742616_1792 on node 172.18.0.3:50010 size 5806080 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742615_1791 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742614_1790 on node 172.18.0.3:50010 size 1548288 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742613_1789 on node 172.18.0.3:50010 size 1935360 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742612_1788 on node 172.18.0.3:50010 size 2128896 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742611_1787 on node 172.18.0.3:50010 size 1999872 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742609_1785 on node 172.18.0.3:50010 size 4128768 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742608_1784 on node 172.18.0.3:50010 size 2967552 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742606_1782 on node 172.18.0.3:50010 size 2644992 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742607_1783 on node 172.18.0.3:50010 size 2967552 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742604_1780 on node 172.18.0.3:50010 size 4515840 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742605_1781 on node 172.18.0.3:50010 size 4451328 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742602_1778 on node 172.18.0.3:50010 size 5096448 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742603_1779 on node 172.18.0.3:50010 size 3483648 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742600_1776 on node 172.18.0.3:50010 size 7354368 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742601_1777 on node 172.18.0.3:50010 size 6064128 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742598_1774 on node 172.18.0.3:50010 size 8515584 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742599_1775 on node 172.18.0.3:50010 size 5419008 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742596_1772 on node 172.18.0.3:50010 size 6257664 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742597_1773 on node 172.18.0.3:50010 size 7934976 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742594_1770 on node 172.18.0.3:50010 size 5935104 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742595_1771 on node 172.18.0.3:50010 size 5612544 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742592_1768 on node 172.18.0.3:50010 size 8064000 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742593_1769 on node 172.18.0.3:50010 size 6057905 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742787_1963 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742786_1962 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742784_1960 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742789_1965 on node 172.18.0.3:50010 size 0 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742795_1971 on node 172.18.0.3:50010 size 0 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742794_1970 on node 172.18.0.3:50010 size 0 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742793_1969 on node 172.18.0.3:50010 size 0 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742792_1968 on node 172.18.0.3:50010 size 0 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742772_1948 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742774_1950 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742775_1951 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742768_1944 on node 172.18.0.3:50010 size 1032192 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742769_1945 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742770_1946 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742771_1947 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742782_1958 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742776_1952 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742777_1953 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742756_1932 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742759_1935 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742754_1930 on node 172.18.0.3:50010 size 193536 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742765_1941 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742764_1940 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742767_1943 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742766_1942 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742763_1939 on node 172.18.0.3:50010 size 1032192 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742742_1918 on node 172.18.0.3:50010 size 2644992 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742743_1919 on node 172.18.0.3:50010 size 322560 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742741_1917 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742736_1912 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,542 INFO BlockStateChange: BLOCK* processReport: blk_1073742751_1927 on node 172.18.0.3:50010 size 193536 does not belong to any file
2019-01-27 13:10:49,543 INFO BlockStateChange: BLOCK* processReport: blk_1073742748_1924 on node 172.18.0.3:50010 size 193536 does not belong to any file
2019-01-27 13:10:49,543 INFO BlockStateChange: BLOCK* processReport: blk_1073742749_1925 on node 172.18.0.3:50010 size 193536 does not belong to any file
2019-01-27 13:10:49,543 INFO BlockStateChange: BLOCK* processReport: blk_1073742746_1922 on node 172.18.0.3:50010 size 258048 does not belong to any file
2019-01-27 13:10:49,543 INFO BlockStateChange: BLOCK* processReport: blk_1073742747_1923 on node 172.18.0.3:50010 size 258048 does not belong to any file
2019-01-27 13:10:49,543 INFO BlockStateChange: BLOCK* processReport: blk_1073742745_1921 on node 172.18.0.3:50010 size 258048 does not belong to any file
2019-01-27 13:10:49,543 INFO BlockStateChange: BLOCK* processReport: blk_1073742727_1903 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,543 INFO BlockStateChange: BLOCK* processReport: blk_1073742726_1902 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,543 INFO BlockStateChange: BLOCK* processReport: blk_1073742725_1901 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,543 INFO BlockStateChange: BLOCK* processReport: blk_1073742722_1898 on node 172.18.0.3:50010 size 193536 does not belong to any file
2019-01-27 13:10:49,543 INFO BlockStateChange: BLOCK* processReport: blk_1073742734_1910 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,543 INFO BlockStateChange: BLOCK* processReport: blk_1073742733_1909 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,543 INFO BlockStateChange: BLOCK* processReport: blk_1073742732_1908 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,543 INFO BlockStateChange: BLOCK* processReport: blk_1073742731_1907 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,543 INFO BlockStateChange: BLOCK* processReport: blk_1073742729_1905 on node 172.18.0.3:50010 size 129024 does not belong to any file
2019-01-27 13:10:49,543 INFO BlockStateChange: BLOCK* processReport: blk_1073742728_1904 on node 172.18.0.3:50010 size 64512 does not belong to any file
2019-01-27 13:10:49,543 INFO BlockStateChange: BLOCK* processReport: from storage DS-7db0f1d4-b9d5-4861-bf5a-00d5191b8ec2 node DatanodeRegistration(172.18.0.3:50010, datanodeUuid=79bc8e05-505a-43b0-a28c-f23d175866bb, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3428d32f-669a-4980-807b-bac983b05aa8;nsid=545847971;c=0), blocks: 413, hasStaleStorage: false, processing time: 13 msecs
2019-01-27 13:10:51,941 INFO BlockStateChange: BLOCK* BlockManager: ask 172.18.0.3:50010 to delete [blk_1073742356_1532, blk_1073742357_1533, blk_1073742359_1535, blk_1073742360_1536, blk_1073742361_1537, blk_1073742362_1538, blk_1073742363_1539, blk_1073742364_1540, blk_1073742374_1550, blk_1073742378_1554, blk_1073742379_1555, blk_1073742382_1558, blk_1073742384_1560, blk_1073742385_1561, blk_1073742386_1562, blk_1073742387_1563, blk_1073742388_1564, blk_1073742389_1565, blk_1073742390_1566, blk_1073742391_1567, blk_1073742393_1569, blk_1073742394_1570, blk_1073742395_1571, blk_1073742397_1573, blk_1073742409_1585, blk_1073742444_1620, blk_1073742447_1623, blk_1073742448_1624, blk_1073742449_1625, blk_1073742450_1626, blk_1073742451_1627, blk_1073742454_1630, blk_1073742457_1633, blk_1073742460_1636, blk_1073742464_1640, blk_1073742466_1642, blk_1073742471_1647, blk_1073742472_1648, blk_1073742473_1649, blk_1073742474_1650, blk_1073742478_1654, blk_1073742479_1655, blk_1073742481_1657, blk_1073742485_1661, blk_1073742486_1662, blk_1073742489_1665, blk_1073742490_1666, blk_1073742493_1669, blk_1073742495_1671, blk_1073742497_1673, blk_1073742498_1674, blk_1073742500_1676, blk_1073742503_1679, blk_1073742504_1680, blk_1073742508_1684, blk_1073742511_1687, blk_1073742514_1690, blk_1073742522_1698, blk_1073742523_1699, blk_1073742524_1700, blk_1073742525_1701, blk_1073742526_1702, blk_1073742527_1703, blk_1073742528_1704, blk_1073742529_1705, blk_1073742530_1706, blk_1073742531_1707, blk_1073742532_1708, blk_1073742533_1709, blk_1073742534_1710, blk_1073742536_1712, blk_1073742537_1713, blk_1073742538_1714, blk_1073742539_1715, blk_1073742540_1716, blk_1073742541_1717, blk_1073742543_1719, blk_1073742544_1720, blk_1073742545_1721, blk_1073742546_1722, blk_1073742547_1723, blk_1073742548_1724, blk_1073742550_1726, blk_1073742551_1727, blk_1073742552_1728, blk_1073742553_1729, blk_1073742554_1730, blk_1073742555_1731, blk_1073742556_1732, blk_1073742558_1734, blk_1073742559_1735, blk_1073742560_1736, blk_1073742561_1737, blk_1073742562_1738, blk_1073742563_1739, blk_1073742564_1740, blk_1073742565_1741, blk_1073742566_1742, blk_1073742567_1743, blk_1073742568_1744, blk_1073742569_1745, blk_1073742570_1746, blk_1073742571_1747, blk_1073742572_1748, blk_1073742573_1749, blk_1073742574_1750, blk_1073742575_1751, blk_1073742576_1752, blk_1073742577_1753, blk_1073742578_1754, blk_1073742579_1755, blk_1073742580_1756, blk_1073742581_1757, blk_1073742582_1758, blk_1073742583_1759, blk_1073742584_1760, blk_1073742585_1761, blk_1073742586_1762, blk_1073742587_1763, blk_1073742588_1764, blk_1073742589_1765, blk_1073742590_1766, blk_1073742591_1767, blk_1073742592_1768, blk_1073742593_1769, blk_1073742594_1770, blk_1073742595_1771, blk_1073742596_1772, blk_1073742597_1773, blk_1073742598_1774, blk_1073742599_1775, blk_1073742600_1776, blk_1073742601_1777, blk_1073742602_1778, blk_1073742603_1779, blk_1073742604_1780, blk_1073742605_1781, blk_1073742606_1782, blk_1073742607_1783, blk_1073742095_1271, blk_1073742608_1784, blk_1073742609_1785, blk_1073742611_1787, blk_1073742612_1788, blk_1073742100_1276, blk_1073742613_1789, blk_1073742614_1790, blk_1073742615_1791, blk_1073742103_1279, blk_1073742616_1792, blk_1073742105_1281, blk_1073742618_1794, blk_1073742619_1795, blk_1073742107_1283, blk_1073742108_1284, blk_1073742621_1797, blk_1073742622_1798, blk_1073742110_1286, blk_1073742623_1799, blk_1073742111_1287, blk_1073742624_1800, blk_1073742113_1289, blk_1073742626_1802, blk_1073742114_1290, blk_1073742627_1803, blk_1073742628_1804, blk_1073742629_1805, blk_1073742630_1806, blk_1073742118_1294, blk_1073742119_1295, blk_1073742632_1808, blk_1073742633_1809, blk_1073742634_1810, blk_1073742636_1812, blk_1073742124_1300, blk_1073742637_1813, blk_1073742638_1814, blk_1073742639_1815, blk_1073742641_1817, blk_1073742129_1305, blk_1073742642_1818, blk_1073742130_1306, blk_1073742643_1819, blk_1073742131_1307, blk_1073742644_1820, blk_1073742132_1308, blk_1073742645_1821, blk_1073742133_1309, blk_1073742646_1822, blk_1073742134_1310, blk_1073742647_1823, blk_1073742648_1824, blk_1073742136_1312, blk_1073742649_1825, blk_1073742137_1313, blk_1073742650_1826, blk_1073742138_1314, blk_1073742651_1827, blk_1073742652_1828, blk_1073742140_1316, blk_1073742653_1829, blk_1073742654_1830, blk_1073742655_1831, blk_1073742143_1319, blk_1073742656_1832, blk_1073742657_1833, blk_1073742658_1834, blk_1073742659_1835, blk_1073742147_1323, blk_1073742660_1836, blk_1073742661_1837, blk_1073742150_1326, blk_1073742664_1840, blk_1073742152_1328, blk_1073742665_1841, blk_1073742153_1329, blk_1073742667_1843, blk_1073742669_1845, blk_1073742159_1335, blk_1073742164_1340, blk_1073742166_1342, blk_1073742167_1343, blk_1073742171_1347, blk_1073742174_1350, blk_1073742175_1351, blk_1073742177_1353, blk_1073742179_1355, blk_1073742182_1358, blk_1073742695_1871, blk_1073742183_1359, blk_1073742697_1873, blk_1073742185_1361, blk_1073742186_1362, blk_1073742699_1875, blk_1073742187_1363, blk_1073742700_1876, blk_1073742188_1364, blk_1073742189_1365, blk_1073742702_1878, blk_1073742703_1879, blk_1073742191_1367, blk_1073742704_1880, blk_1073742705_1881, blk_1073742193_1369, blk_1073742706_1882, blk_1073742707_1883, blk_1073742708_1884, blk_1073742709_1885, blk_1073742710_1886, blk_1073742198_1374, blk_1073742711_1887, blk_1073742712_1888, blk_1073742713_1889, blk_1073742201_1377, blk_1073742202_1378, blk_1073742203_1379, blk_1073742209_1385, blk_1073742722_1898, blk_1073742210_1386, blk_1073742211_1387, blk_1073742212_1388, blk_1073742725_1901, blk_1073742213_1389, blk_1073742726_1902, blk_1073742214_1390, blk_1073742727_1903, blk_1073742215_1391, blk_1073742728_1904, blk_1073742216_1392, blk_1073742729_1905, blk_1073742218_1394, blk_1073742731_1907, blk_1073742732_1908, blk_1073742733_1909, blk_1073742221_1397, blk_1073742734_1910, blk_1073742222_1398, blk_1073742223_1399, blk_1073742736_1912, blk_1073742224_1400, blk_1073742226_1402, blk_1073742227_1403, blk_1073742228_1404, blk_1073742741_1917, blk_1073742742_1918, blk_1073742230_1406, blk_1073742743_1919, blk_1073742232_1408, blk_1073742745_1921, blk_1073742233_1409, blk_1073742746_1922, blk_1073742747_1923, blk_1073742235_1411, blk_1073742748_1924, blk_1073742749_1925, blk_1073742237_1413, blk_1073742238_1414, blk_1073742751_1927, blk_1073742239_1415, blk_1073742240_1416, blk_1073742754_1930, blk_1073742242_1418, blk_1073742756_1932, blk_1073742759_1935, blk_1073742247_1423, blk_1073742249_1425, blk_1073742250_1426, blk_1073742763_1939, blk_1073742764_1940, blk_1073742765_1941, blk_1073742253_1429, blk_1073742766_1942, blk_1073742767_1943, blk_1073742255_1431, blk_1073742768_1944, blk_1073742256_1432, blk_1073742769_1945, blk_1073742770_1946, blk_1073742258_1434, blk_1073742771_1947, blk_1073742259_1435, blk_1073742772_1948, blk_1073742260_1436, blk_1073742261_1437, blk_1073742774_1950, blk_1073742262_1438, blk_1073742775_1951, blk_1073742263_1439, blk_1073742776_1952, blk_1073742264_1440, blk_1073742777_1953, blk_1073742265_1441, blk_1073742266_1442, blk_1073742267_1443, blk_1073742268_1444, blk_1073742269_1445, blk_1073742782_1958, blk_1073742270_1446, blk_1073742271_1447, blk_1073742784_1960, blk_1073742272_1448, blk_1073742786_1962, blk_1073742787_1963, blk_1073742789_1965, blk_1073742792_1968, blk_1073742793_1969, blk_1073742281_1457, blk_1073742794_1970, blk_1073742795_1971, blk_1073742313_1489, blk_1073742314_1490, blk_1073742316_1492, blk_1073742317_1493, blk_1073742318_1494, blk_1073742319_1495, blk_1073742321_1497, blk_1073742323_1499, blk_1073742328_1504, blk_1073742330_1506]
2019-01-27 13:45:23,372 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 172.18.0.4
2019-01-27 13:45:23,372 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2019-01-27 13:45:23,372 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 5123
2019-01-27 13:45:23,373 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 9 Total time for transactions(ms): 4 Number of transactions batched in Syncs: 0 Number of syncs: 6 SyncTimes(ms): 42 
2019-01-27 13:45:23,375 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 9 Total time for transactions(ms): 4 Number of transactions batched in Syncs: 0 Number of syncs: 7 SyncTimes(ms): 45 
2019-01-27 13:45:23,376 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /root/hdfs/namenode/current/edits_inprogress_0000000000000005123 -> /root/hdfs/namenode/current/edits_0000000000000005123-0000000000000005131
2019-01-27 13:45:23,376 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 5132
2019-01-27 13:45:23,434 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 600.00 KB/s
2019-01-27 13:45:23,434 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000005131 size 3114 bytes.
2019-01-27 13:45:23,442 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 5122
2019-01-27 13:45:23,443 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/root/hdfs/namenode/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-01-27 13:51:30,075 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 9 
